---
layout:     post
title:      workday11
subtitle:   
date:       2019-12-30
author:     RJ
header-img: 
catalog: true
tags:
    - Job


---
<p id = "build"></p>
---

## 今日工作安排

## 1. warm up 启动
[reference cs231n](http://cs231n.github.io/neural-networks-3/)

cs231n提供了三种学习率递减的方法：

**Annealing the learning rate**

In training deep networks, it is usually helpful to anneal the learning rate over time. Good intuition to have in mind is that with a high learning rate, the system contains too much kinetic energy and the parameter vector bounces around chaotically, unable to settle down into deeper, but narrower parts of the loss function. Knowing when to decay the learning rate can be tricky: Decay it slowly and you’ll be wasting computation bouncing around chaotically with little improvement for a long time. But decay it too aggressively and the system will cool too quickly, unable to reach the best position it can. There are three common types of implementing the learning rate decay:

Step decay: 
- Reduce the learning rate by some factor every few epochs. Typical values might be reducing the learning rate by a half every 5 epochs, or by 0.1 every 20 epochs. These numbers depend heavily on the type of problem and the model. One heuristic you may see in practice is to watch the validation error while training with a fixed learning rate, and reduce the learning rate by a constant (e.g. 0.5) whenever the validation error stops improving.
- Exponential decay. has the mathematical form α=α0e−kt, where α0,k are hyperparameters and t is the iteration number (but you can also use units of epochs).
- 1/t decay has the mathematical form α=α0/(1+kt) where a0,k are hyperparameters and t is the iteration number.

In practice, we find that the **step decay** is slightly preferable because the hyperparameters it involves (the fraction of decay and the step timings in units of epochs) are more interpretable than the hyperparameter k. Lastly, if you can afford the computational budget, err on the side of slower decay and train for a longer time.

即step decay 是一个可行性较高的方案。

而transformer源代码中也提供了学习率递减的实现：
```python
def noam_scheme(init_lr, global_step, warmup_steps=4000.):
    '''Noam scheme learning rate decay
    init_lr: initial learning rate. scalar.
    global_step: scalar.
    warmup_steps: scalar. During warmup_steps, learning rate increases
        until it reaches init_lr.
    '''
    step = tf.cast(global_step + 1, dtype=tf.float32)
    return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)
```
在模型中，通过以下方式接入warm up学习率：
```python
    global_step = tf.train.get_or_create_global_step()
    lr = noam_scheme(self.hp.lr, global_step, self.hp.warmup_steps)
    optimizer = tf.train.AdamOptimizer(lr)
    train_op = optimizer.minimize(loss, global_step=global_step)
```

加入warmup 启动，在
- sin/cos 位置编码 
- 最后一层才FFNN
- 使用relu 激活

的前提条件下进行训练测试：  
```
batch  3582 : average loss =  2.9584745160363566 average acc =  0.6444797912782645
batch  3582 : average loss =  1.7471029150256296 average acc =  0.8269664532676885
batch  3582 : average loss =  1.6126026326935572 average acc =  0.8761503244205348
batch  3582 : average loss =  1.538097649205167 average acc =  0.9025733148342603
batch  3582 : average loss =  1.487765123244272 average acc =  0.9198538448949727
```
这里warm up 有两个特点，预热启动与权重衰减。使得效果有了明显的提升，由于学习率随着训练的不断减小，模型会朝着loss最低点稳定的前进。

Test结果：
```
 the  7175 th example.
原文汉字id: 782, 1262, 1056, 1135, 2967, 1825, 1499, 2419, 965, 0, 323, 1480, 1298
原文汉字： ['这', '令', '被', '贷', '款', '的', '员', '工', '们', '寝', '食', '难', '安']
识别结果id: [782, 1262, 1056, 1135, 2967, 1825, 1499, 2374, 965, 876, 662, 1480, 1298]
识别结果汉字：： 这令被贷款的员公们钻石难安
词错误率： 0.1337087767861404
```
测试错误率进一步降低。

## 2. 对比gelu & relu 激活
```python

def gelu(x):
  """Gaussian Error Linear Unit.
  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    x: float Tensor to perform activation.
  Returns:
    `x` with the GELU activation applied.
  """
  cdf = 0.5 * (1.0 + tf.tanh(
      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))
  return x * cdf

```
上一次测试时，只改变了FFNN的activation,这次仔细查看bert源码，发现Q,K,V三个权重也需要gelu激活。

bert的config.json文件如下：
```python
{
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

```
为什么transformer要用6个block？没什么神奇的地方，你可以用12甚至24个block尝试看看谁的效果更好，当然，还要计算性价比的问题，毕竟block太多，训练时间也线性增加。

在保持其他参数不变的情况下，替换gelu的10个epoch训练结果：
```
enter epoch training

epoch  1 : average loss =  2.926928342969948  average acc =  0.649870940620385
epoch  2 : average loss =  1.7273865081317863 average acc =  0.8337150723432836
epoch  3 : average loss =  1.5964090809146974 average acc =  0.8821395671557395

epoch  4 : average loss =  1.5161243553635662 average acc =  0.9099843631373941
epoch  5 : average loss =  1.467014171863461  average acc =  0.9271922298930471
epoch  6 : average loss =  1.4326071423946296 average acc =  0.9386106598680162

epoch  7 : average loss =  1.4110040038166616 average acc =  0.9459527923520867
epoch  8 : average loss =  1.3907418881205666 average acc =  0.9525065193027134
epoch  9 : average loss =  1.3765275276296467 average acc =  0.9567605440292369

epoch  10: average loss =  1.3642499469172784 average acc =  0.9599733358486775
```

Test
```
...
...
 the  7175 th example.
原文汉字id: 110, 52, 2248, 1434, 1940, 543, 676, 539, 1221, 0, 2632, 1473, 1570
原文汉字： ['这', '令', '被', '贷', '款', '的', '员', '工', '们', '寝', '食', '难', '安']
识别结果id: [110, 1875, 2248, 1434, 1940, 543, 676, 539, 1221, 164, 259, 602, 1570]
识别结果汉字：： 这另被贷款的员工们钻时男安

词错误率： 0.11740562210661958
```

保持相关参数不变，对比relu激活：
```
epoch  1 : average loss =  2.957162430718504  average acc =  0.6458561538917077
epoch  2 : average loss =  1.7446748105815855 average acc =  0.8273210593423042
epoch  3 : average loss =  1.613691532122493  average acc =  0.8762754570236089

epoch  4 : average loss =  1.5399384186341756 average acc =  0.902203948429196
epoch  5 : average loss =  1.4892072360031567 average acc =  0.9192611633882414
epoch  6 : average loss =  1.452896132278762  average acc =  0.9317814469736936

epoch  7 : average loss =  1.426641392794377  average acc =  0.9400873318526772
epoch  8 : average loss =  1.4072654763602306 average acc =  0.9469653661275702
epoch  9 : average loss =  1.392334524828174  average acc =  0.9520934241772098

epoch  10: average loss =  1.3801505432818661 average acc =  0.955242309264716

```

Test：
```

 the  7175 th example.
原文汉字id: 2872, 278, 327, 1219, 1534, 1842, 2226, 2559, 993, 0, 1175, 367, 2740
原文汉字： ['这', '令', '被', '贷', '款', '的', '员', '工', '们', '寝', '食', '难', '安']
识别结果id: [2872, 278, 327, 1219, 1534, 1842, 2226, 2559, 993, 644, 2584, 367, 2740]
识别结果汉字：： 这令被贷款的员工们钻时难安

词错误率： 0.12010690593232473
```

这一次对比说明，使用gelu激活的效果强于relu激活，此后统一采用gelu激活。


## 3. FFNN
在保持其他参数不变的情况下，使用relu激活，对每个block使用FFNN效果如下：
```
epoch  1 : average loss =  2.766840024721486 average acc =  0.6614723001403645
epoch  2 : average loss =  1.7955591299786178 average acc =  0.8023978024845981
epoch  3 : average loss =  1.6574791981101336 average acc =  0.8523972278631988
    
epoch  4 : average loss =  1.5640668386988252 average acc =  0.8848750209615271
epoch  5 : average loss =  1.4897943796042257 average acc =  0.910016264721722
epoch  6 : average loss =  1.4401253202180562 average acc =  0.9278382186914949
    
epoch  7 : average loss =  1.3998395854019978 average acc =  0.9418324149710211
epoch  8 : average loss =  1.3691804837461492 average acc =  0.9517546607874785
epoch  9 : average loss =  1.3442990054168105 average acc =  0.9586445299382651

epoch  10: average loss =  1.326776669398928 average acc =  0.9640404089673609    
```
Test:
```
词错误率： 0.14050493962678376
```

在保持其他参数不变的情况下，使用gelu激活，对每个block使用FFNN效果如下：

```
epoch  1 : average loss =  2.7176432319607557 average acc =  0.6732534026111747
epoch  2 : average loss =  1.7695135962393855 average acc =  0.8131803985382384
epoch  3 : average loss =  1.6222020005820283 average acc =  0.8636836414895088

epoch  4 : average loss =  1.5205748733679232 average acc =  0.9000663404123839
epoch  5 : average loss =  1.448901398559417 average acc =  0.925205189300895
epoch  6 : average loss =  1.3968746380178798 average acc =  0.9423782911190138

epoch  7 : average loss =  1.3614413557156475 average acc =  0.9542037611965093
epoch  8 : average loss =  1.335362438714827 average acc =  0.9618936337861427
epoch  9 : average loss =  1.316810144455331 average acc =  0.9669127260064053

epoch  10: average loss =  1.300948501598632 average acc =  0.9709832293299515
```
Test:
```
词错误率： 0.14186035412590084
```
对比发现gelu+多个FFNN训练精度是最高的，但是测试的时候，效果一般，可能是少数据量导致多层FFNN有点过拟合。





## 后记

tensorflow transformer guide
[理解transformer模型](https://www.tensorflow.org/tutorials/text/transformer#optimizer)

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191230transformerguide.png)

### transformer的优缺点：
```
一个 transformer 模型用自注意力层而非 RNNs 或 CNNs 来处理变长的输入。这种通用架构有一系列的优势：

它不对数据间的时间/空间关系做任何假设。这是处理一组对象（objects）的理想选择（例如，星际争霸单位（StarCraft units））。
层输出可以并行计算，而非像 RNN 这样的序列计算。
远距离项可以影响彼此的输出，而无需经过许多 RNN 步骤或卷积层（例如，参见场景记忆 Transformer（Scene Memory Transformer））
它能学习长距离的依赖。在许多序列任务中，这是一项挑战。
该架构的缺点是：

对于时间序列，一个单位时间的输出是从整个历史记录计算的，而非仅从输入和当前的隐含状态计算得到。这可能效率较低。
如果输入确实有时间/空间的关系，像文本，则必须加入一些位置编码，否则模型将有效地看到一堆单词。
```

### scaled_dot_product_attention
```python
def scaled_dot_product_attention(q, k, v, mask):
  """计算注意力权重。
  q, k, v 必须具有匹配的前置维度。
  k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。
  虽然 mask 根据其类型（填充或前瞻）有不同的形状，
  但是 mask 必须能进行广播转换以便求和。
  
  参数:
    q: 请求的形状 == (..., seq_len_q, depth)
    k: 主键的形状 == (..., seq_len_k, depth)
    v: 数值的形状 == (..., seq_len_v, depth_v)
    mask: Float 张量，其形状能转换成
          (..., seq_len_q, seq_len_k)。默认为None。
    
  返回值:
    输出，注意力权重
  """

  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)
  
  # 缩放 matmul_qk
  dk = tf.cast(tf.shape(k)[-1], tf.float32)
  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

  # 将 mask 加入到缩放的张量上。
  if mask is not None:
    scaled_attention_logits += (mask * -1e9)  

  # softmax 在最后一个轴（seq_len_k）上归一化，因此分数
  # 相加等于1。
  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)

  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)

  return output, attention_weights
```

其中： 点积注意力被缩小了深度的平方根倍。这样做是因为对于较大的深度值（embedding_size），点积的大小会增大，从而推动 softmax 函数往仅有很小的梯度的方向靠拢，导致了一种很硬的（hard）softmax。例如，假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为 dk。因此，dk 的平方根被用于缩放（而非其他数值），因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样会获得一个更平缓的 softmax。

遮挡（mask）与 -1e9（接近于负无穷）相乘。这样做是因为遮挡与缩放的 Q 和 K 的矩阵乘积相加，并在 softmax 之前立即应用。目标是将这些单元归零，因为 softmax 的较大负数输入在输出中接近于零。

当 softmax 在 K 上进行归一化后，它的值决定了分配到 Q 的重要程度。

输出表示注意力权重和 V（数值）向量的乘积。这确保了要关注的词保持原样，而无关的词将被清除掉。

### FFNN 点式前馈网络（Point wise feed forward network）

点式前馈网络由两层全联接层组成，**两层之间有一个 ReLU 激活函数。**
```PYTHON
def ff(inputs, num_units, scope="positionwise_feedforward"):
    '''position-wise feed forward net. See 3.3
    
    inputs: A 3d tensor with shape of [N, T, C].
    num_units: A list of two integers.
    scope: Optional scope for `variable_scope`.

    Returns:
      A 3d tensor with the same shape and dtype as inputs
    '''
    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
        # Inner layer
        outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)

        # Outer layer
        outputs = tf.layers.dense(outputs, num_units[1])

        # Residual connection
        outputs += inputs
        
        # Normalize
        outputs = ln(outputs)
    
    return outputs
```


### Encoder 详细结构
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191230encoder.png)

结构包括：
- embedding
- self attention   add & layer_norm
- FFNN             add & layer_norm


源代码的实现结构包括：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191230sourceencoder.png)

即除了transformer原实现每个block都有FFNN，项目代码只有最后一层接了FFNN。

### Decoder 详细结构



## transformer model 代码
```python
# -*- coding: utf-8 -*-
# /usr/bin/python3
'''
Feb. 2019 by kyubyong park.
kbpark.linguist@gmail.com.
https://www.github.com/kyubyong/transformer

Transformer network
'''
import tensorflow as tf

from data_load import load_vocab
from modules import get_token_embeddings, ff, positional_encoding, multihead_attention, label_smoothing, noam_scheme
from utils import convert_idx_to_token_tensor
from tqdm import tqdm
import logging

logging.basicConfig(level=logging.INFO)

class Transformer:
    '''
    xs: tuple of
        x: int32 tensor. (N, T1)
        x_seqlens: int32 tensor. (N,)
        sents1: str tensor. (N,)
    ys: tuple of
        decoder_input: int32 tensor. (N, T2)
        y: int32 tensor. (N, T2)
        y_seqlen: int32 tensor. (N, )
        sents2: str tensor. (N,)
    training: boolean.
    '''
    def __init__(self, hp):
        self.hp = hp
        self.token2idx, self.idx2token = load_vocab(hp.vocab)
        self.embeddings = get_token_embeddings(self.hp.vocab_size, self.hp.d_model, zero_pad=True)

    def encode(self, xs, training=True):
        '''
        Returns
        memory: encoder outputs. (N, T1, d_model)
        '''
        with tf.variable_scope("encoder", reuse=tf.AUTO_REUSE):
            x, seqlens, sents1 = xs

            # src_masks
            src_masks = tf.math.equal(x, 0) # (N, T1)

            # embedding
            enc = tf.nn.embedding_lookup(self.embeddings, x) # (N, T1, d_model)
            enc *= self.hp.d_model**0.5 # scale

            enc += positional_encoding(enc, self.hp.maxlen1)
            enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)

            ## Blocks
            for i in range(self.hp.num_blocks):
                with tf.variable_scope("num_blocks_{}".format(i), reuse=tf.AUTO_REUSE):
                    # self-attention
                    enc = multihead_attention(queries=enc,
                                              keys=enc,
                                              values=enc,
                                              key_masks=src_masks,
                                              num_heads=self.hp.num_heads,
                                              dropout_rate=self.hp.dropout_rate,
                                              training=training,
                                              causality=False)
                    # feed forward
                    enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])
        memory = enc
        return memory, sents1, src_masks

    def decode(self, ys, memory, src_masks, training=True):
        '''
        memory: encoder outputs. (N, T1, d_model)
        src_masks: (N, T1)

        Returns
        logits: (N, T2, V). float32.
        y_hat: (N, T2). int32
        y: (N, T2). int32
        sents2: (N,). string.
        '''
        with tf.variable_scope("decoder", reuse=tf.AUTO_REUSE):
            decoder_inputs, y, seqlens, sents2 = ys

            # tgt_masks
            tgt_masks = tf.math.equal(decoder_inputs, 0)  # (N, T2)

            # embedding
            dec = tf.nn.embedding_lookup(self.embeddings, decoder_inputs)  # (N, T2, d_model)
            dec *= self.hp.d_model ** 0.5  # scale

            dec += positional_encoding(dec, self.hp.maxlen2)
            dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)

            # Blocks
            for i in range(self.hp.num_blocks):
                with tf.variable_scope("num_blocks_{}".format(i), reuse=tf.AUTO_REUSE):
                    # Masked self-attention (Note that causality is True at this time)
                    dec = multihead_attention(queries=dec,
                                              keys=dec,
                                              values=dec,
                                              key_masks=tgt_masks,
                                              num_heads=self.hp.num_heads,
                                              dropout_rate=self.hp.dropout_rate,
                                              training=training,
                                              causality=True,
                                              scope="self_attention")

                    # Vanilla attention
                    dec = multihead_attention(queries=dec,
                                              keys=memory,
                                              values=memory,
                                              key_masks=src_masks,
                                              num_heads=self.hp.num_heads,
                                              dropout_rate=self.hp.dropout_rate,
                                              training=training,
                                              causality=False,
                                              scope="vanilla_attention")
                    ### Feed Forward
                    dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])

        # Final linear projection (embedding weights are shared)
        weights = tf.transpose(self.embeddings) # (d_model, vocab_size)
        logits = tf.einsum('ntd,dk->ntk', dec, weights) # (N, T2, vocab_size)
        y_hat = tf.to_int32(tf.argmax(logits, axis=-1))

        return logits, y_hat, y, sents2

    def train(self, xs, ys):
        '''
        Returns
        loss: scalar.
        train_op: training operation
        global_step: scalar.
        summaries: training summary node
        '''
        # forward
        memory, sents1, src_masks = self.encode(xs)
        logits, preds, y, sents2 = self.decode(ys, memory, src_masks)

        # train scheme
        y_ = label_smoothing(tf.one_hot(y, depth=self.hp.vocab_size))
        ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)
        nonpadding = tf.to_float(tf.not_equal(y, self.token2idx["<pad>"]))  # 0: <pad>
        loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + 1e-7)

        global_step = tf.train.get_or_create_global_step()
        lr = noam_scheme(self.hp.lr, global_step, self.hp.warmup_steps)
        optimizer = tf.train.AdamOptimizer(lr)
        train_op = optimizer.minimize(loss, global_step=global_step)

        tf.summary.scalar('lr', lr)
        tf.summary.scalar("loss", loss)
        tf.summary.scalar("global_step", global_step)

        summaries = tf.summary.merge_all()

        return loss, train_op, global_step, summaries

    def eval(self, xs, ys):
        '''Predicts autoregressively
        At inference, input ys is ignored.
        Returns
        y_hat: (N, T2)
        '''
        decoder_inputs, y, y_seqlen, sents2 = ys

        decoder_inputs = tf.ones((tf.shape(xs[0])[0], 1), tf.int32) * self.token2idx["<s>"]
        ys = (decoder_inputs, y, y_seqlen, sents2)

        memory, sents1, src_masks = self.encode(xs, False)

        logging.info("Inference graph is being built. Please be patient.")
        for _ in tqdm(range(self.hp.maxlen2)):
            logits, y_hat, y, sents2 = self.decode(ys, memory, src_masks, False)
            if tf.reduce_sum(y_hat, 1) == self.token2idx["<pad>"]: break

            _decoder_inputs = tf.concat((decoder_inputs, y_hat), 1)
            ys = (_decoder_inputs, y, y_seqlen, sents2)

        # monitor a random sample
        n = tf.random_uniform((), 0, tf.shape(y_hat)[0]-1, tf.int32)
        sent1 = sents1[n]
        pred = convert_idx_to_token_tensor(y_hat[n], self.idx2token)
        sent2 = sents2[n]

        tf.summary.text("sent1", sent1)
        tf.summary.text("pred", pred)
        tf.summary.text("sent2", sent2)
        summaries = tf.summary.merge_all()

        return y_hat, summaries


```