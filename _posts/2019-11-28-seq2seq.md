---
layout:     post
title:      Seq2seq实践
subtitle:   
date:       2019-11-28
author:     RJ
header-img: 
catalog: true
tags:
    - NLP
---
<p id = "build"></p>
---

## 理论参考
[Tensorflow中的Seq2Seq全家桶](https://zhuanlan.zhihu.com/p/47929039)

[简说Seq2Seq原理及实现](https://zhuanlan.zhihu.com/p/57155059)

### 简单的seq2seq实现：

编码和解码都用单向的dynamic_rnn
```python
import tensorflow as tf

class Seq2seq(object):
    def __init__(self, config, w2i_target):
        self.seq_inputs = tf.placeholder(shape=(config.batch_size, None), dtype=tf.int32, name='seq_inputs')
        self.seq_inputs_length = tf.placeholder(shape=(config.batch_size,), dtype=tf.int32, name='seq_inputs_length')
        self.seq_targets = tf.placeholder(shape=(config.batch_size, None), dtype=tf.int32, name='seq_targets')
        self.seq_targets_length = tf.placeholder(shape=(config.batch_size,), dtype=tf.int32, name='seq_targets_length')
        
	with tf.variable_scope("encoder"):
		encoder_embedding = tf.Variable(tf.random_uniform([config.source_vocab_size, config.embedding_dim]), dtype=tf.float32, name='encoder_embedding')
		encoder_inputs_embedded = tf.nn.embedding_lookup(encoder_embedding, self.seq_inputs)
		encoder_cell = tf.nn.rnn_cell.GRUCell(config.hidden_dim)
		encoder_outputs, encoder_state = tf.nn.dynamic_rnn(cell=encoder_cell,                                                           inputs=encoder_inputs_embedded,                                                                    sequence_length=self.seq_inputs_length, 
                                        dtype=tf.float32, time_major=False)
	
	tokens_go = tf.ones([config.batch_size], dtype=tf.int32) * w2i_target["_GO"]
	decoder_inputs = tf.concat([tf.reshape(tokens_go,[-1,1]), self.seq_targets[:,:-1]], 1)

	with tf.variable_scope("decoder"):
		decoder_embedding = tf.Variable(tf.random_uniform([config.target_vocab_size, config.embedding_dim]), dtype=tf.float32, name='decoder_embedding')
		decoder_inputs_embedded = tf.nn.embedding_lookup(decoder_embedding, decoder_inputs)
		decoder_cell = tf.nn.rnn_cell.GRUCell(config.hidden_dim)
		decoder_outputs, decoder_state = tf.nn.dynamic_rnn(cell=decoder_cell,                                                                   inputs=decoder_inputs_embedded, 
                                            initial_state=encoder_state,                                       sequence_length=self.seq_targets_length,                           dtype=tf.float32, time_major=False)

	decoder_logits = tf.layers.dense(decoder_outputs.rnn_output, config.target_vocab_size)
	self.out = tf.argmax(decoder_logits, 2)
```
### 使用tf.contrib下的seq2seq中的BasicDecoder

```python

import tensorflow as tf

from simple_config import config
from tensorflow.contrib import seq2seq as seq2seq_contrib


class simple_seq2seq(object):

    def __init__(self, w2id_target):
        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[None], name='batch_size')

        self.input = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input')
        self.input_length = tf.placeholder(dtype=tf.int32, shape=[None], name='input_length')

        self.target = tf.placeholder(dtype=tf.int32, shape=[None, None], name='target')
        self.target_length = tf.placeholder(dtype=tf.int32, shape=[None], name='target_length')

        with tf.variable_scope('encoder'):
            enc_embedding_table = tf.Variable(tf.random(config.source_vocab_size, config.embedding_size),
                                              dtype=tf.int32,
                                              name='enc_embedding_table')
            enc_input_embedding = tf.nn.embedding_lookup(enc_embedding_table, self.input)

            with tf.variable_scope('gru_cell'):
                cell = tf.nn.rnn_cell.GRUCell(config.hidden_dim)
                ((enc_fw_output, enc_bw_output), (enc_fw_state, enc_bw_state)) = tf.nn.bidirectional_dynamic_rnn(
                    fw_cell=cell, bw_cell=cell, inputs=enc_input_embedding, sequence_length=self.input_length,
                    dtype=tf.float32, time_major=False)

            #  简单版的只用state计算，即保存信息的细胞状态C
            enc_state = tf.add(enc_fw_state, enc_bw_state)
            enc_output = tf.add(enc_fw_output, enc_bw_output)

        with tf.variable_scope('decoder'):
            dec_embedding_table = tf.Variable(tf.random_normal([config.target_vocab_size, config.embedding_size]),
                                              dtype=tf.float32, name='dec_embedding')
            token_go = tf.ones([self.batch_size], dtype=tf.int32, name='token_go') * w2id_target['_GO']
            helper = seq2seq_contrib.GreedyEmbeddingHelper(dec_embedding_table, token_go, w2id_target["_EOS"])

            with tf.variable_scope('gru_cell'):
                decoder_cell = tf.nn.rnn_cell.GRUCell(config.hidden_dim)

                decoder_initial_state = enc_state

        # 构建decoder
        decoder = seq2seq_contrib.BasicDecoder(decoder_cell, helper, decoder_initial_state,
                                               output_layer=tf.layers.Dense(config.target_vocab_size))
        dec_outputs, dec_state, final_sequence_lengths = seq2seq_contrib.dynamic_decode(decoder,
                                                                                        maximum_iterations=tf.reduce_max(
                                                                                                    self.seq_targets_length))

        self.decoder_logits = dec_outputs.rnn_output
        self.out = tf.argmax(self.decoder_logits, 2)

        # mask掉填充的0，使后边计算的时候0不参与计算。
        sequence_mask = tf.sequence_mask(self.seq_targets_length, dtype=tf.float32)
        self.loss = seq2seq_contrib.sequence_loss(logits=self.decoder_logits, targets=self.seq_targets,
                                                  weights=sequence_mask)
        # 防止梯度消失和梯度爆炸
        opt = tf.train.AdamOptimizer(config.learning_rate)
        gradients = opt.compute_gradients(self.loss)
        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]
        self.train_op = opt.apply_gradients(capped_gradients)

```



## 相关任务
- 机器翻译
- 聊天机器人
- 字幕生成

## 相关数据准备
[nlpcc](http://tcci.ccf.org.cn/conference/2017/taskdata.php)



## 机器翻译

[Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf)

[NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf)

## 参考

[清华NMT](https://www.jiqizhixin.com/articles/machinetranslation)

[陈猛seq2seq](https://github.com/KevinChen1994/seq2seq_learning)

[seq2seq简单实践](https://www.zhihu.com/search?type=content&q=seq2seq)

[seq2seq全家桶](https://github.com/wavewangyue/tensorflow_seq2seq)

[机器之心](https://www.jiqizhixin.com/articles/machinetranslation)