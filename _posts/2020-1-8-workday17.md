---
layout:     post
title:      workday17
subtitle:   
date:       2020-1-8
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 前言
今天学校有个会议要开，同学顺便也一起聚个餐。

下午回来接着做手里的事情。



## Pre-training with BERT

We are releasing code to do "masked LM" and "next sentence prediction" on an arbitrary text corpus. Note that this is not the exact code that was used for the paper (the original code was written in C++, and had some additional complexity), but this code does generate pre-training data as described in the paper.

Here's how to run the data generation. The input is a plain text file, with one sentence per line. (It is important that these be actual sentences for the "next sentence prediction" task). Documents are delimited by empty lines. The output is a set of tf.train.Examples serialized into TFRecord file format.

You can perform sentence segmentation with an off-the-shelf NLP toolkit such as spaCy. The create_pretraining_data.py script will concatenate segments until they reach the maximum sequence length to minimize computational waste from padding (see the script for more details). However, you may want to intentionally add a slight amount of noise to your input data (e.g., randomly truncate 2% of input segments) to make it more robust to non-sentential input during fine-tuning.

This script stores all of the examples for the entire input file in memory, so for large data files you should shard the input file and call the script multiple times. (You can pass in a file glob to run_pretraining.py, e.g., tf_examples.tf_record*.)

The max_predictions_per_seq is the maximum number of masked LM predictions per sequence. You should set this to around max_seq_length * masked_lm_prob (the script doesn't do that automatically because the exact value needs to be passed to both scripts).

 bert的预训练：输入数据是由一个纯文本文件(文本中每条数据后空一行)经过create_pretraining_data.py（iqiyi是create_tf_record.py）

 对比两个文件：

create_pretraining_data.py

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/bert_create_data_20200108171806.png)

-----
create_tf_record.py
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/iqiyi_create_data_20200108172027.png)

发现多出了一些item:

全局变量：
```python
MASK_PROB = pickle.load(open('create_data/mask_probability.sav', 'rb'))
WRONG_COUNT = dict([(k, 0) for k in MASK_PROB])
CORRECT_COUNT = dict([(k, 0) for k in MASK_PROB])

```

以及一个方法：

```python
def create_masked_lm_predictions_for_wrong_sentences(tokens, masked_lm_prob,
                                                     max_predictions_per_seq, vocab_words, rng, wrong_tokens):
    """Creates the predictions for the masked LM objective."""

    cand_indexes = []
    if not len(tokens) == len(wrong_tokens):
        print(tokens)
        print(wrong_tokens)
    assert len(tokens) == len(wrong_tokens)
    for (i, token) in enumerate(tokens):
        if token == "[CLS]" or token == "[SEP]":
            assert wrong_tokens[i] == token
            continue
        elif token != wrong_tokens[i]:
            cand_indexes.append(i)
        else:  # when a token is not confused, add it to candidates according to its mask probability
            if token in MASK_PROB:
                if rng.random() < MASK_PROB[token]:
                    WRONG_COUNT[token] += 1
                    # print(f'cover {token} in wrong instance.')
                    cand_indexes.append(i)

    rng.shuffle(cand_indexes)

    output_tokens = list(tokens)

    # num_to_predict = min(max_predictions_per_seq,
    #                      max(1, int(round(len(tokens) * masked_lm_prob ))))
    num_to_predict = min(max_predictions_per_seq,
                         max(1, int(round(len(tokens)))))  # we set 100% masking rate to allow all errors and corresponding non-errors to be masked

    masked_lms = []
    covered_indexes = set()
    for index in cand_indexes:
        if len(masked_lms) >= num_to_predict:
            break
        if index in covered_indexes:
            continue
        covered_indexes.add(index)

        masked_token = wrong_tokens[index]
        output_tokens[index] = masked_token

        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))

    masked_lms = sorted(masked_lms, key=lambda x: x.index)

    masked_lm_positions = []
    masked_lm_labels = []
    for p in masked_lms:
        masked_lm_positions.append(p.index)
        masked_lm_labels.append(p.label)

    return output_tokens, masked_lm_positions, masked_lm_labels
```

**方法的具体调用位置**：

```python
if tokens == wrong_tokens:
    (tokens, masked_lm_positions,
        masked_lm_labels) = create_masked_lm_predictions(
        tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)
else:
    (tokens, masked_lm_positions,
        masked_lm_labels) = create_masked_lm_predictions_for_wrong_sentences(
        tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng, wrong_tokens)
    # print(tokens)
    # print(wrong_tokens)
    # print(masked_lm_positions)
    # print(masked_lm_labels)
    # print('\n')
instance = TrainingInstance(
    tokens=tokens,
    segment_ids=segment_ids,
    is_random_next=is_random_next,
    masked_lm_positions=masked_lm_positions,
    masked_lm_labels=masked_lm_labels)
instances.append(instance)
```

深入方法内部，注意到：

```python
    #num_to_predict = min(max_predictions_per_seq,max(1, int(round(len(tokens) * masked_lm_prob ))))
    num_to_predict = min(max_predictions_per_seq,
                         max(1, int(round(len(tokens)))))  #we set 100% masking rate to allow all errors and corresponding non-errors to be masked
```

bert模型中设置：

flags.DEFINE_float("masked_lm_prob", 0.15, "Masked LM probability.")

iqiyi:

we set 100% masking rate to allow all errors and corresponding non-errors to be masked.

即iqiyi对文本的所有位置都执行掩码操作，而非bert中的15% masked.

有一个问题，Bert中的NSP操作，在create_tfrecord.py代码中的实现中是通过随机挑选准备的数据中的句子，那么每次NSP都是False了？

在create_instance_from_document()中， 
```python
if len(current_chunk) == 1 or rng.random() < 0.5:
    is_random_next = True
```
其中current_chunk始终都是1，所以代码出现冗余，即rng部分。

所以每一次NSP都是is_random_next = True

而每个instance中，被Mask的概率是通过：
```python
        # 80% of the time, replace with [MASK]
        if rng.random() < 0.8:
            masked_token = "[MASK]"
        else:
            # 10% of the time, keep original
            if rng.random() < 0.5:
                masked_token = tokens[index]
            # 10% of the time, replace with random word
            else:
                masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]

        # overwrite the above assignment with mask_prob
        if tokens[index] in MASK_PROB:
            if rng.random() < MASK_PROB[tokens[index]]:
                masked_token = tokens[index]
                # print(f'cover {tokens[index]} in correct instance.')
                CORRECT_COUNT[tokens[index]] += 1

        output_tokens[index] = masked_token
```
其中，对Bert原本的Mask机制作了更改，对句子中的每个字，若该字出现在MASK_PROB[tokens[index]]字典中，就不做替换。

由于判断， 随机值 < MASK_RPOB, 对于这个字典而言，字出错的次数越多，masked的概率越高，所以错误次数多的词反而不MASK了？

更改为>符号后

468 row
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/correct_20200108225746.png)

诸侯的侯就被MASK了。