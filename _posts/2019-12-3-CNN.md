---
layout:     post
title:      深入学习CNN
subtitle:   
date:       2019-12-3
author:     RJ
header-img: 
catalog: true
tags:
    - NLP
---
<p id = "build"></p>
---

## 前言

一直觉得以后要往NLP发展，CNN可以不用学太多，掌握基本的卷积池化。看过那个CNN for TextClassification 就够了，但是在一个简单的清华Cnews文本14分类任务上，能在几分钟之内达到91.07%的精度，而基本的LSTM却只有87.78%，所需要的时间却很长一个多小时，在使用wiki100预训练词向量后得到89.90%的精度，加上双向加上attention之后才能得到90.10%的精度。

为什么LSTM的精度比较低呢？甚至我的实验中还没CNN的效果好？

实际上第一次实验在清华cnews十分类中，用双向GRU+attention+预训练词向量时精度最高到了96.39%, 而CNN用了三种尺寸的卷积核就能达到96.04%，二者训练的时间显然差距是巨大的。

第二次实验使用的是Chinese Glue上的清华cnews十四分类，比第一次实验多了4个类别，除此之外，chinese提供的数据，不同类别的样本是不一样的，有些类别数量少，而有些却很多：

```python
label_dict
{'体育': 5248,
 '股票': 6183,
 '社会': 2041,
 '时政': 2567,
 '娱乐': 3723,
 '科技': 6478,
 '财经': 1487,
 '游戏': 978,
 '彩票': 290,
 '家居': 1305,
 '教育': 1637,
 '房产': 814,
 '时尚': 541,
 '星座': 144}
```

除此之外，CNN和RNN都用的是截断文本长度，取600进行训练的。而在长序列文本分类中，看了Jason Lee 的Handle Long seq method, 了解到LSTM取截断操作的最佳长度在250-500之间。走远了，不过如何使用LSTM在长文档上作一些处理还是需要注意的，后面找时间尝试一下。

这篇blog就当作CNN的学习记录，那么就开始吧。

## 1. CNN简单应用
