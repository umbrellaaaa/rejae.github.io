---
layout:     post
title:      workday14
subtitle:   
date:       2020-1-3
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 工作安排
1. 语音数据错误率统计分析，定位错误分布，思考纠错策略
2. 拼音语言模型初步搭建
3. 纠错基本知识学习归纳


## 1. 错误统计分析
提取数据到compare.txt文件中：
```
hanzi:兰州哪有买路虎汽车的
corrc:lan2 zhou1 na3 you3 mai3 lu4 hu3 qi4 che1 de
error:lai2 diao4 na2 you3 mai3 lu4 hu2 qi2 che1 le
diff:
兰 lan2 lai2
州 zhou1 diao4
哪 na3 na2
虎 hu3 hu2
汽 qi4 qi2
的 de le

hanzi:明天上午六点三十分叫我
corrc:ming2 tian1 shang4 wu3 liu4 dian3 san1 shi2 fen1 jiao4 wo3
error:ming2 tian1 shang4 liu4 dian3 san1 ren2 fen1 jiao4 le
diff:
午 wu3 liu4
六 liu4 dian3
点 dian3 san1
三 san1 ren2
十 shi2 fen1
分 fen1 jiao4
叫 jiao4 le

我 wo3 [miss]


hanzi:QQ通讯录下载安装好了吗
corrc:QQ tong1 xun4 lu4 xia4 zai4 an1 zhuang1 hao3 le ma
error:ke3 he1 tong1 xue2 rou4 xia4 zai4 zhuang1 hao3 le ma
diff:
Q QQ ke3
Q tong1 he1
通 xun4 tong1
讯 lu4 xue2
录 xia4 rou4
下 zai4 xia4
载 an1 zai4

hanzi:范冰冰出演过的电视剧有哪些
corrc:fan4 bing1 bing1 chu1 yan3 guo4 de dian4 shi4 ju4 you3 nei3 xie1
error:fan4 bing1 bin1 chu1 yang3 guo2 de dian4 shi4 qu4 you3 na3 xie1
diff:
冰 bing1 bin1
演 yan3 yang3
过 guo4 guo2
剧 ju4 qu4
哪 nei3 na3

...
...
...
```
得到错误类型：
- 声调  ___  哪 na3 na2
- 韵母   ___  兰 lan2 lai2
- 声母   ___  的 de le
- 缺词错位
- 多词错位
- 完全错误___  我 wo3 le
- 英文缩写 ___ QQ  ke3 he1
- 前后鼻音 ___ 冰 bing1 bin1 ;___   呢 ne le
- 翘舌音  ___曾 ceng2 cheng2

分析完错误类型后，发现纠错的道路任重而道远。

如果一开始就把目标定的太高，如去纠正语音识别造成的缺词、多词、完全错误，势必会陷入很大的麻烦。

仔细思考了一下，先由简到繁，在保证了基础的纠错，如声调，韵母，翘舌，鼻音等有迹可循的错误纠正下，再去纠正那些比较复杂的错误。

## 2. 拼音语言模型搭建
根据拼音和汉字对应，进行分词分拼音。由于汉字的词组很大，会消耗很大的计算量，但是拼音的话就会小很多。


![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/202013glove1.png)
----
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/202013glove2.png)

[glove 训练自己的语料](https://blog.csdn.net/imsuhxz/article/details/87798501)
使用150M，110W条数据训练完成后，得到了三个文件：
- vectors.bin
- vectors.txt
- vocab.txt

其中vocab.txt中有57266个拼音组，对应了vectors.txt中的词向量

拿到这个拼音词向量之后我能做什么？ [word vectors](https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf)

想法：
日常语音的拼音应该都在拼音组里，但是如何切拼音组呢？也就是语音转拼音那块，怎么能得到切分的拼音组？

这部分同事还在研究，后续我再跟进。

## 3. 纠错基本知识归纳

[爱奇艺开源 SOTA 高性能中文拼写检查工具 FASPell](https://www.ithome.com/0/455/232.htm)

[FASP](https://www.aclweb.org/anthology/D19-5522.pdf)

[github](https://github.com/iqiyi/FASPell)

### abstract:

FABS achievements circumvents two bottlenecks:
- First, the DAE curtails the amount of Chinese spell checking data needed for supervised learning (to <10k sentences) by leveraging the power of unsupervisedly pre-trained masked language model as in BERT, XLNet, MASS etc. 
- Second, the decoder helps to eliminate the use of confusion set that is deficient in flexibility and sufficiency of utilizing the salient feature of Chinese character similarity.

即关键在于DAE 和 Decoder两部分。

### 历史研究：
历史研究：
- 拼写错误被简化为替换错误。
- 使用固定的confusion set做候选替换（对资源不足的中文拼写检查数据进行过度拟合；在利用字符相似度时，缺乏灵活性和混淆性。）

英文纠错很难迁移到中文上，因为中文句子不像英文本身就已经分词，此外，中文也没有各种变化，这导致语法和语义的理解都高度依赖上下文。

### 论文核心
DAE: 降噪自编码器，本质上是引入模拟真实场景的噪声，然后降噪来学习纠错的过程，以适应现实场景。

- DAE可以生成更全的confusion set来保证召回率。
- 由于DAE可以在大量无监督语料上训练，所以避免了过拟合。
- 解码器可以运用词间相似性，以减少有用信息的丢失
- DAE-decoder 也是seq2seq模型，但常规的seq2seq一般是编码器进行特征抽取，解码器根据特征生成文本。而在DAE-decoder中, DAE根据上下文，提供候选词，重建句子，而decoder整合其他特征来选择最佳候选词。

额外贡献：
- propose a more precise quantification method of character similarity than the ones proposed by Liu et al. (2010) and Wang et al. (2018) 更精准的字符相似度度量
- propose an empirically effective decoder to filter candidates under the principle of getting the highest possible precision with minimal harm to recall.解码器能在提高精度的情况下，减小recall的损失

### 2 FASPell

#### 2.1 MLM
- 在使用随机标记作为掩码的情况下，模型实际上学会了如何纠正错误字符;
- 在保留原始标记的情况下，模型实际上学会了如何检测字符是否错误。

然而随机替换策略与真实场景下的错误并不相同，可以通过以下方式fine-tune：
- For texts that have no errors, we follow the
original training process as in BERT;
- For texts that have errors, we create two types
of training examples by:
    - 1. given a sentence, we mask the erroneous
tokens with themselves and set their target labels as their corresponding correct
characters;
    - 2. to prevent overfitting, we also mask tokens that are not erroneous with themselves and set their target labels as themselves, too

    The two types of training examples are balanced to have roughly similar quantity.

论文做了MLM的消融实验，证明纠错的主要提升并不是来自于MLM. 除此之外，decoder对纠错性能的提升是必不可少的。

#### 2.2 Character similarity
- 形近字
- 音近字

这里我们做语音，主要研究音近字的处理。

## 

### 4 Conclusion

Future work may include studying if the DAEdecoder paradigm can be used to detect and correct **grammatical errors** or other less frequently
studied types of Chinese spelling errors such as dialectical colloquialism (Fung et al., 2017) and insertion/deletion errors.

即本论文对语法纠错，方言纠错，插删纠错还未实验，亟待研究。