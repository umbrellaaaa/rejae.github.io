---
layout:     post
title:      work day  3
subtitle:   
date:       2019-12-19
author:     RJ
header-img: 
catalog: true
tags:
    - NLP
---
<p id = "build"></p>
---

## 今日工作
1. 使用项目已有LM模型，提取thsch30数据错误类型，进行统计分析。

修改 utils中的data_hparams中的data_length遇到am.ctc_model.oad_weights中的shape不匹配的问题。

```
ValueError: Shapes (256, 887) and (256, 230) are incompatible
```

通过查看ctc_model.h5文件保存内容，发现最后一个dense2这层的参数只有230个，也即是作者在使用daa_length=10时训练的模型，导致vocab_size只有230个。所以需要重新训练ctc_model.h5文件。

抽取train.py中的语音模型训练部分代码到model_speech下，在该目录下训练模型，仅使用thchs文件训练。

该训练过程涉及到参数调优，多GPU并行问题。待调试记录。

![](![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191219142812GPU.png)

训练过程中，发现训练速度太慢，一个epoch需要3个小时，所以考虑增大batch_size。使用多卡并行。

而在使用多GPU并行的过程中，公司的多卡不是一般的多卡GPU，而是XLA_GPU，也即是优化计算的GPU，让我训练出现了问题。

XLA_GPU介绍：
```
 利用 XLA 将 GPU 性能推向极限 一文，里面提到使用Tensorflow xla可以显著提升训练速度，在ResNet上速度最多可以提升3倍，而代码的改动是很少的。

XLA加速原理
按照google官方的说法，xla的加速原理主要是融合内核。看一个例子：

 def  model_fn（x，y，z）：    
       return tf.reduce_sum（x + y * z）    
如果运行模型时不使用 XLA，图表会启动三个内核，分别用于乘法、加法和减法。

使用了XLA后，它会将加法、乘法和减法 “融合” 到单个 GPU 内核中。这种融合运算不会将 y*z 和 x+y*z 生成的中间值写入内存，而是将这些中间计算的结果直接 “流式传输” 给用户，并完整保存在 GPU 寄存器中。因为删除了内存运算，所以能够提升性能。
```

并行遇到的问题，版本兼容冲突：
```
Requested GPU:0, but only XLA_GPU:0 exsits, tf-gpu1.14.0 #30748
I met the same problem on ubuntu 18.04, cuda 10.1 and Tensorflow 1.14.0. However, I uninstalled the pip version tensorflow using pip uninstall tensorflow-gpu and then use conda install -c anaconda tensorflow-gpu to install conda version, and it works for me. You can have a try. @AmitMY

```














## 查看ctc_model.h5文件保存内容

[hdf5 introduction](http://docs.h5py.org/en/latest/high/group.html)

[CSDN hdf5 模型参数读取与写入](https://blog.csdn.net/wanggao_1990/article/details/90446736)


```python
Keras查看model weights .h5 文件的内容
Keras的模型是用hdf5存储的，如果想要查看模型，keras提供了get_weights的函数可以查看：

for layer in model.layers:
    weights = layer.get_weights()  # list of numpy array

而通过hdf5模块也可以读取：hdf5的数据结构主要是File - Group - Dataset三级，具体操作API可以看官方文档。weights的tensor保存在Dataset的value中，而每一集都会有attrs保存各网络层的属性：

import h5py

def print_keras_wegiths(weight_file_path):
    f = h5py.File(weight_file_path)  # 读取weights h5文件返回File类
    try:
        if len(f.attrs.items()):
            print("{} contains: ".format(weight_file_path))
            print("Root attributes:")
        for key, value in f.attrs.items():
            print("  {}: {}".format(key, value))  # 输出储存在File类中的attrs信息，一般是各层的名称

        for layer, g in f.items():  # 读取各层的名称以及包含层信息的Group类
            print("  {}".format(layer))
            print("    Attributes:")
            for key, value in g.attrs.items(): # 输出储存在Group类中的attrs信息，一般是各层的weights和bias及他们的名称
                print("      {}: {}".format(key, value))  

            print("    Dataset:")
            for name, d in g.items(): # 读取各层储存具体信息的Dataset类
                print("      {}: {}".format(name, d.value.shape)) # 输出储存在Dataset中的层名称和权重，也可以打印dataset的attrs，但是keras中是空的
                print("      {}: {}".format(name. d.value))
    finally:
        f.close()
```

## h5 file

```python
import h5py
import warnings

warnings.filterwarnings('ignore')


def print_keras_wegiths(weight_file_path):
    f = h5py.File(weight_file_path)  # 读取weights h5文件返回File类

    try:
        if len(f.attrs.items()):
            print("{} contains: ".format(weight_file_path))

        print(
            "-------------------------------Root attributes:----------------------------------------------------------")

        print(
            "key, value in f.attrs.items:----------------------------------------------------------------------------")
        for key, value in f.attrs.items():
            print("  {}: {}".format(key, value))  # 输出储存在File类中的attrs信息，一般是各层的名称

        print("layer, g in f.items():----------------------------------------------------------------------------")
        for layer, g in f.items():  # 读取各层的名称以及包含层信息的Group类
            print(" layer {}".format(layer))

            print(
                " key, value in g.attrs.items():--------------------------------------------------------------------------------")
            for key, value in g.attrs.items():  # 输出储存在Group类中的attrs信息，一般是各层的weights和bias及他们的名称
                print("      {}: {}".format(key, value))

            print(
                "  name, d in g.items()::--------------------------------------------------------------------------------")
            for name, d in g.items():  # 读取各层储存具体信息的Dataset类
                print("      {}: {}".format(name, d.value.shape))  # 输出储存在Dataset中的层名称和权重，也可以打印dataset的attrs，但是keras中是空的

    finally:
        f.close()


weight_file_path = 'model.h5'
# print_keras_wegiths(weight_file_path)
f = h5py.File(weight_file_path)  # 读取weights h5文件返回File类

print('xxxxxxxxxxxxxxxxxxx   layer, group in f.items()    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')
for layer, group in f.items():
    print(layer, ':', group)
    for key, value in group.attrs.items():
        print(key, ':', value)
    # for name, d in g.items():

print('xxxxxxxxxxxxxxxxxxx    layer, group in f.items()      xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

for layer, group in f.items():
    # print(layer, ':', group)
    for key, value in group.attrs.items():
        print(key, ':', value)

print('xxxxxxxxxxxxxxxxxxx    name, d in group.items()      xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')

for layer, group in f.items():
    for name, d in group.items():
        print(name, ':', d)

print('xxxxxxxxxxxxxxxxxxxxxxx      key, value in f.attrs.items()     xxxxxxxxxxxxxxxxxxxxxxxxxx')
for key, value in f.attrs.items():
    print(key, ':', value)

print('xxxxxxxxxxxxxxxxxxxxxxx      get weights     xxxxxxxxxxxxxxxxxxxxxxxxxx')

layer_list = []
# for key, value in f.attrs.items():
#     if key == 'layer_names':
#         layer_list = value
#         print(value)

layer_list = f.attrs['layer_names']
print(layer_list)
print('dense1:              -')
print(f['dense_1'])
print(f['dense_1'].attrs.keys())
print(f['dense_1'].attrs['weight_names'])

dense1_bias = f['dense_1']['dense_1/bias:0'].value
print(dense1_bias, 'len=',len(dense1_bias))


```