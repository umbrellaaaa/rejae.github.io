{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三张表： user.csv, click_log.csv, ad.csv\n",
    "- age\tgender\n",
    "- creative_id\tad_id\tproduct_id\tproduct_category\tadvertiser_id\tindustry\n",
    "\n",
    "素材id、广告id、产品id、产品类目id、广告主id、广告主行业id \n",
    "\n",
    "- time\tuser_id\tcreative_id\tclick_times\n",
    "\n",
    "户当天点击该广告的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_colwidth',60)\n",
    "\n",
    "\n",
    "user_df = pd.read_csv('user.csv')\n",
    "click_log_df = pd.read_csv('click_log.csv')\n",
    "ad_df = pd.read_csv('ad.csv')\n",
    "\n",
    "### merge到click_log表\n",
    "\n",
    "df_train = user_df.merge(click_log_df,on='user_id',how='left')\n",
    "df_train = df_train.merge(ad_df,on='creative_id',how='left')\n",
    "\n",
    "print(len(user_df))\n",
    "print(len(click_log_df))\n",
    "print(len(ad_df))\n",
    "\n",
    "print(user_df.columns.tolist())\n",
    "print(click_log_df.columns.tolist())\n",
    "print(ad_df.columns.tolist())\n",
    "\n",
    "print(user_df.dtypes)\n",
    "print(click_log_df.dtypes)\n",
    "print(ad_df.dtypes) # industry: 326 个类别, 有 \\\\N   product_id: 33273个类别,有 \\\\N \n",
    "\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- print(len(set(ad_df['creative_id']))) # 2481135\n",
    "- print(len(set(ad_df['ad_id']))) # 2264190\n",
    "- print(len(set(ad_df['product_id']))) # 33273\n",
    "- print(len(set(ad_df['product_category']))) # 18\n",
    "- print(len(set(ad_df['advertiser_id']))) # 52090\n",
    "- print(len(set(ad_df['industry']))) # 326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_log_df_test = pd.read_csv('../test/click_log.csv')\n",
    "ad_df_test = pd.read_csv('../test/ad.csv')\n",
    "df_test = click_log_df_test.merge(ad_df_test,on='creative_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train['product_id'] = df_train['product_id'].replace('\\\\N',-999)\n",
    "df_train['industry'] = df_train['industry'].replace('\\\\N',-999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['user_id']==2]['age'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "creative_id_df = DataFrame()\n",
    "id_list = user_df['user_id'].tolist()\n",
    "age_list = user_df['age'].tolist()\n",
    "gender_list = user_df['gender'].tolist()\n",
    "creative_id_list = []\n",
    "\n",
    "class build_creation_id(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __iter__(self):\n",
    "        for ids in tqdm(user_df['user_id']):\n",
    "            #df_train[df_train['user_id']==ids].sort_values(by=\"time\" , ascending=True)\n",
    "            #creative_id_list.append(df_train[df_train['user_id']==ids].sort_values(by=\"time\" , ascending=True)['creative_id'].tolist())\n",
    "            yield df_train[df_train['user_id']==ids].sort_values(by=\"time\" , ascending=True)['creative_id'].apply(str).tolist()\n",
    "model = gensim.models.Word2Vec(build_creation_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "creative_id_df = DataFrame()\n",
    "id_list = user_df['user_id'].tolist()\n",
    "age_list = user_df['age'].tolist()\n",
    "gender_list = user_df['gender'].tolist()\n",
    "creative_id_list = []\n",
    "\n",
    "for ids in tqdm(user_df['user_id']):\n",
    "        #df_train[df_train['user_id']==ids].sort_values(by=\"time\" , ascending=True)\n",
    "        #creative_id_list.append(df_train[df_train['user_id']==ids].sort_values(by=\"time\" , ascending=True)['creative_id'].tolist())\n",
    "    creative_id_list.append(df_train[df_train['user_id']==ids].sort_values(by=\"time\" , ascending=True)['creative_id'].apply(str).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_id_df = DataFrame({'id':id_list,'age':age_list,'gender':gender_list,'sentence':creative_id_list})\n",
    "creative_id_df.to_csv('creative_id_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('素材id、广告id、产品id、产品类目id、广告主id、广告主行业id ')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label'] = df_train['age']+10*df_train['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in df_train['user_id']:\n",
    "    df_train[df_train['id']==ids][creative_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_train['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_train['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(df_train[df_train['product_id']=='\\\\N']) #1259, 3112\n",
    "\n",
    "len(df_train[df_train['industry']=='\\\\N']) # 127,6874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['user_id','age','gender'], 1)\n",
    "Y = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "train_data = lgb.Dataset(X, label=Y)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,Y,test_size=0.2)\n",
    "xgtrain = lgb.Dataset(X_train, y_train)\n",
    "xgvalid = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "\n",
    "\n",
    "w = np.random.rand(30082771, )\n",
    "# train_data = lgb.Dataset(data, label=label, feature_name=['c1', 'c2', 'c3'], \n",
    "#                    categorical_feature=['c3'],weight=w)\n",
    "\n",
    "\n",
    "train_data.set_weight(w)\n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary', #xentlambda\n",
    "    'metric': 'auc',\n",
    "    'silent':0,\n",
    "    'learning_rate': 0.05,\n",
    "    'is_unbalance': 'true',  #当训练数据是不平衡的，正负样本相差悬殊的时候，可以将这个属性设为true,此时会自动给少的样本赋予更高的权重\n",
    "    'num_leaves': 64,  # 一般设为少于2^(max_depth)\n",
    "    'max_depth': -1,  #最大的树深，设为-1时表示不限制树的深度\n",
    "    'min_child_samples': 15,  # 每个叶子结点最少包含的样本数量，用于正则化，避免过拟合\n",
    "    'max_bin': 200,  # 设置连续特征或大量类型的离散特征的bins的数量\n",
    "    'subsample': 0.8,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.5,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    #'scale_pos_weight':100,\n",
    "    'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "    'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "    'reg_alpha': 2.99,  # L1 regularization term on weights\n",
    "    'reg_lambda': 1.9,  # L2 regularization term on weights\n",
    "    'nthread': 10,\n",
    "    'verbose': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval_spec(preds, train_data):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, threshold = roc_curve(train_data.get_label(), preds)\n",
    "    tpr0001 = tpr[fpr <= 0.0005].max()\n",
    "    tpr001 = tpr[fpr <= 0.001].max()\n",
    "    tpr005 = tpr[fpr <= 0.005].max()\n",
    "    #tpr01 = tpr[fpr.values <= 0.01].max()\n",
    "    tprcal = 0.4 * tpr0001 + 0.3 * tpr001 + 0.3 * tpr005\n",
    "    return 'spec_cal',tprcal,True\n",
    "\n",
    "\n",
    "\n",
    "num_round = 10\n",
    "bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "lgb.cv(param, train_data, num_round, nfold=5)\n",
    "\n",
    "bst = lgb.train(param, train_data, num_round, valid_sets=valid_sets, \n",
    "      early_stopping_rounds=10)\n",
    "bst.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "data = np.random.rand(7, 10)\n",
    "ypred = bst.predict(data)\n",
    "\n",
    "ypred = bst.predict(data, num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "- tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "creation = pd.read_csv('creative_id_df.csv')\n",
    "creation.head()\n",
    "\n",
    "sentences=[]\n",
    "for item in creation['sentence']:\n",
    "    #sent = ''.join(item[1:-1].split(','))\n",
    "    sent = list(item[1:-1].replace(\"'\",'').split(','))\n",
    "    sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:55:48: collecting all words and their counts\n",
      "INFO - 22:55:48: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 22:55:48: PROGRESS: at sentence #10000, processed 335578 words, keeping 153379 word types\n",
      "INFO - 22:55:48: PROGRESS: at sentence #20000, processed 673775 words, keeping 259099 word types\n",
      "INFO - 22:55:48: PROGRESS: at sentence #30000, processed 1012932 words, keeping 347138 word types\n",
      "INFO - 22:55:48: PROGRESS: at sentence #40000, processed 1344506 words, keeping 423601 word types\n",
      "INFO - 22:55:48: PROGRESS: at sentence #50000, processed 1683247 words, keeping 495547 word types\n",
      "INFO - 22:55:48: PROGRESS: at sentence #60000, processed 2018755 words, keeping 560179 word types\n",
      "INFO - 22:55:48: PROGRESS: at sentence #70000, processed 2356409 words, keeping 621113 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #80000, processed 2694811 words, keeping 677894 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #90000, processed 3029095 words, keeping 731004 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #100000, processed 3367959 words, keeping 781884 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #110000, processed 3697062 words, keeping 828847 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #120000, processed 4029212 words, keeping 874986 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #130000, processed 4361118 words, keeping 918683 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #140000, processed 4696305 words, keeping 961254 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #150000, processed 5027376 words, keeping 1001434 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #160000, processed 5362170 words, keeping 1041494 word types\n",
      "INFO - 22:55:49: PROGRESS: at sentence #170000, processed 5694917 words, keeping 1080227 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #180000, processed 6028866 words, keeping 1118092 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #190000, processed 6363699 words, keeping 1154447 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #200000, processed 6700006 words, keeping 1190006 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #210000, processed 7029302 words, keeping 1223463 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #220000, processed 7364072 words, keeping 1257068 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #230000, processed 7703111 words, keeping 1290548 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #240000, processed 8038922 words, keeping 1322155 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #250000, processed 8374730 words, keeping 1354159 word types\n",
      "INFO - 22:55:50: PROGRESS: at sentence #260000, processed 8703381 words, keeping 1383935 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #270000, processed 9034269 words, keeping 1413552 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #280000, processed 9369709 words, keeping 1441748 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #290000, processed 9699127 words, keeping 1469784 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #300000, processed 10038041 words, keeping 1498429 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #310000, processed 10368267 words, keeping 1525759 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #320000, processed 10704686 words, keeping 1553305 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #330000, processed 11036042 words, keeping 1578997 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #340000, processed 11376443 words, keeping 1605411 word types\n",
      "INFO - 22:55:51: PROGRESS: at sentence #350000, processed 11710752 words, keeping 1630822 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #360000, processed 12043735 words, keeping 1655528 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #370000, processed 12376522 words, keeping 1679772 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #380000, processed 12713042 words, keeping 1703966 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #390000, processed 13048871 words, keeping 1727784 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #400000, processed 13383120 words, keeping 1751395 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #410000, processed 13713507 words, keeping 1774974 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #420000, processed 14046448 words, keeping 1797821 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #430000, processed 14379776 words, keeping 1820625 word types\n",
      "INFO - 22:55:52: PROGRESS: at sentence #440000, processed 14714532 words, keeping 1843026 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #450000, processed 15047917 words, keeping 1865086 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #460000, processed 15388077 words, keeping 1887102 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #470000, processed 15716758 words, keeping 1908002 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #480000, processed 16047153 words, keeping 1928724 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #490000, processed 16380413 words, keeping 1949874 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #500000, processed 16713729 words, keeping 1970435 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #510000, processed 17040938 words, keeping 1989792 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #520000, processed 17380478 words, keeping 2010220 word types\n",
      "INFO - 22:55:53: PROGRESS: at sentence #530000, processed 17713633 words, keeping 2030244 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #540000, processed 18050044 words, keeping 2049710 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #550000, processed 18385676 words, keeping 2069026 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #560000, processed 18714971 words, keeping 2087588 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #570000, processed 19044921 words, keeping 2105739 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #580000, processed 19380521 words, keeping 2124064 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #590000, processed 19709975 words, keeping 2142118 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #600000, processed 20047563 words, keeping 2160404 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #610000, processed 20376153 words, keeping 2178041 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #620000, processed 20711580 words, keeping 2195568 word types\n",
      "INFO - 22:55:54: PROGRESS: at sentence #630000, processed 21044761 words, keeping 2213145 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #640000, processed 21378841 words, keeping 2230698 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #650000, processed 21715471 words, keeping 2247843 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #660000, processed 22048734 words, keeping 2264410 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #670000, processed 22386196 words, keeping 2281242 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #680000, processed 22722812 words, keeping 2298538 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #690000, processed 23061046 words, keeping 2315536 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #700000, processed 23397501 words, keeping 2332070 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #710000, processed 23733177 words, keeping 2348364 word types\n",
      "INFO - 22:55:55: PROGRESS: at sentence #720000, processed 24066390 words, keeping 2364316 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #730000, processed 24404756 words, keeping 2380499 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #740000, processed 24737990 words, keeping 2396407 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #750000, processed 25070279 words, keeping 2411725 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #760000, processed 25401959 words, keeping 2427265 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #770000, processed 25737883 words, keeping 2443086 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #780000, processed 26071729 words, keeping 2458193 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #790000, processed 26404351 words, keeping 2473125 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #800000, processed 26735949 words, keeping 2487715 word types\n",
      "INFO - 22:55:56: PROGRESS: at sentence #810000, processed 27072153 words, keeping 2502744 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:55:57: PROGRESS: at sentence #820000, processed 27402382 words, keeping 2517129 word types\n",
      "INFO - 22:55:57: PROGRESS: at sentence #830000, processed 27732476 words, keeping 2531438 word types\n",
      "INFO - 22:55:57: PROGRESS: at sentence #840000, processed 28080276 words, keeping 2546875 word types\n",
      "INFO - 22:55:57: PROGRESS: at sentence #850000, processed 28417412 words, keeping 2561301 word types\n",
      "INFO - 22:55:57: PROGRESS: at sentence #860000, processed 28751550 words, keeping 2575644 word types\n",
      "INFO - 22:55:57: PROGRESS: at sentence #870000, processed 29085965 words, keeping 2589918 word types\n",
      "INFO - 22:55:57: PROGRESS: at sentence #880000, processed 29420540 words, keeping 2603695 word types\n",
      "INFO - 22:55:57: PROGRESS: at sentence #890000, processed 29751798 words, keeping 2617206 word types\n",
      "INFO - 22:55:57: collected 2630643 word types from a corpus of 30082771 raw words and 900000 sentences\n",
      "INFO - 22:55:57: Loading a fresh vocabulary\n",
      "INFO - 22:56:05: effective_min_count=1 retains 2630643 unique words (100% of original 2630643, drops 0)\n",
      "INFO - 22:56:05: effective_min_count=1 leaves 30082771 word corpus (100% of original 30082771, drops 0)\n",
      "INFO - 22:56:11: deleting the raw counts dictionary of 2630643 items\n",
      "INFO - 22:56:11: sample=6e-05 downsamples 416 most-common words\n",
      "INFO - 22:56:11: downsampling leaves estimated 27742312 word corpus (92.2% of prior 30082771)\n",
      "INFO - 22:56:18: estimated required memory for 2630643 words and 300 dimensions: 7628864700 bytes\n",
      "INFO - 22:56:18: resetting layer weights\n",
      "INFO - 22:56:50: training model with 5 workers on 2630643 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 1.03 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:56:51: EPOCH 1 - PROGRESS: at 0.72% examples, 201176 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:56:52: EPOCH 1 - PROGRESS: at 1.72% examples, 236158 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:56:53: EPOCH 1 - PROGRESS: at 2.72% examples, 250639 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:56:54: EPOCH 1 - PROGRESS: at 3.76% examples, 258530 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:56:55: EPOCH 1 - PROGRESS: at 4.71% examples, 259007 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:56:56: EPOCH 1 - PROGRESS: at 5.63% examples, 258470 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:56:57: EPOCH 1 - PROGRESS: at 6.65% examples, 260424 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:56:58: EPOCH 1 - PROGRESS: at 7.66% examples, 261870 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:56:59: EPOCH 1 - PROGRESS: at 8.65% examples, 262597 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:00: EPOCH 1 - PROGRESS: at 9.65% examples, 263851 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:01: EPOCH 1 - PROGRESS: at 10.77% examples, 268166 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:02: EPOCH 1 - PROGRESS: at 11.80% examples, 267394 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:03: EPOCH 1 - PROGRESS: at 12.96% examples, 268984 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:04: EPOCH 1 - PROGRESS: at 14.03% examples, 270628 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:05: EPOCH 1 - PROGRESS: at 15.03% examples, 270967 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 22:57:06: EPOCH 1 - PROGRESS: at 16.05% examples, 271804 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:07: EPOCH 1 - PROGRESS: at 17.13% examples, 271679 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:08: EPOCH 1 - PROGRESS: at 18.29% examples, 272609 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:09: EPOCH 1 - PROGRESS: at 19.29% examples, 272577 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:10: EPOCH 1 - PROGRESS: at 20.37% examples, 274025 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:57:11: EPOCH 1 - PROGRESS: at 21.44% examples, 273847 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:12: EPOCH 1 - PROGRESS: at 22.59% examples, 274560 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:14: EPOCH 1 - PROGRESS: at 23.59% examples, 274156 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:15: EPOCH 1 - PROGRESS: at 24.62% examples, 274003 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:16: EPOCH 1 - PROGRESS: at 25.60% examples, 273869 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:17: EPOCH 1 - PROGRESS: at 26.55% examples, 273400 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:18: EPOCH 1 - PROGRESS: at 27.59% examples, 273399 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:19: EPOCH 1 - PROGRESS: at 28.66% examples, 273271 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:20: EPOCH 1 - PROGRESS: at 29.66% examples, 273160 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:21: EPOCH 1 - PROGRESS: at 30.66% examples, 273036 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:22: EPOCH 1 - PROGRESS: at 31.80% examples, 274176 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:23: EPOCH 1 - PROGRESS: at 32.82% examples, 273513 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:24: EPOCH 1 - PROGRESS: at 33.88% examples, 274081 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:25: EPOCH 1 - PROGRESS: at 34.91% examples, 274345 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:26: EPOCH 1 - PROGRESS: at 35.90% examples, 274275 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:57:27: EPOCH 1 - PROGRESS: at 36.97% examples, 274081 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:28: EPOCH 1 - PROGRESS: at 38.01% examples, 274587 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:29: EPOCH 1 - PROGRESS: at 39.08% examples, 275051 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:30: EPOCH 1 - PROGRESS: at 40.10% examples, 274804 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:31: EPOCH 1 - PROGRESS: at 41.28% examples, 275138 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:32: EPOCH 1 - PROGRESS: at 42.43% examples, 275448 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:33: EPOCH 1 - PROGRESS: at 43.58% examples, 275776 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:57:35: EPOCH 1 - PROGRESS: at 44.74% examples, 276037 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:36: EPOCH 1 - PROGRESS: at 45.88% examples, 276831 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:37: EPOCH 1 - PROGRESS: at 46.91% examples, 276479 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:38: EPOCH 1 - PROGRESS: at 48.06% examples, 276714 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 22:57:39: EPOCH 1 - PROGRESS: at 49.06% examples, 276507 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:40: EPOCH 1 - PROGRESS: at 50.06% examples, 276190 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:41: EPOCH 1 - PROGRESS: at 51.03% examples, 276011 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:57:42: EPOCH 1 - PROGRESS: at 52.11% examples, 276354 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:43: EPOCH 1 - PROGRESS: at 53.02% examples, 275767 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:44: EPOCH 1 - PROGRESS: at 54.01% examples, 275650 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:45: EPOCH 1 - PROGRESS: at 55.05% examples, 275769 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:46: EPOCH 1 - PROGRESS: at 56.08% examples, 275519 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:47: EPOCH 1 - PROGRESS: at 57.12% examples, 275638 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:57:48: EPOCH 1 - PROGRESS: at 58.20% examples, 276104 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:49: EPOCH 1 - PROGRESS: at 59.26% examples, 275996 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:50: EPOCH 1 - PROGRESS: at 60.38% examples, 276608 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:51: EPOCH 1 - PROGRESS: at 61.41% examples, 276368 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:52: EPOCH 1 - PROGRESS: at 62.56% examples, 276938 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:53: EPOCH 1 - PROGRESS: at 63.58% examples, 276667 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:57:54: EPOCH 1 - PROGRESS: at 64.76% examples, 276857 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:57:56: EPOCH 1 - PROGRESS: at 65.92% examples, 277019 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:57:57: EPOCH 1 - PROGRESS: at 67.04% examples, 277510 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:57:58: EPOCH 1 - PROGRESS: at 68.08% examples, 277283 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:57:59: EPOCH 1 - PROGRESS: at 69.21% examples, 277781 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:00: EPOCH 1 - PROGRESS: at 70.24% examples, 277526 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:58:01: EPOCH 1 - PROGRESS: at 71.39% examples, 277687 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:58:02: EPOCH 1 - PROGRESS: at 72.52% examples, 278163 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:03: EPOCH 1 - PROGRESS: at 73.54% examples, 277941 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:04: EPOCH 1 - PROGRESS: at 74.69% examples, 278077 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:05: EPOCH 1 - PROGRESS: at 75.84% examples, 278217 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:06: EPOCH 1 - PROGRESS: at 76.96% examples, 278668 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:07: EPOCH 1 - PROGRESS: at 77.97% examples, 278461 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:08: EPOCH 1 - PROGRESS: at 79.13% examples, 278578 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:09: EPOCH 1 - PROGRESS: at 80.26% examples, 278992 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:11: EPOCH 1 - PROGRESS: at 81.27% examples, 278775 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:58:12: EPOCH 1 - PROGRESS: at 82.44% examples, 278891 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:13: EPOCH 1 - PROGRESS: at 83.57% examples, 279289 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:14: EPOCH 1 - PROGRESS: at 84.60% examples, 279083 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:15: EPOCH 1 - PROGRESS: at 85.66% examples, 279257 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:16: EPOCH 1 - PROGRESS: at 86.76% examples, 279216 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:17: EPOCH 1 - PROGRESS: at 87.89% examples, 279568 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:18: EPOCH 1 - PROGRESS: at 88.93% examples, 279385 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:19: EPOCH 1 - PROGRESS: at 90.05% examples, 279756 words/s, in_qsize 9, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:58:20: EPOCH 1 - PROGRESS: at 91.08% examples, 279553 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:21: EPOCH 1 - PROGRESS: at 92.23% examples, 279899 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:22: EPOCH 1 - PROGRESS: at 93.26% examples, 279717 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:23: EPOCH 1 - PROGRESS: at 94.33% examples, 279957 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:24: EPOCH 1 - PROGRESS: at 95.38% examples, 279866 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:25: EPOCH 1 - PROGRESS: at 96.54% examples, 279947 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:26: EPOCH 1 - PROGRESS: at 97.69% examples, 280037 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 22:58:28: EPOCH 1 - PROGRESS: at 98.86% examples, 280127 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:58:28: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:58:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:58:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:58:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:58:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:58:28: EPOCH - 1 : training on 30082771 raw words (27737417 effective words) took 98.9s, 280511 effective words/s\n",
      "INFO - 22:58:30: EPOCH 2 - PROGRESS: at 1.01% examples, 265494 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:31: EPOCH 2 - PROGRESS: at 2.14% examples, 284418 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:32: EPOCH 2 - PROGRESS: at 3.18% examples, 287108 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:33: EPOCH 2 - PROGRESS: at 4.29% examples, 286384 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:34: EPOCH 2 - PROGRESS: at 5.43% examples, 286364 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:35: EPOCH 2 - PROGRESS: at 6.58% examples, 287267 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:36: EPOCH 2 - PROGRESS: at 7.73% examples, 287527 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:37: EPOCH 2 - PROGRESS: at 8.88% examples, 287795 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:38: EPOCH 2 - PROGRESS: at 10.04% examples, 288142 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:39: EPOCH 2 - PROGRESS: at 11.19% examples, 288300 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:40: EPOCH 2 - PROGRESS: at 12.36% examples, 288531 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:42: EPOCH 2 - PROGRESS: at 13.54% examples, 288704 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:58:43: EPOCH 2 - PROGRESS: at 14.70% examples, 288886 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:44: EPOCH 2 - PROGRESS: at 15.85% examples, 288992 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:45: EPOCH 2 - PROGRESS: at 17.03% examples, 289125 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:46: EPOCH 2 - PROGRESS: at 18.19% examples, 289197 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:47: EPOCH 2 - PROGRESS: at 19.35% examples, 289301 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:48: EPOCH 2 - PROGRESS: at 20.50% examples, 289389 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:49: EPOCH 2 - PROGRESS: at 21.67% examples, 289605 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:50: EPOCH 2 - PROGRESS: at 22.77% examples, 290220 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:51: EPOCH 2 - PROGRESS: at 23.82% examples, 290344 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:52: EPOCH 2 - PROGRESS: at 24.85% examples, 289474 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:53: EPOCH 2 - PROGRESS: at 26.00% examples, 289500 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:55: EPOCH 2 - PROGRESS: at 27.16% examples, 289581 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:56: EPOCH 2 - PROGRESS: at 28.32% examples, 289636 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 22:58:57: EPOCH 2 - PROGRESS: at 29.49% examples, 289709 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:58:58: EPOCH 2 - PROGRESS: at 30.66% examples, 289758 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:58:59: EPOCH 2 - PROGRESS: at 31.83% examples, 289791 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:00: EPOCH 2 - PROGRESS: at 32.98% examples, 289768 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:01: EPOCH 2 - PROGRESS: at 34.15% examples, 289787 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:02: EPOCH 2 - PROGRESS: at 35.30% examples, 289826 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:03: EPOCH 2 - PROGRESS: at 36.47% examples, 289840 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 22:59:05: EPOCH 2 - PROGRESS: at 37.62% examples, 289893 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:06: EPOCH 2 - PROGRESS: at 38.77% examples, 289895 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:07: EPOCH 2 - PROGRESS: at 39.94% examples, 289932 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:08: EPOCH 2 - PROGRESS: at 41.11% examples, 289970 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 22:59:09: EPOCH 2 - PROGRESS: at 42.26% examples, 289957 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:10: EPOCH 2 - PROGRESS: at 43.41% examples, 289983 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:11: EPOCH 2 - PROGRESS: at 44.57% examples, 290017 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:12: EPOCH 2 - PROGRESS: at 45.74% examples, 290026 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:13: EPOCH 2 - PROGRESS: at 46.91% examples, 290039 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:14: EPOCH 2 - PROGRESS: at 48.06% examples, 290063 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:16: EPOCH 2 - PROGRESS: at 49.22% examples, 290103 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:17: EPOCH 2 - PROGRESS: at 50.39% examples, 290128 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:18: EPOCH 2 - PROGRESS: at 51.54% examples, 290132 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:19: EPOCH 2 - PROGRESS: at 52.72% examples, 290138 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:20: EPOCH 2 - PROGRESS: at 53.87% examples, 290145 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:21: EPOCH 2 - PROGRESS: at 55.05% examples, 290158 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:22: EPOCH 2 - PROGRESS: at 56.21% examples, 290156 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:23: EPOCH 2 - PROGRESS: at 57.38% examples, 290162 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:24: EPOCH 2 - PROGRESS: at 58.53% examples, 290194 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:26: EPOCH 2 - PROGRESS: at 59.69% examples, 290196 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:27: EPOCH 2 - PROGRESS: at 60.85% examples, 290211 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:28: EPOCH 2 - PROGRESS: at 62.01% examples, 290261 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:29: EPOCH 2 - PROGRESS: at 63.19% examples, 290282 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:30: EPOCH 2 - PROGRESS: at 64.34% examples, 290288 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:31: EPOCH 2 - PROGRESS: at 65.53% examples, 290320 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:32: EPOCH 2 - PROGRESS: at 66.67% examples, 290361 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:33: EPOCH 2 - PROGRESS: at 67.85% examples, 290387 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:34: EPOCH 2 - PROGRESS: at 69.01% examples, 290400 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 22:59:35: EPOCH 2 - PROGRESS: at 70.17% examples, 290411 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:37: EPOCH 2 - PROGRESS: at 71.33% examples, 290426 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:38: EPOCH 2 - PROGRESS: at 72.48% examples, 290453 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:39: EPOCH 2 - PROGRESS: at 73.64% examples, 290489 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:40: EPOCH 2 - PROGRESS: at 74.79% examples, 290505 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:41: EPOCH 2 - PROGRESS: at 75.94% examples, 290545 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:42: EPOCH 2 - PROGRESS: at 77.09% examples, 290542 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:43: EPOCH 2 - PROGRESS: at 78.23% examples, 290573 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:44: EPOCH 2 - PROGRESS: at 79.40% examples, 290583 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 22:59:45: EPOCH 2 - PROGRESS: at 80.55% examples, 290612 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:46: EPOCH 2 - PROGRESS: at 81.71% examples, 290645 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:48: EPOCH 2 - PROGRESS: at 82.88% examples, 290667 words/s, in_qsize 10, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:59:49: EPOCH 2 - PROGRESS: at 84.04% examples, 290694 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 22:59:50: EPOCH 2 - PROGRESS: at 85.20% examples, 290693 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:51: EPOCH 2 - PROGRESS: at 86.35% examples, 290720 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:52: EPOCH 2 - PROGRESS: at 87.52% examples, 290740 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:53: EPOCH 2 - PROGRESS: at 88.70% examples, 290723 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:54: EPOCH 2 - PROGRESS: at 89.85% examples, 290745 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:55: EPOCH 2 - PROGRESS: at 91.02% examples, 290763 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 22:59:56: EPOCH 2 - PROGRESS: at 92.19% examples, 290751 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 22:59:58: EPOCH 2 - PROGRESS: at 93.33% examples, 290782 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 22:59:59: EPOCH 2 - PROGRESS: at 94.48% examples, 290799 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:00:00: EPOCH 2 - PROGRESS: at 95.64% examples, 290797 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:01: EPOCH 2 - PROGRESS: at 96.80% examples, 290826 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:00:02: EPOCH 2 - PROGRESS: at 97.95% examples, 290854 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:03: EPOCH 2 - PROGRESS: at 99.12% examples, 290874 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:00:04: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:00:04: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:00:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:00:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:00:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:00:04: EPOCH - 2 : training on 30082771 raw words (27735293 effective words) took 95.3s, 291112 effective words/s\n",
      "INFO - 23:00:05: EPOCH 3 - PROGRESS: at 1.01% examples, 271276 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:06: EPOCH 3 - PROGRESS: at 2.17% examples, 286027 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:07: EPOCH 3 - PROGRESS: at 3.21% examples, 285294 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:08: EPOCH 3 - PROGRESS: at 4.29% examples, 286346 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:09: EPOCH 3 - PROGRESS: at 5.33% examples, 287743 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:10: EPOCH 3 - PROGRESS: at 6.42% examples, 288806 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:11: EPOCH 3 - PROGRESS: at 7.54% examples, 290536 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:12: EPOCH 3 - PROGRESS: at 8.55% examples, 289540 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:13: EPOCH 3 - PROGRESS: at 9.72% examples, 290214 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:14: EPOCH 3 - PROGRESS: at 10.87% examples, 290699 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:15: EPOCH 3 - PROGRESS: at 12.03% examples, 291213 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:16: EPOCH 3 - PROGRESS: at 13.20% examples, 291674 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:17: EPOCH 3 - PROGRESS: at 14.37% examples, 291950 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:19: EPOCH 3 - PROGRESS: at 15.52% examples, 292178 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:20: EPOCH 3 - PROGRESS: at 16.70% examples, 292392 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:21: EPOCH 3 - PROGRESS: at 17.85% examples, 292519 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:22: EPOCH 3 - PROGRESS: at 19.02% examples, 292786 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:23: EPOCH 3 - PROGRESS: at 20.18% examples, 292968 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:24: EPOCH 3 - PROGRESS: at 21.34% examples, 293090 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:25: EPOCH 3 - PROGRESS: at 22.49% examples, 293236 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:26: EPOCH 3 - PROGRESS: at 23.66% examples, 293407 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:27: EPOCH 3 - PROGRESS: at 24.81% examples, 293317 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:00:28: EPOCH 3 - PROGRESS: at 25.97% examples, 293349 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:00:29: EPOCH 3 - PROGRESS: at 27.13% examples, 293445 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:31: EPOCH 3 - PROGRESS: at 28.29% examples, 293549 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:32: EPOCH 3 - PROGRESS: at 29.46% examples, 293627 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:33: EPOCH 3 - PROGRESS: at 30.62% examples, 293717 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 23:00:34: EPOCH 3 - PROGRESS: at 31.80% examples, 293598 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:35: EPOCH 3 - PROGRESS: at 32.95% examples, 293660 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:36: EPOCH 3 - PROGRESS: at 34.12% examples, 293613 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:37: EPOCH 3 - PROGRESS: at 35.27% examples, 293640 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:38: EPOCH 3 - PROGRESS: at 36.44% examples, 293736 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:39: EPOCH 3 - PROGRESS: at 37.59% examples, 293755 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:40: EPOCH 3 - PROGRESS: at 38.74% examples, 293808 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:41: EPOCH 3 - PROGRESS: at 39.91% examples, 293917 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:43: EPOCH 3 - PROGRESS: at 41.07% examples, 293958 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:44: EPOCH 3 - PROGRESS: at 42.22% examples, 293975 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:45: EPOCH 3 - PROGRESS: at 43.38% examples, 294039 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:00:46: EPOCH 3 - PROGRESS: at 44.54% examples, 294122 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:47: EPOCH 3 - PROGRESS: at 45.71% examples, 294137 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:48: EPOCH 3 - PROGRESS: at 46.88% examples, 294196 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:49: EPOCH 3 - PROGRESS: at 48.03% examples, 294224 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:00:50: EPOCH 3 - PROGRESS: at 49.19% examples, 294245 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:51: EPOCH 3 - PROGRESS: at 50.36% examples, 294283 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:52: EPOCH 3 - PROGRESS: at 51.51% examples, 294303 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:53: EPOCH 3 - PROGRESS: at 52.69% examples, 294323 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:55: EPOCH 3 - PROGRESS: at 53.84% examples, 294331 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:56: EPOCH 3 - PROGRESS: at 54.95% examples, 294465 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:57: EPOCH 3 - PROGRESS: at 56.01% examples, 294098 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:00:58: EPOCH 3 - PROGRESS: at 57.06% examples, 293900 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:00:59: EPOCH 3 - PROGRESS: at 58.07% examples, 293725 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:00: EPOCH 3 - PROGRESS: at 59.16% examples, 293388 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:01: EPOCH 3 - PROGRESS: at 60.31% examples, 293407 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:02: EPOCH 3 - PROGRESS: at 61.48% examples, 293447 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:03: EPOCH 3 - PROGRESS: at 62.66% examples, 293507 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:04: EPOCH 3 - PROGRESS: at 63.82% examples, 293569 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:05: EPOCH 3 - PROGRESS: at 64.98% examples, 293608 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:06: EPOCH 3 - PROGRESS: at 66.15% examples, 293648 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:07: EPOCH 3 - PROGRESS: at 67.31% examples, 293674 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 23:01:08: EPOCH 3 - PROGRESS: at 68.48% examples, 293718 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:10: EPOCH 3 - PROGRESS: at 69.64% examples, 293751 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:11: EPOCH 3 - PROGRESS: at 70.80% examples, 293781 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:12: EPOCH 3 - PROGRESS: at 71.96% examples, 293812 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:13: EPOCH 3 - PROGRESS: at 73.12% examples, 293874 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:14: EPOCH 3 - PROGRESS: at 74.27% examples, 293911 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:15: EPOCH 3 - PROGRESS: at 75.42% examples, 293924 words/s, in_qsize 9, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:01:16: EPOCH 3 - PROGRESS: at 76.56% examples, 293909 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:17: EPOCH 3 - PROGRESS: at 77.71% examples, 293940 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:18: EPOCH 3 - PROGRESS: at 78.87% examples, 293854 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:19: EPOCH 3 - PROGRESS: at 80.03% examples, 293867 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:01:20: EPOCH 3 - PROGRESS: at 81.18% examples, 293894 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:22: EPOCH 3 - PROGRESS: at 82.34% examples, 293911 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:23: EPOCH 3 - PROGRESS: at 83.51% examples, 293956 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:24: EPOCH 3 - PROGRESS: at 84.67% examples, 294001 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:25: EPOCH 3 - PROGRESS: at 85.82% examples, 294015 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:26: EPOCH 3 - PROGRESS: at 87.00% examples, 294051 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:27: EPOCH 3 - PROGRESS: at 88.16% examples, 294069 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:01:28: EPOCH 3 - PROGRESS: at 89.32% examples, 294103 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:29: EPOCH 3 - PROGRESS: at 90.48% examples, 294137 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:01:30: EPOCH 3 - PROGRESS: at 91.65% examples, 294157 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:31: EPOCH 3 - PROGRESS: at 92.83% examples, 294187 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:32: EPOCH 3 - PROGRESS: at 93.97% examples, 294219 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:33: EPOCH 3 - PROGRESS: at 95.11% examples, 294258 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:35: EPOCH 3 - PROGRESS: at 96.28% examples, 294279 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:36: EPOCH 3 - PROGRESS: at 97.43% examples, 294300 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:37: EPOCH 3 - PROGRESS: at 98.60% examples, 294332 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:38: EPOCH 3 - PROGRESS: at 99.76% examples, 294358 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 23:01:38: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:01:38: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:01:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:01:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:01:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:01:38: EPOCH - 3 : training on 30082771 raw words (27736254 effective words) took 94.1s, 294623 effective words/s\n",
      "INFO - 23:01:39: EPOCH 4 - PROGRESS: at 1.01% examples, 276643 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:40: EPOCH 4 - PROGRESS: at 2.14% examples, 293352 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:41: EPOCH 4 - PROGRESS: at 3.21% examples, 295846 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:42: EPOCH 4 - PROGRESS: at 4.32% examples, 296928 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:43: EPOCH 4 - PROGRESS: at 5.39% examples, 295822 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:44: EPOCH 4 - PROGRESS: at 6.55% examples, 296224 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:45: EPOCH 4 - PROGRESS: at 7.70% examples, 296783 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:46: EPOCH 4 - PROGRESS: at 8.85% examples, 297282 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:01:47: EPOCH 4 - PROGRESS: at 10.00% examples, 297226 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:01:48: EPOCH 4 - PROGRESS: at 10.99% examples, 294996 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:49: EPOCH 4 - PROGRESS: at 12.06% examples, 294648 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:50: EPOCH 4 - PROGRESS: at 13.09% examples, 291899 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:51: EPOCH 4 - PROGRESS: at 14.13% examples, 290779 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:52: EPOCH 4 - PROGRESS: at 15.16% examples, 290116 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:54: EPOCH 4 - PROGRESS: at 16.26% examples, 290342 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:01:55: EPOCH 4 - PROGRESS: at 17.42% examples, 290797 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:01:56: EPOCH 4 - PROGRESS: at 18.58% examples, 291270 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:57: EPOCH 4 - PROGRESS: at 19.74% examples, 291671 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:01:58: EPOCH 4 - PROGRESS: at 20.90% examples, 292052 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:01:59: EPOCH 4 - PROGRESS: at 22.06% examples, 292383 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:02:00: EPOCH 4 - PROGRESS: at 23.23% examples, 292502 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:01: EPOCH 4 - PROGRESS: at 24.39% examples, 292736 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:02: EPOCH 4 - PROGRESS: at 25.50% examples, 293521 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:03: EPOCH 4 - PROGRESS: at 26.52% examples, 293019 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:04: EPOCH 4 - PROGRESS: at 27.68% examples, 293183 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:05: EPOCH 4 - PROGRESS: at 28.86% examples, 293336 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:06: EPOCH 4 - PROGRESS: at 30.03% examples, 293466 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:07: EPOCH 4 - PROGRESS: at 31.09% examples, 293433 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:08: EPOCH 4 - PROGRESS: at 32.13% examples, 293103 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:09: EPOCH 4 - PROGRESS: at 33.08% examples, 292234 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:10: EPOCH 4 - PROGRESS: at 34.12% examples, 291990 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:11: EPOCH 4 - PROGRESS: at 35.04% examples, 290959 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:02:12: EPOCH 4 - PROGRESS: at 36.18% examples, 291318 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:13: EPOCH 4 - PROGRESS: at 37.23% examples, 291282 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:14: EPOCH 4 - PROGRESS: at 38.34% examples, 291372 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:16: EPOCH 4 - PROGRESS: at 39.51% examples, 291576 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:17: EPOCH 4 - PROGRESS: at 40.67% examples, 291792 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:18: EPOCH 4 - PROGRESS: at 41.83% examples, 291964 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:19: EPOCH 4 - PROGRESS: at 42.99% examples, 292199 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:20: EPOCH 4 - PROGRESS: at 44.14% examples, 292239 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:21: EPOCH 4 - PROGRESS: at 45.25% examples, 292338 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:22: EPOCH 4 - PROGRESS: at 46.31% examples, 292136 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:23: EPOCH 4 - PROGRESS: at 47.47% examples, 292311 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:02:24: EPOCH 4 - PROGRESS: at 48.63% examples, 292463 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:25: EPOCH 4 - PROGRESS: at 49.79% examples, 292622 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:02:26: EPOCH 4 - PROGRESS: at 50.93% examples, 292759 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:27: EPOCH 4 - PROGRESS: at 52.11% examples, 292893 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:02:28: EPOCH 4 - PROGRESS: at 53.28% examples, 293036 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:29: EPOCH 4 - PROGRESS: at 54.45% examples, 293145 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:31: EPOCH 4 - PROGRESS: at 55.61% examples, 293228 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:02:32: EPOCH 4 - PROGRESS: at 56.79% examples, 293358 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:33: EPOCH 4 - PROGRESS: at 57.94% examples, 293475 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:34: EPOCH 4 - PROGRESS: at 59.10% examples, 293507 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:35: EPOCH 4 - PROGRESS: at 60.25% examples, 293600 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:36: EPOCH 4 - PROGRESS: at 61.41% examples, 293662 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:02:37: EPOCH 4 - PROGRESS: at 62.59% examples, 293732 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:02:38: EPOCH 4 - PROGRESS: at 63.75% examples, 293821 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:39: EPOCH 4 - PROGRESS: at 64.92% examples, 293877 words/s, in_qsize 9, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:02:40: EPOCH 4 - PROGRESS: at 66.08% examples, 293972 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:41: EPOCH 4 - PROGRESS: at 67.25% examples, 294072 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:42: EPOCH 4 - PROGRESS: at 68.41% examples, 294183 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:43: EPOCH 4 - PROGRESS: at 69.57% examples, 294306 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:45: EPOCH 4 - PROGRESS: at 70.73% examples, 294393 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:46: EPOCH 4 - PROGRESS: at 71.89% examples, 294488 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:47: EPOCH 4 - PROGRESS: at 73.05% examples, 294566 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:48: EPOCH 4 - PROGRESS: at 74.17% examples, 294792 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:49: EPOCH 4 - PROGRESS: at 75.26% examples, 294710 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:50: EPOCH 4 - PROGRESS: at 76.33% examples, 294689 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:02:51: EPOCH 4 - PROGRESS: at 77.48% examples, 294740 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:52: EPOCH 4 - PROGRESS: at 78.63% examples, 294763 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:02:53: EPOCH 4 - PROGRESS: at 79.79% examples, 294779 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:54: EPOCH 4 - PROGRESS: at 80.94% examples, 294831 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:02:55: EPOCH 4 - PROGRESS: at 82.11% examples, 294892 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 23:02:56: EPOCH 4 - PROGRESS: at 83.28% examples, 294955 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:02:57: EPOCH 4 - PROGRESS: at 84.44% examples, 295006 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:02:58: EPOCH 4 - PROGRESS: at 85.60% examples, 295052 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:02:59: EPOCH 4 - PROGRESS: at 86.76% examples, 295104 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:01: EPOCH 4 - PROGRESS: at 87.93% examples, 295124 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:02: EPOCH 4 - PROGRESS: at 89.09% examples, 295177 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:03: EPOCH 4 - PROGRESS: at 90.24% examples, 295213 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:03:04: EPOCH 4 - PROGRESS: at 91.42% examples, 295269 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:05: EPOCH 4 - PROGRESS: at 92.59% examples, 295322 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:06: EPOCH 4 - PROGRESS: at 93.73% examples, 295449 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:07: EPOCH 4 - PROGRESS: at 94.88% examples, 295508 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:08: EPOCH 4 - PROGRESS: at 96.04% examples, 295551 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:09: EPOCH 4 - PROGRESS: at 97.09% examples, 295352 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:10: EPOCH 4 - PROGRESS: at 98.16% examples, 295338 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:11: EPOCH 4 - PROGRESS: at 99.26% examples, 295284 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:03:12: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:03:12: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:03:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:03:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:03:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:03:12: EPOCH - 4 : training on 30082771 raw words (27735245 effective words) took 93.9s, 295517 effective words/s\n",
      "INFO - 23:03:13: EPOCH 5 - PROGRESS: at 1.01% examples, 278920 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:14: EPOCH 5 - PROGRESS: at 2.17% examples, 295002 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:15: EPOCH 5 - PROGRESS: at 3.21% examples, 294235 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:16: EPOCH 5 - PROGRESS: at 4.38% examples, 296062 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:17: EPOCH 5 - PROGRESS: at 5.53% examples, 297007 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:18: EPOCH 5 - PROGRESS: at 6.68% examples, 297421 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:19: EPOCH 5 - PROGRESS: at 7.84% examples, 297846 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:20: EPOCH 5 - PROGRESS: at 8.98% examples, 298399 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:21: EPOCH 5 - PROGRESS: at 10.14% examples, 298374 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:22: EPOCH 5 - PROGRESS: at 11.29% examples, 298534 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:23: EPOCH 5 - PROGRESS: at 12.46% examples, 298803 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:24: EPOCH 5 - PROGRESS: at 13.64% examples, 298922 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:26: EPOCH 5 - PROGRESS: at 14.80% examples, 299010 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:27: EPOCH 5 - PROGRESS: at 15.95% examples, 299126 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:03:28: EPOCH 5 - PROGRESS: at 17.13% examples, 299284 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:29: EPOCH 5 - PROGRESS: at 18.29% examples, 299378 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:30: EPOCH 5 - PROGRESS: at 19.45% examples, 299136 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:03:31: EPOCH 5 - PROGRESS: at 20.61% examples, 299187 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:03:32: EPOCH 5 - PROGRESS: at 21.77% examples, 299244 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:03:33: EPOCH 5 - PROGRESS: at 22.94% examples, 299318 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:34: EPOCH 5 - PROGRESS: at 24.02% examples, 299189 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:35: EPOCH 5 - PROGRESS: at 25.08% examples, 298554 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:36: EPOCH 5 - PROGRESS: at 26.24% examples, 298507 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:37: EPOCH 5 - PROGRESS: at 27.39% examples, 298581 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:38: EPOCH 5 - PROGRESS: at 28.56% examples, 298449 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:03:39: EPOCH 5 - PROGRESS: at 29.73% examples, 298382 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:41: EPOCH 5 - PROGRESS: at 30.89% examples, 298373 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:42: EPOCH 5 - PROGRESS: at 32.07% examples, 298323 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:43: EPOCH 5 - PROGRESS: at 33.21% examples, 298334 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 23:03:44: EPOCH 5 - PROGRESS: at 34.39% examples, 298373 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:03:45: EPOCH 5 - PROGRESS: at 35.54% examples, 298393 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:46: EPOCH 5 - PROGRESS: at 36.71% examples, 298423 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:47: EPOCH 5 - PROGRESS: at 37.85% examples, 298457 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:03:48: EPOCH 5 - PROGRESS: at 39.01% examples, 298483 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:03:49: EPOCH 5 - PROGRESS: at 40.17% examples, 298442 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:50: EPOCH 5 - PROGRESS: at 41.35% examples, 298452 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:51: EPOCH 5 - PROGRESS: at 42.49% examples, 298489 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:52: EPOCH 5 - PROGRESS: at 43.64% examples, 298444 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:53: EPOCH 5 - PROGRESS: at 44.80% examples, 298366 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:55: EPOCH 5 - PROGRESS: at 45.98% examples, 298255 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:56: EPOCH 5 - PROGRESS: at 47.13% examples, 298236 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:57: EPOCH 5 - PROGRESS: at 48.30% examples, 298254 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:03:58: EPOCH 5 - PROGRESS: at 49.45% examples, 298085 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:03:59: EPOCH 5 - PROGRESS: at 50.61% examples, 298055 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:00: EPOCH 5 - PROGRESS: at 51.78% examples, 298027 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:04:01: EPOCH 5 - PROGRESS: at 52.95% examples, 298041 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:02: EPOCH 5 - PROGRESS: at 54.11% examples, 298026 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:03: EPOCH 5 - PROGRESS: at 55.28% examples, 298031 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 23:04:04: EPOCH 5 - PROGRESS: at 56.45% examples, 298043 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:05: EPOCH 5 - PROGRESS: at 57.61% examples, 298060 words/s, in_qsize 10, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:04:06: EPOCH 5 - PROGRESS: at 58.76% examples, 298104 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:04:08: EPOCH 5 - PROGRESS: at 59.92% examples, 298111 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:04:09: EPOCH 5 - PROGRESS: at 61.07% examples, 298124 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:10: EPOCH 5 - PROGRESS: at 62.25% examples, 298117 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:11: EPOCH 5 - PROGRESS: at 63.43% examples, 298101 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:12: EPOCH 5 - PROGRESS: at 64.59% examples, 298082 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:13: EPOCH 5 - PROGRESS: at 65.76% examples, 298078 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:14: EPOCH 5 - PROGRESS: at 66.91% examples, 298085 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:15: EPOCH 5 - PROGRESS: at 68.08% examples, 298076 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:16: EPOCH 5 - PROGRESS: at 69.24% examples, 298017 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:17: EPOCH 5 - PROGRESS: at 70.41% examples, 298024 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:18: EPOCH 5 - PROGRESS: at 71.56% examples, 298043 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:19: EPOCH 5 - PROGRESS: at 72.72% examples, 297998 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:04:21: EPOCH 5 - PROGRESS: at 73.87% examples, 297986 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:22: EPOCH 5 - PROGRESS: at 75.03% examples, 297956 words/s, in_qsize 9, out_qsize 1\n",
      "INFO - 23:04:23: EPOCH 5 - PROGRESS: at 76.17% examples, 297983 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:24: EPOCH 5 - PROGRESS: at 77.32% examples, 297991 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:25: EPOCH 5 - PROGRESS: at 78.47% examples, 297992 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:26: EPOCH 5 - PROGRESS: at 79.63% examples, 298001 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:27: EPOCH 5 - PROGRESS: at 80.78% examples, 298021 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:28: EPOCH 5 - PROGRESS: at 81.93% examples, 298019 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:29: EPOCH 5 - PROGRESS: at 83.11% examples, 298029 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:30: EPOCH 5 - PROGRESS: at 84.27% examples, 298024 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:04:31: EPOCH 5 - PROGRESS: at 85.43% examples, 298034 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:32: EPOCH 5 - PROGRESS: at 86.59% examples, 298050 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:04:33: EPOCH 5 - PROGRESS: at 87.76% examples, 298042 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:35: EPOCH 5 - PROGRESS: at 88.93% examples, 297984 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:04:36: EPOCH 5 - PROGRESS: at 90.08% examples, 297964 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:37: EPOCH 5 - PROGRESS: at 91.26% examples, 297942 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:38: EPOCH 5 - PROGRESS: at 92.42% examples, 297965 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:39: EPOCH 5 - PROGRESS: at 93.56% examples, 298032 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:40: EPOCH 5 - PROGRESS: at 94.67% examples, 298198 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:41: EPOCH 5 - PROGRESS: at 95.71% examples, 298001 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 23:04:42: EPOCH 5 - PROGRESS: at 96.87% examples, 298023 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 23:04:43: EPOCH 5 - PROGRESS: at 98.02% examples, 298033 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 23:04:44: EPOCH 5 - PROGRESS: at 99.19% examples, 298032 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 23:04:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:04:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:04:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:04:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:04:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:04:45: EPOCH - 5 : training on 30082771 raw words (27735565 effective words) took 93.0s, 298342 effective words/s\n",
      "INFO - 23:04:45: training on a 150413855 raw words (138679774 effective words) took 475.2s, 291861 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 7.92 mins\n"
     ]
    }
   ],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "w2v_model.corpus_count\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=5, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:04:45: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' 777148', 0.9957123398780823),\n",
       " (' 2300520', 0.9950451850891113),\n",
       " (' 4374155', 0.9949368238449097),\n",
       " (' 96687', 0.9949037432670593),\n",
       " (' 237751', 0.9948924779891968),\n",
       " (' 125680', 0.9948118925094604),\n",
       " ('490972', 0.9947587847709656),\n",
       " (' 222040', 0.9947294592857361),\n",
       " (' 435810', 0.9947066903114319),\n",
       " (' 567535', 0.9947012662887573)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"1223\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02852395,  0.12255157,  0.00364791,  0.1142091 ,  0.02547306,\n",
       "        0.03763156,  0.06508412,  0.01677829,  0.05980371,  0.00129941,\n",
       "       -0.05996538,  0.09917008,  0.01787447,  0.06870812, -0.00550458,\n",
       "        0.03171371, -0.00551774,  0.07187467,  0.02806522, -0.0625302 ,\n",
       "        0.03758309, -0.0647551 , -0.08147276, -0.00129494,  0.13440752,\n",
       "       -0.07413115,  0.04718652, -0.00169556,  0.04430559,  0.00626139,\n",
       "       -0.10673538,  0.05980246, -0.06952408, -0.05417531,  0.00287848,\n",
       "        0.02553058, -0.00341029,  0.03896419,  0.01680904, -0.03892746,\n",
       "       -0.00174816,  0.02242888, -0.04416034, -0.04266509, -0.04192096,\n",
       "       -0.03992682, -0.01612154,  0.00453351, -0.00509735, -0.02759567,\n",
       "        0.1687546 , -0.10769073, -0.06315976,  0.02836495, -0.04426255,\n",
       "        0.08104195,  0.17512217,  0.01137067, -0.0193985 , -0.04283449,\n",
       "       -0.0085313 ,  0.01186987, -0.01000066,  0.07693687,  0.04670606,\n",
       "       -0.05314675,  0.06060739, -0.04227314, -0.02658184, -0.01949798,\n",
       "        0.05164561,  0.05477045,  0.07363084,  0.10787047,  0.0473001 ,\n",
       "        0.05236414, -0.04958775,  0.03065209, -0.08405518,  0.04910036,\n",
       "       -0.00625639,  0.04516153,  0.04026585,  0.01391777, -0.05089459,\n",
       "        0.05927833,  0.01896584, -0.1303145 ,  0.03389514, -0.02875792,\n",
       "        0.04498991,  0.07328843,  0.00305631,  0.08773042,  0.01033634,\n",
       "       -0.01106775,  0.032963  ,  0.11737164, -0.15963869, -0.0384881 ,\n",
       "       -0.01064625, -0.02887087,  0.03292292, -0.1147074 , -0.1438229 ,\n",
       "       -0.015444  ,  0.076079  , -0.03020901,  0.11208441, -0.00274936,\n",
       "       -0.05135236, -0.02311442,  0.06112928, -0.06510642,  0.01377573,\n",
       "       -0.04708313,  0.07329953,  0.04064567, -0.04403799, -0.0136816 ,\n",
       "       -0.03009417, -0.03195746,  0.00959666, -0.06829213, -0.00210493,\n",
       "       -0.01773134,  0.08332384, -0.01352688, -0.01946378,  0.08182691,\n",
       "       -0.02321532, -0.0190867 ,  0.01421527,  0.03138093, -0.07808213,\n",
       "        0.09894434, -0.01990916, -0.06064196, -0.01225646,  0.03213963,\n",
       "        0.08509812, -0.02063599, -0.04362615, -0.08369353, -0.017527  ,\n",
       "       -0.05742945,  0.07203144,  0.03491411,  0.02216161, -0.02628174,\n",
       "        0.02581628,  0.0443056 , -0.01284855, -0.05316323, -0.02249702,\n",
       "       -0.0066574 ,  0.04202244, -0.08340061,  0.03460325, -0.02377984,\n",
       "       -0.01832848,  0.04459265, -0.06138285, -0.06363477,  0.06835496,\n",
       "       -0.02308789,  0.09794636,  0.02548302, -0.07834794, -0.02825884,\n",
       "       -0.02151973, -0.05617389,  0.00075208,  0.07007066,  0.02129972,\n",
       "        0.04345334, -0.07636471,  0.04455462,  0.01132688,  0.05247451,\n",
       "       -0.05040577,  0.00739613,  0.06182586,  0.00404852,  0.04858758,\n",
       "        0.00347901,  0.05328682,  0.01273097, -0.01217744,  0.08499271,\n",
       "       -0.00488474,  0.01808746,  0.07215798,  0.02141003, -0.00023135,\n",
       "       -0.09392197, -0.00082134,  0.05520097,  0.02060959, -0.02858431,\n",
       "        0.05236324,  0.02351492, -0.13810228,  0.15798381,  0.0350683 ,\n",
       "        0.05669054,  0.07365577, -0.06719125,  0.08968976,  0.01039913,\n",
       "       -0.00262882,  0.20781122,  0.015004  , -0.04728428,  0.05776262,\n",
       "        0.04501541,  0.03422788, -0.08510369, -0.02967027, -0.1093161 ,\n",
       "        0.06625781, -0.08352704, -0.10592717, -0.01237695,  0.03647532,\n",
       "       -0.01645193, -0.01567196, -0.04321539,  0.01537118,  0.00405328,\n",
       "       -0.01929244,  0.09193027,  0.12141601, -0.03003761,  0.0210311 ,\n",
       "       -0.00712638, -0.08167263,  0.00700033, -0.03622483,  0.03041495,\n",
       "       -0.02332142,  0.03159961,  0.03114393,  0.01799119,  0.00459954,\n",
       "       -0.0200277 ,  0.0129565 , -0.03489075,  0.06440084,  0.05987505,\n",
       "        0.0067564 ,  0.07387342, -0.02094061, -0.03310479, -0.04487532,\n",
       "        0.01529729,  0.06537183,  0.01587421, -0.02025243, -0.02369653,\n",
       "       -0.06152384,  0.04879897,  0.00802895,  0.03488323, -0.06823318,\n",
       "        0.01140669,  0.00088499,  0.12350909,  0.04029452,  0.05226519,\n",
       "       -0.01257909,  0.0343095 ,  0.07134508, -0.01030695,  0.00378747,\n",
       "       -0.08966874,  0.00047974, -0.04052252, -0.07848811, -0.02155444,\n",
       "       -0.02840563,  0.07460395,  0.10371932,  0.04355473, -0.08569027,\n",
       "       -0.05585865,  0.05331273, -0.09472017,  0.03764533, -0.01912956,\n",
       "        0.05055763,  0.08894923, -0.00372793, -0.08692486,  0.0795981 ,\n",
       "       -0.0268777 ,  0.09178985,  0.03296929, -0.03271288, -0.04606705],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['260644']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:11:10: storing 2630643x300 projection weights into model.bin\n"
     ]
    }
   ],
   "source": [
    "#w2v_model.save('w2v.model')\n",
    "w2v_model.wv.save_word2vec_format('model.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:29:59: loading Word2Vec object from w2v.model\n",
      "INFO - 22:30:07: loading wv recursively from w2v.model.wv.* with mmap=None\n",
      "INFO - 22:30:07: loading vectors from w2v.model.wv.vectors.npy with mmap=None\n",
      "INFO - 22:30:33: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:30:33: loading vocabulary recursively from w2v.model.vocabulary.* with mmap=None\n",
      "INFO - 22:30:33: loading trainables recursively from w2v.model.trainables.* with mmap=None\n",
      "INFO - 22:30:33: loading syn1neg from w2v.model.trainables.syn1neg.npy with mmap=None\n",
      "INFO - 22:30:59: setting ignored attribute cum_table to None\n",
      "INFO - 22:30:59: loaded w2v.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load('w2v.model') \n",
    "\n",
    "## 2 构造包含所有词语的 list，以及初始化 “词语-序号”字典 和 “词向量”矩阵\n",
    "vocab_list = [word for word, Vocab in w2v_model.wv.vocab.items()]# 存储 所有的 词语\n",
    "\n",
    "word_index = {\" \": 0}# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "word_vector = {} # 初始化`[word : vector]`字典\n",
    "\n",
    "# 初始化存储所有向量的大矩阵，留意其中多一位（首行），词向量全为 0，用于 padding补零。\n",
    "# 行数 为 所有单词数+1 比如 10000+1 ； 列数为 词向量“维度”比如100。\n",
    "embeddings_matrix = np.zeros((len(vocab_list) + 1, w2v_model.vector_size))\n",
    "\n",
    "#embeddings_matrix.shape\n",
    "\n",
    "## 3 填充 上述 的字典 和 大矩阵\n",
    "for i in range(len(vocab_list)):\n",
    "    # print(i)\n",
    "    word = vocab_list[i]  # 每个词语\n",
    "    word_index[word] = i + 1 # 词语：序号\n",
    "    word_vector[word] = w2v_model.wv[word] # 词语：词向量\n",
    "    embeddings_matrix[i + 1] = w2v_model.wv[word]  # 词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "EMBEDDING_DIM = 300 \n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "# 字典长度  # 词向量 长度（100） # 重点：预训练的词向量系数# 每句话的 最大长度（必须padding）  # 是否在 训练的过程中 更新词向量\n",
    "embedding_layer = Embedding(input_dim = len(embeddings_matrix), \n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embeddings_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>['821396', '209778', '877468', '1683713', '122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>['63441', '155822', '39714', '609050', '126618...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>['661347', '808612', '710859', '825434', '5935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>['39588', '589886', '574787', '1892854', '1962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>['296145', '350759', '24333', '43235', '852327...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  age  gender  \\\n",
       "0           0   1    4       1   \n",
       "1           1   2   10       1   \n",
       "2           2   3    7       2   \n",
       "3           3   4    5       1   \n",
       "4           4   5    4       1   \n",
       "\n",
       "                                            sentence  \n",
       "0  ['821396', '209778', '877468', '1683713', '122...  \n",
       "1  ['63441', '155822', '39714', '609050', '126618...  \n",
       "2  ['661347', '808612', '710859', '825434', '5935...  \n",
       "3  ['39588', '589886', '574787', '1892854', '1962...  \n",
       "4  ['296145', '350759', '24333', '43235', '852327...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gender = list(creation['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    602610\n",
       "2    297390\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creation['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n\t [[embedding_1/ReadVariableOp/_7]]\n  (1) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n\t [[embedding_1/ReadVariableOp/_7]]\n  (1) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-608ecdd2de05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msequence_1_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membedded_sequences_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_1_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_sequences_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    466\u001b[0m                 \u001b[1;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                                  'provided weight shape ' + str(w.shape))\n\u001b[0;32m   1127\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1128\u001b[1;33m         \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   2958\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m     \"\"\"\n\u001b[1;32m-> 2960\u001b[1;33m     \u001b[0mtf_keras_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3069\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3070\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3071\u001b[1;33m         \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n\t [[embedding_1/ReadVariableOp/_7]]\n  (1) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {1: 2, 2: 1}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save_word2vec_format('model.bin', binary=True)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('w2v.model',binary=False,encoding='utf-8', unicode_errors='ignore')\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "\"I come to China to travel\",\n",
    "\"This is a car polupar in China\",\n",
    "\"I love tea and Apple\",\n",
    "\"The work is to write some papers in science\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "re = tfidf.fit_transform(corpus)\n",
    "print(re)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
