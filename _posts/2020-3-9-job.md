---
layout:     post
title:      job
subtitle:   
date:       2020-3-9
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---


## 文本纠错论文梳理

[2018 中文文本真词错误自动校对算法研究](https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1019100894.nh&uid=WEEvREcwSlJHSldRa1Fhb09pSnNvVWhKSWYzeVE3aHJnb2Zmb0Fld1g3MD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&v=MDA4NTZQSVI4ZVgxTHV4WVM3RGgxVDNxVHJXTTFGckNVUjdxZlpPWm5GeUhnVzcvS1ZGMjZGN0s0SHRuRnE1RWI=)

[2004 文本自动校对技术研究综述](https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFQ&dbname=CJFD2006&filename=JSYJ200606001&uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1UllzZHJHNnFRdG1ScjkzT3FlcFMzOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&v=MTA4NzhSOGVYMUx1eFlTN0RoMVQzcVRyV00xRnJDVVI3cWZaT1puRnlIbVVMN01MejdTWkxHNEh0Zk1xWTlGWlk=)



[2019 Faspell](https://www.aclweb.org/anthology/D19-5522.pdf)

[2017 Chinese Spelling Error Detection and Correction Based on Language Model, Pronunciation, and Shape](https://www.aclweb.org/anthology/W14-6835.pdf)

论文提供了一种准确较高、召回较低的纠错方法。

- Character级别 n-gram language model。
- 拼音和字形召回候选
- 词典过滤掉部分无效候选
- 取最高语言模型打分
- 高于既定阈值则认为是替换候选

[Visually and phonologically similar characters in incorrect simplified chinese words](https://www.aclweb.org/anthology/C10-2085.pdf)
We collected 621 incorrect Chinese words reported on the Internet, and analyzed the causes of these errors. 83% of
these errors were related to phonological similarity, and 48% of them were related to visual similarity between the involved characters.


## 演示PPT制作 完毕


## 修改Bert Mask策略使字和拼音产生联系


## 对接ASRT模型
ASRT路径： /raid/houtao/DeepSpeechRecognition-master

执行脚本：python AMLMtest_changed.py

数据加载参数： util.py中的get_wav_list()方法，read_file_scp = [ 'thchs_dev.txt']，需要将测试对应的文件名修改

具体讲文件放在：/raid/houtao/DeepSpeechRecognition-master/data/test_asrt





## MASS模型下载安装测试

Prerequisites
After download the repository, you need to install fairseq by pip:

pip install fairseq==0.7.1

fairseq是现有比较完善的seq2seq库，由于是大公司出品，因此也写得较为完善，不论是代码还是文档
[如何使用fairseq复现Transformer NMT](http://www.linzehui.me/2019/01/28/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8fairseq%E5%A4%8D%E7%8E%B0Transformer%20NMT/)

[核心issue](https://github.com/microsoft/MASS/issues/43)

数据的下载： [WMT2017](http://www.statmt.org/wmt17/translation-task.html#download)

mono：

Common Crawl 33G


para：

News Commentary v12	162MB

UN Parallel Corpus V1.0	3.6 GB

CWMT Corpus ？


Data Ready
We first prepare the monolingual and bilingual sentences for Chinese and English respectively. The data directory looks like:

- data/
  ├─ mono/
  |  ├─ train.en
  |  ├─ train.zh
  |  ├─ valid.en
  |  ├─ valid.zh
  |  ├─ dict.en.txt
  |  └─ dict.zh.txt
  └─ para/
     ├─ train.en
     ├─ train.zh
     ├─ valid.en
     ├─ valid.zh
     ├─ dict.en.txt
     └─ dict.zh.txt
The files under mono are monolingual data, while under para are bilingual data. dict.en(zh).txt in different directory should be identical. The dictionary for different language can be different. Running the following command can generate the binarized data:

# Ensure the output directory exists
data_dir=data/
mono_data_dir=$data_dir/mono/
para_data_dir=$data_dir/para/
save_dir=$data_dir/processed/

# set this relative path of MASS in your server
user_dir=mass

mkdir -p $data_dir $save_dir $mono_data_dir $para_data_dir


# Generate Monolingual Data
for lg in en zh
do

  fairseq-preprocess \
  --task cross_lingual_lm \
  --srcdict $mono_data_dir/dict.$lg.txt \
  --only-source \
  --trainpref $mono_data_dir/train --validpref $mono_data_dir/valid \
  --destdir $save_dir \
  --workers 20 \
  --source-lang $lg

  # Since we only have a source language, the output file has a None for the
  # target language. Remove this

  for stage in train valid
  do
    mv $save_dir/$stage.$lg-None.$lg.bin $save_dir/$stage.$lg.bin
    mv $save_dir/$stage.$lg-None.$lg.idx $save_dir/$stage.$lg.idx
  done
done

# Generate Bilingual Data
fairseq-preprocess \
  --user-dir $mass_dir \
  --task xmasked_seq2seq \
  --source-lang en --target-lang zh \
  --trainpref $para_data_dir/train --validpref $para_data_dir/valid \
  --destdir $save_dir \
  --srcdict $para_data_dir/dict.en.txt \
  --tgtdict $para_data_dir/dict.zh.txt
Pre-training
We provide a simple demo code to demonstrate how to deploy mass pre-training.

save_dir=checkpoints/mass/pre-training/
user_dir=mass
data_dir=data/processed/

mkdir -p $save_dir

fairseq-train $data_dir \
    --user-dir $user_dir \
    --save-dir $save_dir \
    --task xmasked_seq2seq \
    --source-langs en,zh \
    --target-langs en,zh \
    --langs en,zh \
    --arch xtransformer \
    --mass_steps en-en,zh-zh \
    --memt_steps en-zh,zh-en \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr-scheduler inverse_sqrt --lr 0.00005 --min-lr 1e-09 \
    --criterion label_smoothed_cross_entropy \
    --max-tokens 4096 \
    --dropout 0.1 --relu-dropout 0.1 --attention-dropout 0.1 \
    --max-update 100000 \
    --share-decoder-input-output-embed \
    --valid-lang-pairs en-zh \
We also provide a pre-training script which is used for our released model.

Fine-tuning
After pre-training stage, we fine-tune the model on bilingual sentence pairs:

data_dir=data/processed
save_dir=checkpoints/mass/fine_tune/
user_dir=mass
model=checkpoint/mass/pre-training/checkpoint_last.pt # The path of pre-trained model

mkdir -p $save_dir

fairseq-train $data_dir \
    --user-dir $user_dir \
    --task xmasked_seq2seq \
    --source-langs zh --target-langs en \
    --langs en,zh \
    --arch xtransformer \
    --mt_steps zh-en \
    --save-dir $save_dir \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr-scheduler inverse_sqrt --lr-shrink 0.5 --lr 0.00005 --min-lr 1e-09 \
    --criterion label_smoothed_cross_entropy \
    --max-tokens 4096 \
    --max-update 100000 --max-epoch 50 \
    --dropout 0.1 --relu-dropout 0.1 --attention-dropout 0.1 \
    --share-decoder-input-output-embed \
    --valid-lang-pairs zh-en \
    --reload_checkpoint $model
We also provide a fine-tuning script which is used for our pre-trained model.