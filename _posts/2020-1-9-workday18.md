---
layout:     post
title:      workday17
subtitle:   
date:       2020-1-8
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 前言

今天换了位置，将相关文件前移到新的电脑上，由于新的电脑读不了我的U盘，所以花了1个多小时安装相关软件和VPN，搭建工作环境等。


## create_tf_record
昨天下午回家后，调试了一下create_tf_record.py文件。对于这个文件的468行代码，有疑问：

```python
if tokens[index] in MASK_PROB:
    if rng.random() < MASK_PROB[tokens[index]]:
        masked_token = tokens[index]
        # print(f'cover {tokens[index]} in correct instance.')
        CORRECT_COUNT[tokens[index]] += 1
```

MASK_PROB是我们之前生成的字典文件：
```
好 0.007894736842105263
了 0.003243243243243243
， 0.0013003901170351106
然 0.005154639175257732
后 0.0038314176245210726
再 0.015873015873015872
回 0.027777777777777776
老 0.008928571428571428
家 0.002304147465437788
) 0.9722222222222222
包 0.03225806451612903
括 0.14285714285714285
的 0.0012414649286157666
一 0.00574300071787509
些 0.016666666666666666
碰 0.16666666666666666
啊 0.010582010582010581
撞 0.2
我 0.0011487650775416428
这 0.002129925452609159
```
该文件的计算通过：
```python
proportions[k] = min(top_confusions[k] / correct_count[k], 1.0)


confusion statistics:
most frequent confusion pair for ( occurs 40 times, correct ones occur 22 times, mask probability should be 1.0
most frequent confusion pair for ) occurs 35 times, correct ones occur 36 times, mask probability should be 0.9722222222222222
most frequent confusion pair for ? occurs 20 times, correct ones occur 7 times, mask probability should be 1.0
```

其中，）和撞字都是出错率较高的字。那么随机一个[0,1]值与这个字典中的概率比较，若取小于号，那么大概两次会执行这个条件块，那么masked_token就使用原来的字符，而不Mask了。

代码中，这样描述：
```python
# 80% of the time, replace with [MASK]
if rng.random() < 0.8:
    masked_token = "[MASK]"
else:
    # 10% of the time, keep original
    if rng.random() < 0.5:
        masked_token = tokens[index]
    # 10% of the time, replace with random word
    else:
        masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]

# overwrite the above assignment with mask_prob
if tokens[index] in MASK_PROB:
    if rng.random() < MASK_PROB[tokens[index]]:
        masked_token = tokens[index]
        # print(f'cover {tokens[index]} in correct instance.')
        CORRECT_COUNT[tokens[index]] += 1

output_tokens[index] = masked_token
```
即，如果该字大概率出错（ocr形近出错），则该字不做替换和Mask，就使用该字。小概率出错的就按Bert原来的Mask机制。


**在debug程序的时候，发现电脑配置太低，本地配置的cpu,gpu版本的tensorflow都跑不起来，所以不能debug，这让我分析代码不太方便。**


注意到调用代码的地方：

```python
if tokens == wrong_tokens:
    (tokens, masked_lm_positions,
        masked_lm_labels) = create_masked_lm_predictions(
        tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)
else:
    (tokens, masked_lm_positions,
        masked_lm_labels) = create_masked_lm_predictions_for_wrong_sentences(
        tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng, wrong_tokens)
```

即当correct.txt与wrong.txt对应的token相同的时候，执行的create_masked_lm_predictions，

如果该字大概率出错（ocr形近出错），则取消bert之前进行的MASK和替换操作。小概率出错的就按Bert原来的Mask机制。

否则执行：create_masked_lm_predictions_for_wrong_sentences()

该方法对错的字100%mask，对大概率错的原本正确的字，按概率进行MASK。

最大MASK数量限制在20.


```python
    for (i, token) in enumerate(tokens):
        if token == "[CLS]" or token == "[SEP]":
            assert wrong_tokens[i] == token
            continue
        elif token != wrong_tokens[i]:
            cand_indexes.append(i)
        else:  # when a token is not confused, add it to candidates according to its mask probability
            if token in MASK_PROB:
                if rng.random() < MASK_PROB[token]:
                    WRONG_COUNT[token] += 1
                    # print(f'cover {token} in wrong instance.')
                    cand_indexes.append(i)
```
根据：
```python
    num_to_predict = min(max_predictions_per_seq,
                         max(1, int(round(len(
                             tokens)))))  # we set 100% masking rate to allow all errors and corresponding non-errors to be masked
```




##　NSP & SOP
BERT的 NSP 损失替换为句子顺序预测(SOP)损失，使模型关注于句子间一致性。

[NSP & SOP](https://zhuanlan.zhihu.com/p/86717674)

句子A,B:

    # We DON'T just concatenate all of the tokens from a document into a long
    # sequence and choose an arbitrary split point because this would make the
    # next sentence prediction task too easy. Instead, we split the input into
    # segments "A" and "B" based on the actual "sentences" provided by the user
    # input.


疑问： NSP 与 SOP?

