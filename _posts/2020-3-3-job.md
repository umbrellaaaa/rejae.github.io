---
layout:     post
title:      job
subtitle:   
date:       2020-3-3
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## kenlm

下载安装并调试kenlm模型，使用2014人民日报语料训练N-gram语言模型。

需要安装很多dependency文件，包括：
```python
1、boost
在boost官网下载boost：http://www.boost.org，这里下载了boost 1.67

cd boost
./bootstrap.sh
./b2 install

2、xz
wget http://tukaani.org/xz/xz-5.2.2.tar.gz
tar xzvf xz-5.2.2.tar.gz
cd xz-5.2.2
./configure
make
make install

3、zlib
wget http://zlib.net/zlib-1.2.11.tar.gz
tar xzf zlib-1.2.11.tar.gz
cd zlib-1.2.11
./configure
make
make install

4、bzip
wget https://fossies.org/linux/misc/bzip2-1.0.6.tar.gz
tar xzvf bzip2-1.0.6.tar.gz
cd bzip2-1.0.6/
make
make install

5、libbz2-dev
apt-get install libbz2-dev

5、kenlm
在官网下载 http://kheafield.com/code/kenlm.tar.gz，解压

cd kenlm
mkdir build
cd build
cmake ..
make
python setup.py install
```

使用2014人民日报语料，训练模型：

```
wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz
mkdir kenlm/build
cd kenlm/build
cmake ..
make -j2

build/bin/lmplz -o 3 --verbose_header --text 2014corpus.txt --arpa 2014corpus.arps
```


安装完成后，在python环境中，pip install https://github.com/kpu/kenlm/archive/master.zip

模型的使用：

需要注意的是：我们用的人民日报语料是分词且带词性的，所以我们在使用模型的时候需要对应的输入：

使用jieba，将句子切分成对应的形式：
```python
import jieba.posseg as pseg
words = pseg.cut("三司限城市")
sentence = ''
for word, flag in words:
    sentence=sentence+word+'/'+flag+' '
print(sentence)

out:三司/n 限/v 城市/ns 
```

对句子切分后，得到各个组合的分数结果：
```python
import kenlm
model = kenlm.LanguageModel('/data/language/chenyi/kenlm/kenlm/build/lm.bin')

print(model.score('三司/n 限/v 城市/ns', bos=False, eos=False))
print(model.score('三/m 司线/n 城市/ns', bos=False, eos=False))
print(model.score('三四/m 限/v 城市/ns', bos=False, eos=False))
print(model.score('三四/m 线/n 城市/ns', bos=False, eos=False))
out:

-17.74773406982422
-16.960710525512695
-17.302309036254883
-16.745254516601562
```
可以看到，三四线城市的score是较大的，所以纠错正确。

## 处理bert result.json文件
```python
import json
with open("results_0.json", 'r') as f:
    temp = json.loads(f.read())
    
    for item in temp[3:]:
        print(item)
        print('\n')
        original_sentence = item['original_sentence']
        corrected_sentence = item['corrected_sentence']        
        num_errors = item['num_errors']        
        errors = item['errors']
        for item in errors:
            print(item)
            
        break
```
```python
{'original_sentence': '因此土地储备之观重要', 'corrected_sentence': '因此土地儲备非观重要', 'num_errors': 2, 'errors': [{'error_position': 4, 'original': '储', 'corrected_to': '儲', 'candidates': {'储': 0.9982870221138, '儲': 0.0001897053443826735, '贮': 0.00012521656753960997, '库': 0.00011806152906501666, '保': 0.00010554010805208236}, 'confidence': 0.0001897053443826735, 'similarity': 1.0, 'sentence_len': 10}, {'error_position': 6, 'original': '之', 'corrected_to': '非', 'candidates': {'尤': 0.38277190923690796, '非': 0.17129772901535034, '极': 0.13685572147369385, '十': 0.11843526363372803, '相': 0.024108897894620895}, 'confidence': 0.17129772901535034, 'similarity': 0.5666666666666668, 'sentence_len': 10}]}


{'error_position': 4, 'original': '储', 'corrected_to': '儲', 'candidates': {'储': 0.9982870221138, '儲': 0.0001897053443826735, '贮': 0.00012521656753960997, '库': 0.00011806152906501666, '保': 0.00010554010805208236}, 'confidence': 0.0001897053443826735, 'similarity': 1.0, 'sentence_len': 10}
{'error_position': 6, 'original': '之', 'corrected_to': '非', 'candidates': {'尤': 0.38277190923690796, '非': 0.17129772901535034, '极': 0.13685572147369385, '十': 0.11843526363372803, '相': 0.024108897894620895}, 'confidence': 0.17129772901535034, 'similarity': 0.5666666666666668, 'sentence_len': 10}
```
进一步分析错误位置和候选：
```python
import json
with open("results_0.json", 'r') as f:
    temp = json.loads(f.read())
    
    for item in temp[5:]:

        original_sentence = item['original_sentence']
        corrected_sentence = item['corrected_sentence']        
        num_errors = item['num_errors']        
        errors = item['errors']
        error_position = []
        cand_list = []
        for item in errors:
            error_position.append(item['error_position'])
            cand_list.append(list(item['candidates'].keys()))
        
        error_words = [original_sentence[index] for index in error_position]
        print(original_sentence)
        print('error_words',error_words)
        print(error_position)
        print(cand_list)
        break
```
```python
因此土地储备之观重要
error_words ['储', '之']
[4, 6]
[['储', '儲', '贮', '库', '保'], ['尤', '非', '极', '十', '相']]
```

构建所有候选的句子：
...






## 论文分享

爱奇艺 Faspell

Mask Language Model 具体掩码每个位置，通过上下文预测该位置的字。



ocr纠错和asr纠错: ocr通过偏旁拆解，根据笔画元素的编辑距离计算形近度，根据拼音编辑距离计算发音相似度。

数据增强： 错误字数


核心：

- A Fast, Adaptable, Simple, Powerful Chinese Spell Checker Based On DAE-Decoder Paradigm

- FASPell based on a new paradigm which consists of a denoising autoencoder (DAE) and a decoder. 

在引入中，可以大致了解到，中文拼写纠错不能很好的使用已有的英文纠错方法，原因是汉字不像英文句子，自带分词，有各种形态变换，并且汉字的语义信息高度依赖于上下文。

目前的瓶颈：

- 纠错需要的数据量不足，导致模型过拟合。
（A Hybrid Approach to Automatic Corpus Generationfor Chinese Spelling Check） 腾讯出的文章，即通过生成的方式增加数据量。具体是用ocr和ASR模型，得到识别后的错误数据。这只能视作一种增加数据实用方法。
- 在利用字符相似度时，缺乏灵活性和混淆性。
inflexibility

insufficiency