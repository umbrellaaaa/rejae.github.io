---
layout:     post
title:      NPLM 神经概率语言模型
subtitle:   summary
date:       2019-11-12
author:     RJ
header-img: 
catalog: true
tags:
    - NLP

---
<p id = "build"></p>
---

This is an implementation of a Feedforward neural network for building a language model. Though, I took inspiration from Y. Bengio's 2003 classic paper published in JMLR : "A Neural Probabilistic Language Model", I tweaked the word embeddings generation procedure to the current SOTA method : skip-gram model with Noise Contrastive Estimation.

I have also employed the use of Xavier algorithm to initialize weights of the neural network. **It determines the variance of the distribution of the weights based on the number of input and output neurons.** It helps to propagate the signal deep into the network. This is because if the weights are initialized with a small value, it starts to diminish as it passes through layers and drops off to a really low value. Considering the activation function to be sigmoid, the low value of input makes the activations almost linear which beats the point of introducing non-linearity in the network. Vice versa, in the case the weights are initialized with a large value, the variance of input increases with each passing layer. Eventually, the activations again become linear since sigmoid function becomes flat for large values. Hence the need to initialize the weights with the right amount of variance :

Var(Wi) = 2 / (Ninp + Nout)      
where Ninp = number of input neurons and Nout = number of output neurons. The variance of weights is set to this value in order to equalize the variance of output and input to a linear neuron (Y = W1X1 + .... + WnXn) per layer (kindly check the paper for detailed description).

I am using TensorFlow for generating the word embeddings as well as for the language model.

## 1. 数据准备

```python
import re
import argparse
import collections
import random
import numpy as np
import tensorflow as tf
import math

# Import One hot encoder here

vocabulary_size = 379
FILE_PATH = '../data/text_file.txt'
global data_index
data_index = 0


def data_generate(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        text = f.read()
    regex = re.compile(r'\([^)]*\)')
    sub_text = regex.sub('', text)
    data = re.findall('\w+', sub_text)
    return data


def build_dataset(words):
    count = [['rare', -1]]

    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    dictionary = dict()

    for word, _ in count:
        dictionary[word] = len(dictionary)

    data = list()
    rare_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0
            rare_count = rare_count + 1
        data.append(index)
    count[0][1] = rare_count
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reverse_dictionary


def generate_batch(batch_size, skip_window, num_skips, data):
    global data_index
    assert batch_size % num_skips == 0
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)

    # Span includes the middle element and the surrounding elements. Hence plus one.
    span = 2 * skip_window + 1

    buffer = collections.deque(maxlen=span)

    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)

    for i in range(batch_size // num_skips):
        target = skip_window  # Middle element is the target
        targets_to_avoid = [skip_window]

        for j in range(num_skips):
            while target in targets_to_avoid:  # we dont want already included word
                target = random.randint(0, span - 1)  # to select a random word from the buffer
            targets_to_avoid.append(target)

            batch[i * num_skips + j] = buffer[skip_window]
            labels[i * num_skips + j] = buffer[target]  # here labels are the surrounding words

        # This is to get the next element in the queue.
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)

    return batch, labels
```
**comment here**: 

不起眼的generate_batch函数却是Skip-gram的核心，不debug一下，还不容易与其的原理对应上，在本次代码中，由于采取的skip_window=1,num_skips就只能取<=skip_window*2的大小了，表示在两侧窗口内总共取多少个词来预测。其中细节是在采样的时候不重复且不能取到中心词，所以需要targets_to_avoid来判断。<br>
**需要注意的是：**
在for i in range(batch_size // num_skips)循环中，<br>
data_index的取值范围是3+batch_size // num_skips<br>
于是buffer.append(data[data_index])只移动了128维的一半多一点。<br>
所以batch[i * num_skips + j] = buffer[skip_window]生成的数据只用到了传入data的一半多一点, 如下图debug结果所示，意味着还有近一半的数据没有用到，有点让人不解。
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191113dubg2.jpg)
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191113dubg1.jpg)

## 2. Skip-Gram模型构建和训练
接下来的代码就是模型的构建了，代码很简单，就是一个embedding层，不过损失函数是nce_loss噪声对比估算损失。为什么不用交叉熵损失呢，主要原因是模型采用了负采样来加速模型训练，负采样其实就是正样本与负样本的极大似然估计，这里不做深入讲解，不清楚的小伙伴建议google一下把这里弄清楚，其中embeddings 是嵌入矩阵，即我们要求的词向量，nce_weights是计算逻辑回归的权重，nce_biases是偏移量。
- NCE做了一件很intuitive的事情：用负样本采样的方式，不计算完整的归一化项。让模型通过负样本，估算出真是样本的概率，从而在真是样本上能做得了极大似然。相当于把任务转换成了一个分类任务，然后再用类似交叉熵的方式来对模型进行优化（其实本质上是优化了两个部分：模型本身，和一个负例采样的分布和参数）。
- NCE loss的直观想法：把多分类问题转化成二分类。之前计算softmax的时候class数量太大，NCE索性就把分类缩减为二分类问题。之前的问题是计算某个类的归一化概率是多少，二分类的问题是input和label正确匹配的概率是多少。
- Sampled softmax则是只抽取一部分样本计算softmax。这个想法也很好理解，训练的时候我不需要特别精准的softmax归一化概率，我只需要一个粗略值做back propoagation就好了。
- NCE bridges the gap between generative models and discriminative models, rather than simply speedup the softmax layer.



```python
def skip_gram(args, data):
    batch_size = args.batch_size
    embedding_size = args.embedding_size  # Feature vector size
    skip_window = args.skip_window
    num_skips = args.num_skips

    # Negative sampling

    valid_size = 16
    valid_window = 100
    valid_examples = np.random.choice(valid_window, valid_size, replace=False)
    num_sampled = 64  # Number of negative samples to sample.

    # Creating the computation graph

    graph = tf.Graph()

    with graph.as_default():
        # Inputs
        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

        with tf.device('/cpu:0'):
            embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], \
                                                       -1.0, 1.0))

            embed = tf.nn.embedding_lookup(embeddings, train_inputs)

            # Construct the variables for the NCE loss
            nce_weights = tf.Variable(
                tf.truncated_normal([vocabulary_size, embedding_size],
                                    stddev=1.0 / math.sqrt(embedding_size)))
            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

        # Computing NCE loss
        loss = tf.reduce_mean(
            tf.nn.nce_loss(weights=nce_weights,
                           biases=nce_biases,
                           labels=train_labels,
                           inputs=embed,
                           num_sampled=num_sampled,
                           num_classes=vocabulary_size))

        # Construct the SGD optimizer using a learning rate of 1.0.
        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)

        # Compute the cosine similarity between minibatch valid_examples and all embeddings.
        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
        normalized_embeddings = embeddings / norm
        valid_embeddings = tf.nn.embedding_lookup(
            normalized_embeddings, valid_dataset)
        similarity = tf.matmul(
            valid_embeddings, normalized_embeddings, transpose_b=True)

        # Add variable initializer.
        init = tf.global_variables_initializer()

        # Step 5: Begin training.
        num_steps = 10000

        with tf.Session(graph=graph) as session:
            # We must initialize all variables before we use them.
            init.run()
            # print("Initialized")

            average_loss = 0
            for step in range(num_steps):
                batch_inputs, batch_labels = generate_batch(
                    batch_size, num_skips, skip_window, data)
                feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

                # We perform one update step by evaluating the optimizer op (including it
                # in the list of returned values for session.run()
                _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
                # average_loss += loss_val
                """
                if step % 2000 == 0:
                  if step > 0:
                    average_loss /= 2000
                  # The average loss is an estimate of the loss over the last 2000 batches.
                  print("Average loss at step ", step, ": ", average_loss)
                  average_loss = 0
                """

            final_embeddings = normalized_embeddings.eval(session=session)

    return final_embeddings

```

## main函数参数配置

```python
def main():
    parser = argparse.ArgumentParser(description="Parameters for skip gram model")

    parser.add_argument('-b', '--batch_size', action='store', type=int, default=128,
                        help='batch_size for training the model')
    parser.add_argument('-e', '--embedding_size', action='store', type=int, default=128,
                        help='Number of the dimensions of the feature vector')
    parser.add_argument('-s', '--skip_window', action='store', type=int, default=1,
                        help='Window size of the words around the context word')
    parser.add_argument('-ns', '--num_skips', action='store', type=int, default=2,
                        help='Number of times an input can be used in each batch')

    args = parser.parse_args()
    words = data_generate(FILE_PATH)
    data, count, dictionary, reverse_dictionary = build_dataset(words)
    # print data
    print(data)
    print(len(data))
    word_embeddings = skip_gram(args, data)
    # print word_embeddings
    with open('result.txt', "w+", encoding='utf-8') as f:
        print(word_embeddings.shape)
        for item in word_embeddings:
            f.write(str(item))
    return word_embeddings


if __name__ == "__main__":
    main()

```

## 总结：
- 对于CBOW和Skip-Gram这两种模型来说，都是P(Y|X)模型的实现，差别在于CBOW是context->center;而skip-gram是 center->context;
- 训练过程需要注意batch和label的分别代表什么，在skipgram中，batch就是中心词，比如skip_window=1，num_skips=2的时候，batch[0]和batch[1]都是中心词，而label[0]和label[1]是它的context； CBOW则正好相反。
- 疑惑：但是在dubug的时候，发现一个128大小的batch_size只用到了一半多一点的数据作为生成的batch，其余数据浪费了有些不解





## 参考

[通俗理解nce_loss](https://www.zhihu.com/question/50043438)