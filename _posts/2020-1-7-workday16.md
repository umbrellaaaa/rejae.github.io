---
layout:     post
title:      workday16
subtitle:   
date:       2020-1-7
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 前言
昨天分析了create_data.py文件，从中知道了汉字被mask的概率计算方式。

分析了char_meta.txt文件的结构，以及笔画构建的流程和进一步拆分上下左右结构以及偏旁部首-->笔画。从这个meta_char.txt文件中收益良多，并产生了新的idea，此处先mark，待进一步分析。

## 今日工作安排：

1. 分析create_tf_record.py代码功能
2. 学习微调bert模型

## create_tf_record.py

**全局变量**
```python
MASK_PROB = pickle.load(open('mask_probability.sav', 'rb'))
WRONG_COUNT = dict([(k, 0) for k in MASK_PROB])
CORRECT_COUNT = dict([(k, 0) for k in MASK_PROB])

MaskedLmInstance = collections.namedtuple("MaskedLmInstance",
                                          ["index", "label"])
```
注意这个MaskedLmInstance, 由集合中的namedtuple构建了一种名为MaskedLmInstance的数据结构，包含index和label两个属性。



**程序入口**
```python
if __name__ == "__main__":
    flags.mark_flag_as_required("input_file")
    flags.mark_flag_as_required("wrong_input_file")
    flags.mark_flag_as_required("output_file")
    flags.mark_flag_as_required("vocab_file")
    tf.app.run()
```
关注这四个输入文件

**唯一class, 对应bert论文中的NSP**
```python
class TrainingInstance(object):
    """A single training instance (sentence pair)."""

    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,
                 is_random_next):
        self.tokens = tokens
        self.segment_ids = segment_ids
        self.is_random_next = is_random_next
        self.masked_lm_positions = masked_lm_positions
        self.masked_lm_labels = masked_lm_labels

    def __str__(self):
        s = ""
        s += "tokens: %s\n" % (" ".join(
            [tokenization.printable_text(x) for x in self.tokens]))
        s += "segment_ids: %s\n" % (" ".join([str(x) for x in self.segment_ids]))
        s += "is_random_next: %s\n" % self.is_random_next
        s += "masked_lm_positions: %s\n" % (" ".join(
            [str(x) for x in self.masked_lm_positions]))
        s += "masked_lm_labels: %s\n" % (" ".join(
            [tokenization.printable_text(x) for x in self.masked_lm_labels]))
        s += "\n"
        return s

    def __repr__(self):
        return self.__str__()
```


**这里需要关注一下，由于XLNet对NSP提出质疑：**
```
XLNet-Large does not use the objective of next sentence prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4).
```
除此以外，span_bert也得出了NSP对实验效果不好的结论：[span bert](https://zhuanlan.zhihu.com/p/75893972)

**main函数**
```python
def main(_):
    tf.logging.set_verbosity(tf.logging.INFO)

    tokenizer = tokenization.FullTokenizer(
        vocab_file=FLAGS.vocab_file)

    input_files = []
    for input_pattern in FLAGS.input_file.split(","):
        input_files.extend(tf.gfile.Glob(input_pattern))

    wrong_input_files = []
    for wrong_input_pattern in FLAGS.wrong_input_file.split(","):
        wrong_input_files.extend(tf.gfile.Glob(wrong_input_pattern))

    print(input_files)

    tf.logging.info("*** Reading from input files ***")
    for input_file in input_files:
        tf.logging.info("  %s", input_file)

    rng = random.Random(FLAGS.random_seed)
    instances = create_training_instances(
        input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,
        FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,
        rng, wrong_input_files)

    output_files = FLAGS.output_file.split(",")
    tf.logging.info("*** Writing to output files ***")
    for output_file in output_files:
        tf.logging.info("  %s", output_file)

    write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,
                                    FLAGS.max_predictions_per_seq, output_files)
```

最关键的函数create_training_instances.py, 它完成从

input_file-->all_document-->create_instances_from_document.py

--> training_instance

最终得到training_instance:
```python
                instance = TrainingInstance(
                    tokens=tokens,
                    segment_ids=segment_ids,
                    is_random_next=is_random_next,
                    masked_lm_positions=masked_lm_positions,
                    masked_lm_labels=masked_lm_labels)
```

运行：


虚拟环境出现问题，192.168.100.40服务器anaconda的base环境出现问题，导致无法启动虚拟环境。

python create_tf_record.py --input_file create_data/correct.txt --wrong_input_file create_data/wrong.txt --output_file  create_data/tf_examples.tfrecord --vocab_file   ../model/pre-trained/vocab.txt

切换到chenyi 的服务器 192.168.96.220下调试程序，也出现了conda命令失效的情况。重装anaconda，配置好环境后运行以上命令。


运行结束后再create_data目录下得到文件：tf_examples.tfrecord


## finetune
此文件是对bert model进行finetune的文件，此后需要下载bert项目文件，对其中的run_pretraining.py文件进行相应更改，以完成对bert模型的微调。

程序入口，三个文件，输入对应我们上面生成的tf_record文件，output对应finetune后模型的输出位置。
```python
if __name__ == "__main__":
  flags.mark_flag_as_required("input_file")
  flags.mark_flag_as_required("bert_config_file")
  flags.mark_flag_as_required("output_dir")
  tf.app.run()

```

[bert finetune整体流程](http://octopuscoder.github.io/2019/05/31/BERT%E4%BA%8C%E9%98%B6%E6%AE%B5fine-tune%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/)

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/bertfinetune20200107154941.png)

finetune后模型参数大小的控制：
```python
 I have exactly the same issue here, that I would like the size of the model to be small when doing the prediction. As stated by @jacobdevlin-google in #63 , the weight file contains momentum ('adam_m') and variance ('adam_v'). Then I found a solution here to exclude all Adam variables in this link

sess = tf.Session()
imported_meta = tf.train.import_meta_graph('./model.ckpt-322.meta')
imported_meta.restore(sess, './model.ckpt-322')
my_vars = []
for var in tf.all_variables():
    if 'adam_v' not in var.name and 'adam_m' not in var.name:
        my_vars.append(var)
saver = tf.train.Saver(my_vars)
saver.save(sess, './model.ckpt')
There must be some tidier solutions, but at least this one works for me, and the size of the weight file drops from 1.3GB to 400MB.
```

bert微调分了两个文件
- 一个是分类任务的微调run_classifier.py
- 一个是阅读理解任务的微调run_squad.py

预训练过程是run_pretraining.py文件，用自己的数据集对BERT模型进行预训练

之前做文本分类任务，finetune的时候只用到了run_classifier.py。

开始还不太明白这三个文件的关系，后来发现前两个都是特定的具体任务的微调，而run_classifier.py是拿数据对模型参数直接“微调”。

区别应该在于具体任务的finetune对自己的工程有更强的指导意义，而run_pretraining，只是补充自己任务特定领域的语料来对模型所有方面进行调节(masked probabilities, weight)。

具体的run_pretraining.py 要求传入文件tf_examples.tfrecord，此文件是我们通过correct.txt和wrong.txt文件生成mask概率文件，然后传入create_tf_record.py生成的文件。应该对某些汉字的mask概率具有指导意义。

此处需要注意：我们用的是char masked的bert model， 如果要切换whole word masked的哈工大讯飞全词masked模型，还需要将之前生成的概率文件，以及tf_examples.tfrecord文件作改动。

除此之外，爱奇艺生成的概率文件中，mask概率是根据形近字生成的，如果切换到我们的语音任务，需要用音近字生成。

这里先Mark一下。


