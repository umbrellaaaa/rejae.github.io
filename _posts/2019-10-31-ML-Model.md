---
layout:     post
title:      ML Model
subtitle:   summary
date:       2019-10-31
author:     RJ
header-img: 
catalog: true
tags:
    - ML

---
<p id = "build"></p>
---

一.最简单的机器学习模型：K邻近

K邻近分类：from sklearn.neighbors import KNeighborsClassifier<br>
K邻近回归：from sklearn.neighbors import KNeighborsRegressor<br>
      主要参数：邻居个数：n_neighbors<br>

二.用于回归的线性模型：

线性回归（又名普通最小二乘法，OLS）：from sklearn.linear_model import LinearRegression。<br>
Ridge岭回归（在线性回归上加上了L2正则化约束防止过拟合）：from sklearn.linear_model import Ridge,   重要参数：alpha(值越大，约束越强，模型复杂度也越小，泛化能力也越好，但在训练集上性能差），solver='sag'(数据上十万甚至百万时使用）。<br>
Lasso回归(使用L1正则化，自动化的特征选择,当有用的特征不多时使用）：from sklearn.linear_model import Lasso，  重要参数：alpha，max_iter等。<br>
ElasticNet回归（弹性回归，L1+L2正则化）：from sklearn.linear_model import ElasticNet。
SGDRegressor（梯度下降回归，数据量大于十万适用）：from sklearn.linear_model import SGDClassifier<br>

     回归算法比较：实践中一般首选线性回归和岭回归，但如果特征比较多且你发现只有几个特征是有用的，则选Lasso回归。当多个特征和另一个特征相关的时候ElasticNet回归非常有用.。数据集很大还是优先考虑 SGDRegressor，尤其特别大数据集，SGD外，几乎别无他法。跑不动。不是特别特别大的， 先试试岭回归或linear kernal的SVR（非线性）。

     L1与L2正则化的比较：假如只有几个特征是真正重要的，就用L1正则化（它更容易解释哪些特征对模型重要），否则用L2正则化。

 

三.用于分类的线性模型

Logistic回归（L2正则化）：from sklearn.linear_model import LogisticRegression,  重要参数：c(值越大，约束越弱，容易过拟合)，penalty(制定正则化是L1还是L2)，solver='sag'(数据上十万甚至百万时使用）.<br>

LinearSVC(线性支持向量机,默认L2正则化）：from sklearn.svm import LinearSVC,重要参数：c(值越大，约束越弱，容易过拟合).<br> 

SGDClassifier(梯度下降分类器，数据量大于十万适用）：from sklearn.linear_model import SGDClassifier，逻辑回归的扩展。<br>

分类算法比较：

数据量大于十万适用适用SGDClassifier，收敛速度会快很多但SGDClassifier对于特征的幅度非常敏感，也就是说，我们在把数据灌给它之前，应该先对特征做幅度调整，当然，用sklearn的StandardScaler可以很方便地完成这一点；另外由于SGDClassifier每次只使用一部分(mini-batch)做训练，在这种情况下，我们使用交叉验证(cross-validation)并不是很合适，我们会使用相对应的progressive validation。

 

四.朴素贝叶斯分类器模型（只用于分类）

GaussianNB:主要用于高维数据<br>
BernoulliNB和MultinomialNB:主要用于文本分类，稀疏计数数据，相对来说MultinomialNB更适用于包含很多非0特征的大型数据集(文档）<br>
       与线性模型相比，贝叶斯分类器训练速度更快，但是泛化能力更差。

 

五.基于决策树的集成模型(可用于回归和分类）

单决策树：DecisionTreeClassifier和DecisionTreeRegressor：from sklearn.tree import  DecisionTreeClassifier; 为防止过拟合，我们设置预剪枝参数：max_depth。
<br>
集成：随机森林（不需仔细调参）：RandomForestClassifier和RandomForestRegressor：from sklearn.ensemble import  RandomForestClassifier;  重要参数：n_estimators(树的个数），max_depth（预剪枝参数），max_feature或者max_leaf_nodes(做决策时不同特征子集中特征最大个数）
<br>

集成：梯度提升回归树（需要仔细调参，也可用于分类）：GradientBoostingClassifier和GradientBoostingRegressor：from sklearn.ensemble import GradientBoostingClassifier； 重要参数：learning_rate(学习率：每棵树纠正前一颗树的强度），n_estimators(树的个数），max_depth（预剪枝参数,梯度提升一般设置在1-5之间），max_feature或者max_leaf_nodes(做决策时不同特征子集中特征最大个数）；补充：xgboost是基于梯度提升的库，广泛应用于大规模数据集。<br>
- 1）所有树的模型都可以给出特征重要性:模型.feature_importances。

- 2）基于树的模型都不适用与高维稀疏数据。

- 3）模型不受数据缩放的影响，不需要特征预处理，比如归一化和标准化

- 4）集成模型生成的多棵树都存储在estimator_属性中：模型.estimator_。

 

六.核支持向量机（非线性）

SVM-SVC（非线性支持向量机，用于分类）:from sklearn.svm import SVC，重要参数：kernel(选择核类型），c参数（正则化参数，越大模型越复杂），gamma参数（用英语控制高斯核的宽度，越大模型越复杂）<br>
SVM-SVR（非线性支持向量机，用于回归）:from sklearn.svm import SVR
      注意：SVM对低维数据和高维数据表现都很好，但对样本非常大的情况下会面临挑战，同时SVM对数据非常敏感，要求每个    特征都大致缩放在同意范围内，故使用SVM时，数据预处理和调参都要十分小心。
