---
layout:     post
title:      Char RNN 文本生成
subtitle:   summary
date:       2019-11-13
author:     RJ
header-img: 
catalog: true
tags:
    - NLP

---
<p id = "build"></p>
---

<h1>中文简单版Char RNN</h1>

## 1. 数据准备
```python
import numpy as np
import tensorflow as tf
from tensorflow.contrib import rnn
import random
import time
from collections import Counter

start_time = time.time()

def elapsed(sec):
    if sec < 60:
        return str(sec) + " sec"

    elif sec < (60 * 60):
        return str(sec / 60) + " min"
    else:
        return str(sec / (60 * 60)) + " hr"


# Target log path
tf.reset_default_graph()
training_file = './wordstest.txt'


# 中文多文件
def readalltxt(txt_files):
    labels = []
    for txt_file in txt_files:
        target = get_ch_lable(txt_file)
        labels.append(target)
    return labels


# 中文字（处理编码问题）
def get_ch_lable(txt_file):
    labels = ""
    with open(txt_file, 'rb') as f:
        for label in f:
            # labels =label.decode('utf-8')
            labels = labels + label.decode('gb2312')

    return labels


# 优先转文件里的字符到向量（字典映射得到编号）
def get_ch_lable_v(txt_file, word_num_map, txt_label=None):
    words_size = len(word_num_map)
    to_num = lambda word: word_num_map.get(word, words_size)
    if txt_file != None:
        txt_label = get_ch_lable(txt_file)

    labels_vector = list(map(to_num, txt_label))
    return labels_vector
```

------------------------

## 2. 处理数据，利用Counter构建word_dict词表，并按词频从高到底排序
```python
training_data = get_ch_lable(training_file)

print("Loaded training data...")
training_data
len(set(training_data))

count = [('UNK', -1)]
counter = Counter(training_data)
vocab_size = len(counter)
count.extend(list(counter.most_common(vocab_size)))

dictionary = dict()
# 存入字典
for word, _ in count:
    dictionary[word] = len(dictionary)

print('字表大小:', len(dictionary))
words_size = len(dictionary)
wordlabel = get_ch_lable_v(training_file, dictionary)

words = list(dictionary.keys())
words_size = len(words)
print(words_size)
```

--------------------------------------------------
## 3. 网络模型搭建
```python
tf.reset_default_graph()

# 参数设置
learning_rate = 0.001
training_iters = 10000
display_step = 1000
n_input = 4

n_hidden1 = 256
n_hidden2 = 512
n_hidden3 = 512
# tf Graph input
x = tf.placeholder("float", [None, n_input, 1])
wordy = tf.placeholder("float", [None, words_size])

x1 = tf.reshape(x, [-1, n_input])
x2 = tf.split(x1, n_input, 1)
# 2-layer LSTM, each layer has n_hidden units.
rnn_cell = rnn.MultiRNNCell([rnn.LSTMCell(n_hidden1), rnn.LSTMCell(n_hidden2), rnn.LSTMCell(n_hidden3)])

# generate prediction
outputs, states = rnn.static_rnn(rnn_cell, x2, dtype=tf.float32)

#  last output
pred = tf.contrib.layers.fully_connected(outputs[-1], words_size, activation_fn=None)

# Loss optimizer
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=wordy))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

# Model evaluation
correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(wordy, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

savedir = "log/rnnword/"
saver = tf.train.Saver(max_to_keep=1)  # 生成saver

is_retrain = False
```

## 4. 启动session进行训练
```python
# 启动session
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    step = 0
    offset = random.randint(0, n_input + 1)
    end_offset = n_input + 1
    acc_total = 0
    loss_total = 0

    kpt = tf.train.latest_checkpoint(savedir)
    print("kpt:", kpt)
    startepo = 0
    if kpt != None and is_retrain:
        saver.restore(session, kpt)
        ind = kpt.find("-")
        startepo = int(kpt[ind + 1:])
        print(startepo)
        step = startepo

    while step < training_iters:

        # 随机取一个位置偏移

        if offset > (len(training_data) - end_offset):
            offset = random.randint(0, n_input + 1)
        # 按照指定的位置偏移获取后面的4个文字向量当作输入

        inwords = [[wordlabel[i]] for i in range(offset, offset + n_input)] 

        inwords = np.reshape(np.array(inwords), [-1, n_input, 1])

        out_onehot = np.zeros([words_size], dtype=float)
        out_onehot[wordlabel[offset + n_input]] = 1.0
        out_onehot = np.reshape(out_onehot, [1, -1])  # 所有的字都变成onehot

        _, acc, lossval, onehot_pred = session.run([optimizer, accuracy, loss, pred],
                                                   feed_dict={x: inwords, wordy: out_onehot})
        loss_total += lossval
        acc_total += acc
        if (step + 1) % display_step == 0:
            print("Iter= " + str(step + 1) + ", Average Loss= " + \
                  "{:.6f}".format(loss_total / display_step) + ", Average Accuracy= " + \
                  "{:.2f}%".format(100 * acc_total / display_step))
            acc_total = 0
            loss_total = 0
            in2 = [words[wordlabel[i]] for i in range(offset, offset + n_input)]
            out2 = words[wordlabel[offset + n_input]]
            out_pred = words[int(tf.argmax(onehot_pred, 1).eval())]
            print("%s - [%s] vs [%s]" % (in2, out2, out_pred))
            saver.save(session, savedir + "rnnwordtest.cpkt", global_step=step)
        step += 1
        offset += (n_input + 1)  # 中间隔了一个，作为预测

    print("Finished!")
    saver.save(session, savedir + "rnnwordtest.cpkt", global_step=step)
    print("Elapsed time: ", elapsed(time.time() - start_time))

    while True:
        prompt = "请输入%s个字: " % n_input
        sentence = input(prompt)
        inputword = sentence.strip()

        if len(inputword) != n_input:
            print("您输入的字符长度为：", len(inputword), "请输入4个字")
            continue
        try:
            inputword = get_ch_lable_v(None, dictionary, inputword)

            for i in range(32):
                keys = np.reshape(np.array(inputword), [-1, n_input, 1])
                onehot_pred = session.run(pred, feed_dict={x: keys})
                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())
                sentence = "%s%s" % (sentence, words[onehot_pred_index])
                inputword = inputword[1:]
                inputword.append(onehot_pred_index)
            print(sentence)
        except:
            print("该字我还没学会")

```

## 实验
用一个段落测试的时候效果还比较好，当输入一个长篇小说的时候，模型输入任何内容都输出相同的句子。这个简单的模型很容易受高频词的影响从而过拟合，最后预测的词都变成了高频词。

## 优化
处理梯度

通过采用gradient clippling的方式来防止梯度爆炸。即设置一个阈值，当gradients超过这个阈值时，就将它重置为阈值大小，这就保证了梯度不会变得很大。

![](https://pic3.zhimg.com/80/v2-9ac5baff6463dd3734a04dbbb7d8e93a_hd.png)


```python
# Loss optimizer
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=wordy))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
```
将代码替换：
```python
def build_optimizer(loss,learning_rate,grad_clip):
    tvars = tf.trainable_variables()
    grads,_ = tf.clip_by_global_norm(tf.gradients(loss,tvars),grad_clip)
    train_op = tf.train.AdamOptimizer(learning_rate)
    optimizer = train_op.apply_gradients(zip(grads,tvars))

    return optimizer

optimizer = build_optimizer(loss,learning_rate,grad_clip)
```
但是重复的问题依旧没有解决。

添加dropout
```python
rnn_cell = tf.nn.rnn_cell.DropoutWrapper(
    rnn.MultiRNNCell([rnn.LSTMCell(n_hidden1), rnn.LSTMCell(n_hidden2), rnn.LSTMCell(n_hidden3)]),
    output_keep_prob=1.0 - dropout)
```

训练loss及acc:
```python
['服', '饰', '在', '变'] - [，] vs [求]
Iter= 97000, Average Loss= 2.841114, Average Accuracy= 33.10%
['们', '依', '旧', '努'] - [力] vs [力]
Iter= 98000, Average Loss= 2.749744, Average Accuracy= 36.70%
['深', '意', '，', '但'] - [却] vs [然]
Iter= 99000, Average Loss= 2.332893, Average Accuracy= 39.20%
['拍', '照', '留', '影'] - [。] vs [。]
Iter= 100000, Average Loss= 1.798648, Average Accuracy= 53.00%
['醉', '了', '，', '情'] - [愿] vs [愿]
Finished!
```
得到结果：

请输入4个字: 我们依旧<br>
我们依旧努：！芝边都笔、前陈、缘你的远我在改那份安慰，别有直都，最我们并没有因息，还地远十，那份有年轻时年的我我离春来园”，但我们并没却有前的事有们都多人都，我班的同学热时参！情愿驳儿，别是直温、接你的多谊畅、缘，同有那几棵意大。成天再聚，都同一些奢，而我们并没有因息，珍已共人不些从道，只是直温、接你，我们何信，故。直念的故念中！芝，一年们们离开却，它是直温、接你的路自，我们学有起旧力，别是当是完，它已永...

这是在训练10w个step得到的结果，训练越久效果是越来越好的，至此，一个简单版的中文文本生成告一段落。