---
layout:     post
title:      workday15
subtitle:   
date:       2020-1-6
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

# 今日工作安排：

1. 调试FASPell代码，厘清各部分功能
2. 关注影响模型可调节的超参数
3. 跟进上一位同学的调试结果

# FASPell project

## 工作目录

```
FASPell/
  - bert_modified/
      - create_data.py
      - create_tf_record.py
      - modeling.py
      - tokenization.py
  - data/
      - char_meta.txt
  - model/
      - fine-tuned/
          - model.ckpt-10000.data-00000-of-00001
          - model.ckpt-10000.index
          - model.ckpt-10000.meta
      - pre-trained/
          - bert_config.json
          - bert_model.ckpt.data-00000-of-00001
          - bert_model.ckpt.index
          - bert_model.ckpt.meta
          - vocab.txt
  - plots/
      ...
  - char_sim.py
  - faspell.py
  - faspell_configs.json
  - masked_lm.py
  - plot.py
```

## 配置参数
```

{
  "general_configs": {
    "rank": 4,
    "round": 1,
    "weights": {
      "visual": 0,
      "phonological": 1
    },
    "char_meta": "data/char_meta.txt",
    "lm": {
      "max_seq": 128,
      "top_n": 5,
      "batch_size": 5,
      "vocab": "model/pre-trained/vocab.txt",
      "bert_configs": "model/pre-trained/bert_config.json",
      "pre-trained": "model/pre-trained/bert_model.ckpt",
      "fine-tuned": "model/fine-tuned_nlp/model.ckpt-1200",
      "fine_tuning_is_on": true
    }
  },
  "exp_configs": {
    "union_of_sims": false,
    "testing_set": "./data/test_nlp.txt",
    "training_set": "./data/train_final.txt",
    "tackle_n_gram_bias": true,
    "dump_candidates": "",
    "read_from_dump": false,
    "filter_is_on": true,
    "store_plots": "plots/sound_false",
    "store_latex": ""
  }
}
```

## 运行流程：
```
$ cd bert_modified
$ python create_data.py -f /path/to/training/data/file
$ python create_tf_record.py 
                            --input_file correct.txt 
                            --wrong_input_file wrong.txt 
                            --output_file  tf_examples.tfrecord 
                            --vocab_file   ../model/pre-trained/vocab.txt


python3 run_pretraining.py \
      --input_file=/tmp/tf_examples.tfrecord \
      --output_dir=/tmp/pretraining_output \
      --do_train=True \
      --do_eval=True \
      --bert_config_file=./tmp/model/bert_config.json \
      --init_checkpoint=./tmp/model/bert_model.ckpt \
      --train_batch_size=32 \
      --max_seq_length=128 \
      --max_predictions_per_seq=20 \
      --num_train_steps=20 \
      --num_warmup_steps=10 \
      --learning_rate=2e-5
```

## issue问题
在github的issue中，大部分问题都围绕着char_meta.txt这个文件，因为这个文件没有开源，而且这个文件是是CSD即confidence-similarity-Decoder的关键，大家都在围绕这个问题讨论。

这个文件的格式如下：

编码值 ; 汉字; 汉字五种语言发音（MC CC JO K V 分别是普通话，粤语，日语，韩文，越南语）; 汉字的详细笔画结构信息
```
U+5E74	年	ning4,nian2;nin4;NYEN;NEN;nên	⿱⿰丿一⿻⿳一丨一丨
U+725B	牛	niu2;ngau4;WU;GYUU;ngưu	⿻⿰丿一⿻一丨
U+5343	千	qian1;cin1;CHEN;SEN;thiên	⿱丿⿻一丨
U+7530	田	tian2;tin4;CEN;DEN,TEN;điền	⿵⿰丨𠃌⿱⿻一丨一
U+7531	由	you2,yao1;jau4;YU;YUU,YUI,YU;do	⿻⿰丨𠃌⿱⿻一丨一
U+5348	午	wu3;ng5;O;GO;ngọ	⿱⿰丿一⿻一丨
```
其中 ⿱⿰ 表示上下结构和左右结构，论文中，以“贫”字作为示例，如下图所示：
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20200106144414faspell.png)

以上结构切分还可以通过开源的ids.txt获得，但是具体的笔画还需要递归获得
it should be easy to use tree recursion (starting from the IDS of simple characters which do have stroke-level IDS) to produce stroke-level IDS for all characters yourself.


为什么要用5种语言的发音呢？
the combination of the pronunciations in multiple languages gives us a more continuous phonological similarity.

多种语言中发音的组合给了我们一个更连续的语音相似性


跟上一位负责这个项目的同学沟通了一下，这个char_meta.txt文件需要自己动手构建。他花了一些时间构建了char_meta.txt的一个版本，但相关的构建过程的文件留在了本地文件下，没保存，所以这块我要自己动手去构建，才能发现构建过程中的一些问题。

除此之外，由于刚接手这个项目，项目里的文件，比最初github上下载下来的文件多了很多，包括一些txt，和model文件。因而不能轻易调试一些文件，所以我重新在192.168.100.40服务器重头开始调试这个项目。




## 开始调试

python create_data.py -f ../data/ocr_train_3575.txt -o ./create_data

得到三个文件：
- correct.txt
- wrong.txt
- mask_probability.sav

其中，第三个文件是由pickle命令保存的：
```
  pickle.dump(proportions, open(os.path.join(args.output, 'mask_probability.sav'), 'wb'))
```
打开这个pickle文件：
```python
import pickle       #使用该模块前需导入

f = open('test_pickle.txt','wb')
inlet = '''这只是一个小小的实例
请一定要在电脑上自己敲出来
只有这样
你才能学好编程
'''
pickle.dump(inlet,f)     #将字符串以二进制的形式写入到'test.txt'中
f.close()
#--------------
f = open('test_pickle.txt','rb')
result = pickle.load(f)
print(result)
f.close()
```

查看该文件，发现保存的都是ocr_train_3575.txt中顺序字典存储的唯一词对应的masked概率。

计算方式：proportions[k] = min(top_confusions[k] / correct_count[k], 1.0)

即masked的概率计算方式通过，词组对中错误的字符count和正确的字符count的比例，即错误的次数越多，masked的概率越高。
```
...
) : 0.9722222222222222
包 : 0.03225806451612903
括 : 0.14285714285714285
的 : 0.0012433944668946222
一 : 0.005747126436781609
些 : 0.016666666666666666
碰 : 0.16666666666666666
啊 : 0.01098901098901099
撞 : 0.2
我 : 0.0011507479861910242
这 : 0.0021413276231263384
...
```

注意，这里的概率是针对正确的句子的masked概率。因为撞字对应的栓字不在此数据中。

