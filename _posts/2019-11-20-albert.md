---
layout:     post
title:      Albert
subtitle:   summary
date:       2019-11-20
author:     RJ
header-img: 
catalog: true
tags:
    - DL

---
<p id = "build"></p>
---


论文地址：[ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://openreview.net/pdf?id=H1eA7AEtvS)

## abstract

Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a **self-supervised loss** that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having **fewer parameters** compared to BERT-large.


ALBERT主要对BERT做了两点改进，缩小了整体的参数量加快了训练速度，提高了模型效果。

## 1. INTRODUCTION
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120table1.jpg)
 Table 1 and Fig. 1 show

a typical example, where we simply increase the hidden size of BERT-large to be 2x larger and get
worse results with this BERT-xlarge model.
对特定的任务而言模型大不一定更好！

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120table2.jpg)

针对上述问题的现有解决方案包括模型并行化(Shoeybi et al.， 2019)和智能内存管理(Chen et al.， 2016;Gomez等人，2017)。这些解决方案解决了内存限制问题，但没有解决**通信开销和模型退化问题**。在本文中，我们通过设计一个比传统BERT体系结构参数少得多的Lite BERT (ALBERT)来解决所有上述问题。

ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models： 

- The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. (第一个技术是对嵌入参数进行因式分解。研究者将大的词汇嵌入矩阵分解为两个小的矩阵，从而将隐藏层的大小与词汇嵌入的大小分离开来。这种分离使得隐藏层的增加更加容易，同时不显著增加词汇嵌入的参数量。)
- The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.(第二种技术是跨层参数共享。这一技术可以避免参数量随着网络深度的增加而增加。两种技术都显著降低了 BERT 的参数量，同时不对其性能造成明显影响，从而提升了参数效率。ALBERT 的配置类似于 BERT-large，但参数量仅为后者的 1/18，训练速度却是后者的 1.7 倍。)


**To further improve the performance of ALBERT:**

 we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.

SOP关注句子间的连贯性 VS NSP下一句预测



## 2. RELATED WORK
### 2.1 SCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE
scaling up representation learning for natural language is not as easy as simply increasing model size. In addition, it is difficult to experiment with large models due to computational constraints, especially in terms of GPU/TPU memory limitations. Given that current state-of-the-art models often
have hundreds of millions or even billions of parameters, we can easily hit memory limits. To address this issue,：
- Chen et al. (2016) propose a method called gradient checkpointing to reduce the
memory requirement to be sublinear at the cost of an extra forward pass. 
- Gomez et al. (2017) propose a way to reconstruct each layer’s activations from the next layer so that they do not need to store the intermediate activations. Both methods reduce the memory consumption at the cost of speed.
- **In contrast, our parameter-reduction techniques reduce memory consumption and increase training
speed.**



### 2.2 CROSS-LAYER PARAMETER SHARING
The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting. Different from our observations, Dehghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard
transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard
transformer.

该Trick本质上就是对参数共享机制在Transformer内的探讨。在Transfor中有两大主要的组件：FFN与多头注意力机制。ALBERT主要是对这两大组件的共享机制进行探讨。
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120crosslayershare1.jpg)

论文里采用了四种方式： all-shared，shared-attention，shared-FFN以及 not-shared。我们首选关注一下参数量，not-shared与all-shared的参数量相差极为明显，因此可以得出共享机制才是参数量大幅减少的根本原因。然后，我们看到，**只共享Attention参数能够获得参数量与性能的权衡**。最后，很明显的就是，随着层数的加深，基于共享机制的 ALBERT 参数量与BERT参数量相比下降的更加明显。



### 2.3 SENTENCE ORDERING OBJECTIVES
ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. 

Several researchers have experimented with pretraining objectives that similarly relate to discourse coherence. Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979; Halliday & Hasan, 1976; Grosz et al., 1995). 
- Most objectives found effective in practice are quite simple. Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. 
- Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). 
- Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is defined on textual segments rather than sentences. BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and find that sentence ordering is a more challenging pretraining task and more useful for certain downstream tasks. 
- Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classification task rather than empirically comparing the two.



## 3 THE ELEMENTS OF ALBERT
In this section, we present the design decisions for ALBERT and provide quantified comparisons
against corresponding configurations of the original BERT architecture (Devlin et al., 2019).

### 3.1 MODEL ARCHITECTURE CHOICES
The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with **GELU** nonlinearities (Hendrycks & Gimpel, 2016).(GELUs正是在激活中引入了随机正则的思想，是一种对神经元输入的概率描述，直观上更符合自然的认识，同时实验效果要比Relus与ELUs都要好。)

We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H. Following Devlin et al. (2019), we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64. There are three main contributions that ALBERT makes over the design choices of BERT.
#### 嵌入向量参数化的因式分解

在 BERT 以及后续的 XLNet 和 RoBERTa 中，WordPiece 词嵌入大小 E 和隐藏层大小 H 是相等的，即 E ≡ H。由于建模和实际使用的原因，这个决策看起来可能并不是最优的。

- 从建模的角度来说，WordPiece 词嵌入的目标是学习上下文无关的表示，而隐藏层嵌入的目标是学习上下文相关的表示。通过上下文相关的实验，BERT 的表征能力很大一部分来自于使用上下文为学习过程提供上下文相关的表征信号。因此，将 WordPiece 词嵌入大小 E 从隐藏层大小 H 分离出来，可以更高效地利用总体的模型参数，其中 H 要远远大于 E。
- 从实践的角度，自然语言处理使用的词典大小 V 非常庞大，如果 E 恒等于 H，那么增加 H 将直接加大嵌入矩阵的大小，这种增加还会通过 V 进行放大。

因此，对于 ALBERT 而言，研究者对词嵌入参数进行了因式分解，将它们分解为两个小矩阵。研究者不再将 one-hot 向量直接映射到大小为 H 的隐藏空间，而是先将它们映射到一个低维词嵌入空间 E，然后再映射到隐藏空间。通过这种分解，研究者可以将词嵌入参数从 O(V × H) 降低到 O(V × E + E × H)，这在 H 远远大于 E 的时候，参数量减少得非常明显。

 O(V * H) to O(V * E + E * H)
 
 如以ALBert_xxlarge为例，V=30000, H=4096, E=128
   
 那么原先参数为V * H= 30000 * 4096 = 1.23亿个参数，现在则为V * E + E * H = 30000*128+128*4096 = 384万 + 52万 = 436万，
   
 词嵌入相关的参数变化前是变换后的28倍


在bert以及诸多bert的改进版中，embedding size都是等于hidden size的，这不一定是最优的。因为bert的token embedding是上下文无关的，而经过multi-head attention+ffn后的hidden embedding是上下文相关的，bert预训练的目的是提供更准确的hidden embedding，而不是token embedding，因此token embedding没有必要和hidden embedding一样大。albert将token embedding进行了分解，首先降低embedding size的大小，然后用一个Dense操作将低维的token embedding映射回hidden size的大小。bert的embedding size=hidden size，因此词向量的参数量是vocab size * hidden size，进行分解后的参数量是vocab size * embedding size + embedding size * hidden size，只要embedding size << hidden size，就能起到减少参数的效果。

#### 跨层权重共享

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120crosslayerstructure.jpg)
跨层参数共享，就是不管12层还是24层都只用一个transformer。来源于论文Universal Transformer.

跨层参数共享（参数量减少主要贡献）

- 这主要是为了减少参数量（性能轻微降低，参数大量减少，这整体上是好的事情）
- 论文里的消融实验的分数也说明no-share的分数是最高的

对于 ALBERT，研究者提出了另一种跨层参数共享机制来进一步提升参数效率。其实目前有很多方式来共享参数，例如只贡献前馈网络不同层之间的参数，或者只贡献注意力机制的参数，而 ALBERT 采用的是贡献所有层的所有参数。参数共享能显著减少参数。共享可以分为全连接层、注意力层的参数共享；**注意力层的参数对效果的减弱影响小一点。**

作者对比了每层输入输出的L2距离和相似度，发现了BERT的结果比较震荡，而ALBERT就很稳定，可见ALBERT有稳定网络参数的作用。

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120crosslayerfigure1.jpg)

#### 句间连贯性损失
后BERT时代很多研究(XLNet、RoBERTa)都发现next sentence prediction没什么用处，所以作者也审视了一下这个问题，认为NSP之所以没用是因为这个任务不仅包含了句间关系预测，也包含了主题预测，而主题预测显然更简单些（比如一句话来自新闻财经，一句话来自文学小说），模型会倾向于通过主题的关联去预测。因此换成了SOP(sentence order prediction)，预测两句话有没有被交换过顺序。实验显示新增的任务有1个点的提升：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120SOPtable.jpg)




### 3.2 MODEL SETUP
We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 2. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models. For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. If we set BERT to have an extra-large size with H = 2048, we end up with a model that has 1.27 billion parameters and under-performs (Fig. 1). In contrast, an ALBERT-xlarge configuration with H = 2048 has only 60M parameters, while an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERT-large’s parameters. Note that for ALBERTxxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive. This improvement in parameter efficiency is the most important advantage of ALBERT’s design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail.
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120parameterconfigurationtable2.jpg)
参数量级上的对比如上表所示，十分明显。这里需要提到的是ALBERT-xxlarge，它只有12层，但是隐层维度高达4096，这是考虑到深层网络的计算量问题，其本质上是一个浅而宽的网络。

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120contrastberttable3.jpg)
实验使用的xlarge是24层，2048维度，xxlarge是12层，4096维度。可以仔细看一下model的量级，并且注意一下这个speedup是训练时间而不是inference时间（因为数据少了，分布式训练时吞吐上去了，所以ALBERT训练更快），但inference还是需要和BERT一样的transformer计算。

- ALBERT-xxlarge的表现完全超过了BERT-large的表现，但是BERT-large的速度是要比ALBERT-xxlarge快3倍左右的。其次，BERT-xlarge的表现反而变差。
- 在相同的训练时间下，ALBERT得到的效果确实比BERT好。
- 在相同的Inference时间下，ALBERT base和large的效果都是没有BERT好的，而且差了2-3个点，作者在最后也提到了会继续寻找提高速度的方法（sparse attention和block attention）。

## 4 EXPERIMENTAL RESULTS

### 4.8 CURRENT STATE-OF-THE-ART ON NLU TASKS

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120glueboard.jpg)
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120gluemodel.jpg)


## 5 DISCUSSION

- While ALBERT-xxlarge has **less parameters** than BERT-large and gets significantly better results, it is computationally more expensive due to its **larger structure**. 
- An important next step is thus to speed up the training and inference speed of ALBERT through methods like **sparse attention** (Child et al.,2019) and **block attention** (Shen et al., 2018). 
- An orthogonal line of research, which could provide additional representation power, includes **hard example mining** (Mikolov et al., 2013) and **more efficient language modeling training** (Yang et al., 2019). 
- Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be **more dimensions not yet captured** by the current self-supervised training losses that could create additional representation power for the resulting representations.


ALBERT 的目的论文开篇就提到，在预训练语言模型领域，增大模型往往能够到来不错的效果提升，但这种提升不是无止境的。预训练语言模型已经很大了，大到绝大多数的实验室和公司都没有资格参与这场游戏，对于大模型而言，一个很浅的idea就是：如何对大模型进行压缩？ ALBERT 本质就是对 BERT 模型压缩后的产物。模型压缩有很多手段，包括剪枝，参数共享，低秩分解，网络结构设计，知识蒸馏等。ALBERT 也没能逃出这一框架，它其实是一个相当工程化的思想，它的两大压缩Trick 也很容易想到, 即
- Factorized embedding parameterization - 低秩分解
- Cross-layer parameter sharing - 参数共享机制



## 参考

[ALBERT 告诉了我们什么？](https://zhuanlan.zhihu.com/p/92849070)  <br>
[ALBERT粗读](https://zhuanlan.zhihu.com/p/84273154)   <br>
[解读ALBERT](https://blog.csdn.net/weixin_37947156/article/details/101529943)  <br>
[Gaussian Error Linerar Units(GELUS)](https://arxiv.org/abs/1606.08415)<br>
[GELUS解读](https://blog.csdn.net/liruihongbob/article/details/86510622)<br>
[Universal Transformers](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1807.03819.pdf)<br>
[UT解读](https://zhuanlan.zhihu.com/p/44655133)<br>

## 问答
机器之心：我们知道 ALBERT 中使用的参数化因式分解和参数共享两个方法，它们分别是怎样提升了模型的表现呢？

蓝振忠：这两个方法实际上都是将模型的参数进行降低。从架构的角度来看，可以把 BERT 或者 Transformer 的架构分为两个部分。其中一个是把单词投射到隐层。另一个部分是在隐层上做迭代的词向量变换。两块的模型参数都比较大。所以，我们如果想整体的去减少整个模型的参数量，就需要把两块的参数量都降下来。参数的因式分解和共享刚好对应的是这两个模块的降低参数量的方法。除了这两个方法可以降低参数量，从而降低模型的内存占用，我们还发现去掉 Dropout 以后可以显著的降低内存占用。

机器之心：为什么说去掉 Dropout 可以减少内存的占用呢？

蓝振忠：在计算的时候，除了参数会占用内存之外，还有临时变量也会占用内存。去掉 Dropout 可以显著减少临时变量对内存的占用，所以提升了性能。

机器之心：现在是否可以说，预训练模型已经可以应用在移动设备上了？如果不成熟的话，还有哪些技术难点需要解决？

蓝振忠：我觉得已经可以应用了。最近有一个这样研究，是 ChineseGLUE 的项目组做的，是 ALBERT 的中文版。他们训练了一个 ALBERT-Tiny 模型。我在谷歌的同事将它转成 tensorflow-lite 之后在手机端上做了一些测试。在 4 线程的 CPU 上的延时是 120 毫秒左右；这个是符合某些手机端的要求的。当然，我们最近还在做一些更新，还有进一步提升的空间。

## 实验
使用github[开源代码](https://github.com/brightmart/albert_zh)
1. 在windows下需要自己写入相关配置参数
2. 配置自己的数据
```python
if FLAGS.do_train:
    train_examples = processor.get_train_examples(FLAGS.data_dir) 

def get_train_examples(self, data_dir):
    """See base class."""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, "train_0827.tsv")), "train")

def _create_examples(self, lines, set_type):
    """Creates examples for the training and dev sets."""
    examples = []
    print("length of lines:", len(lines))
    for (i, line) in enumerate(lines):
        # print('#i:',i,line)
        if i == 0:
            continue
        guid = "%s-%s" % (set_type, i)
        try:
            label = tokenization.convert_to_unicode(line[0])
            text_a = tokenization.convert_to_unicode(line[1])
            #text_b = tokenization.convert_to_unicode(line[1])
            examples.append(
                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
        except Exception:
            print('###error.i:', i, line)
    return examples
```


开启训练后始终看不到loss和acc，只有一下输出：
```
I1121 13:41:06.619483 26056 tpu_estimator.py:2159] global_step/sec: 0.882243
INFO:tensorflow:examples/sec: 28.2318
I1121 13:41:06.619483 26056 tpu_estimator.py:2160] examples/sec: 28.2318
INFO:tensorflow:global_step/sec: 0.886527
```
查阅资料后说是一个[bug](https://github.com/google-research/bert/issues/70)，解决办法如下：

```
#search  keyword TPUEstimatorSpec  
logging_hook = tf.train.LoggingTensorHook({"loss": total_loss}, every_n_iter=10)
output_spec = tf.contrib.tpu.TPUEstimatorSpec(
mode=mode,
loss=total_loss,
train_op=train_op,
training_hooks=[logging_hook],
scaffold_fn=scaffold_fn)
```

第一次尝试运行模型，记录问题和结果：
```
***** Eval results output\model.ckpt-1000 *****
eval_accuracy = 0.10002001
eval_loss = 5.3717093
global_step = 1000
loss = 5.371818
***** Eval results output\model.ckpt-2000 *****
eval_accuracy = 0.10002001
eval_loss = 7.076848
global_step = 2000
loss = 7.076906
***** Eval results output\model.ckpt-3000 *****
eval_accuracy = 0.10002001
eval_loss = 3.559589
global_step = 3000
loss = 3.5590944
***** Eval results output\model.ckpt-4000 *****
eval_accuracy = 0.10002001
eval_loss = 6.765761
global_step = 4000
loss = 6.7658377
***** Eval results output\model.ckpt-4687 *****
eval_accuracy = 0.10002001
eval_loss = 5.5797668
global_step = 4687
loss = 5.5786552
```
loss值降不下去，eval_accuracy全都相同。

## 深入模型代码分析

```python

# This function is not used by this file but is still used by the Colab and
# people who depend on it.
def convert_examples_to_features(examples, label_list, max_seq_length,
                                 tokenizer):
    """Convert a set of `InputExample`s to a list of `InputFeatures`."""

    features = []
    for (ex_index, example) in enumerate(examples):
        if ex_index % 10000 == 0:
            tf.logging.info("Writing example %d of %d" % (ex_index, len(examples)))

        feature = convert_single_example(ex_index, example, label_list,
                                         max_seq_length, tokenizer)

        features.append(feature)
    return features

```

```python

def convert_single_example(ex_index, example, label_list, max_seq_length,
                           tokenizer):
    """Converts a single `InputExample` into a single `InputFeatures`."""

    if isinstance(example, PaddingInputExample):
        return InputFeatures(
            input_ids=[0] * max_seq_length,
            input_mask=[0] * max_seq_length,
            segment_ids=[0] * max_seq_length,
            label_id=0,
            is_real_example=False)

    label_map = {}
    for (i, label) in enumerate(label_list):
        label_map[label] = i

    tokens_a = tokenizer.tokenize(example.text_a)
    tokens_b = None
    if example.text_b:
        tokens_b = tokenizer.tokenize(example.text_b)

    if tokens_b:
        # Modifies `tokens_a` and `tokens_b` in place so that the total
        # length is less than the specified length.
        # Account for [CLS], [SEP], [SEP] with "- 3"
        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
    else:
        # Account for [CLS] and [SEP] with "- 2"
        if len(tokens_a) > max_seq_length - 2:
            tokens_a = tokens_a[0:(max_seq_length - 2)]

    # The convention in BERT is:
    # (a) For sequence pairs:
    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
    # (b) For single sequences:
    #  tokens:   [CLS] the dog is hairy . [SEP]
    #  type_ids: 0     0   0   0  0     0 0
    #
    # Where "type_ids" are used to indicate whether this is the first
    # sequence or the second sequence. The embedding vectors for `type=0` and
    # `type=1` were learned during pre-training and are added to the wordpiece
    # embedding vector (and position vector). This is not *strictly* necessary
    # since the [SEP] token unambiguously separates the sequences, but it makes
    # it easier for the model to learn the concept of sequences.
    #
    # For classification tasks, the first vector (corresponding to [CLS]) is
    # used as the "sentence vector". Note that this only makes sense because
    # the entire model is fine-tuned.
    tokens = []
    segment_ids = []
    tokens.append("[CLS]")
    segment_ids.append(0)
    for token in tokens_a:
        tokens.append(token)
        segment_ids.append(0)
    tokens.append("[SEP]")
    segment_ids.append(0)

    if tokens_b:
        for token in tokens_b:
            tokens.append(token)
            segment_ids.append(1)
        tokens.append("[SEP]")
        segment_ids.append(1)

    input_ids = tokenizer.convert_tokens_to_ids(tokens)

    # The mask has 1 for real tokens and 0 for padding tokens. Only real
    # tokens are attended to.
    input_mask = [1] * len(input_ids)

    # Zero-pad up to the sequence length.
    while len(input_ids) < max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)

    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length

    label_id = label_map[example.label]
    if ex_index < 5:
        tf.logging.info("*** Example ***")
        tf.logging.info("guid: %s" % (example.guid))
        tf.logging.info("tokens: %s" % " ".join(
            [tokenization.printable_text(x) for x in tokens]))
        tf.logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
        tf.logging.info("input_mask: %s" % " ".join([str(x) for x in input_mask]))
        tf.logging.info("segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
        tf.logging.info("label: %s (id = %d)" % (example.label, label_id))

    feature = InputFeatures(
        input_ids=input_ids,
        input_mask=input_mask,
        segment_ids=segment_ids,
        label_id=label_id,
        is_real_example=True)
    return feature
```

## run_classifier解读
五个class:

    - class InputExample(object):
    - class PaddingInputExample(object):
    - class InputFeatures(object):
    - class DataProcessor(object):
    - class TextClassificationProcessor(DataProcessor):

其中TextClassificationProcessor是需要自己实现的，相关的数据处理方式在方法_create_examples中：
```python
class TextClassificationProcessor(DataProcessor):  # TODO NEED CHANGE2
    """Processor for the internal data set. sentence pair classification"""

    def __init__(self):
        self.language = "zh"

    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.txt")), "train")
        # dev_0827.tsv

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.txt")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "test.txt")), "test")

    def get_labels(self):
        """See base class."""
        return ['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经']
        # return ["-1","0", "1"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training and dev sets."""
        examples = []
        print("length of lines:", len(lines))
        for (i, line) in enumerate(lines):
            #print('#i:',i,line)

            guid = "%s-%s" % (set_type, i)
            try:
                label = tokenization.convert_to_unicode(line[0])
                text_a = tokenization.convert_to_unicode(line[1])
                # text_b = tokenization.convert_to_unicode(line[1])
                examples.append(
                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
            except Exception:
                print('###error.i:', i, line)
        return examples
```


七个method:
- convert_single_example
    - _truncate_seq_pair
- convert_examples_to_features
    - convert_single_example
- file_based_convert_examples_to_features
    - convert_single_example
- create_model
    - layer_norm

- model_fn_builder
- input_fn_builder
- file_based_input_fn_builder

## Bert  run_classifier.py 注解版
```python
# coding=utf-8

# bert注解版
# raw author: Google
# explain author：putdoor
"""BERT finetuning runner."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import csv
import os
import modeling
import random
import optimization  # 优化器
import tokenization  # 令牌化
import tensorflow as tf

flags = tf.flags

FLAGS = flags.FLAGS
# 用于支持命令行传递参数：如：python test_flags.py --model "My model"
## Required parameters  # 这5项参数为必填参数，把None改为自己所需要
flags.DEFINE_string(  
    "data_dir", "/home/work/my_model/bert/corpus/sentiment",  # 此sentiment下应有三个文件，名称分别为:train.tsv, eval.tsv, test.tsv
    "The input data dir. Should contain the .tsv files (or other data files) "
    "for the task.")

flags.DEFINE_string(
    "bert_config_file", os.path.join(os.path.dirname(os.path.abspath(__file__)), 'chinese_L-12_H-768_A-12/bert_config.json'),
    "The config json file corresponding to the pre-trained BERT model. "
    "This specifies the model architecture.")

flags.DEFINE_string("task_name", "sentiment", "The name of the task to train.")  # 自己随意起一个便于标识processor类的任务名,后续需填入main函数的字典

flags.DEFINE_string("vocab_file", os.path.join(os.path.dirname(os.path.abspath(__file__)), 'chinese_L-12_H-768_A-12/vocab.txt'),
                    "The vocabulary file that the BERT model was trained on.")

flags.DEFINE_string(
    "output_dir", "/home/work/my_model/output",
    "The output directory where the model checkpoints will be written.")

# Other parameters
flags.DEFINE_string(
    "init_checkpoint", None,
    "Initial checkpoint (usually from a pre-trained BERT model).")

flags.DEFINE_bool(
    "do_lower_case", True,
    "Whether to lower case the input text. Should be True for uncased "
    "models and False for cased models.")

flags.DEFINE_integer(
    "max_seq_length", 128,
    "The maximum total input sequence length after WordPiece tokenization. "
    "Sequences longer than this will be truncated, and sequences shorter "
    "than this will be padded.")

flags.DEFINE_bool("do_train", False, "Whether to run training.")

flags.DEFINE_bool("do_eval", False, "Whether to run eval on the dev set.")

flags.DEFINE_bool(
    "do_predict", False,
    "Whether to run the model in inference mode on the test set.")

flags.DEFINE_integer("train_batch_size", 32, "Total batch size for training.")  # mini-batch方式的梯度下降，每批处理32个样本

flags.DEFINE_integer("eval_batch_size", 8, "Total batch size for eval.")  # batch为每批样本的数量，每批更新一次权重，使loss最小

flags.DEFINE_integer("predict_batch_size", 8, "Total batch size for predict.")

flags.DEFINE_float("learning_rate", 5e-5, "The initial learning rate for Adam.")

flags.DEFINE_float("num_train_epochs", 3.0,  # 所有样本轮一次，即为一个epoch，增大此值，计算量增大，一个20000条数据的二分类问题，epochs=4大概要10分钟(16G 单GPU)
                   "Total number of training epochs to perform.")

flags.DEFINE_float(
    "warmup_proportion", 0.1,  # 预热训练中，线性地增加学习率，详见注意力机制部分
    "Proportion of training to perform linear learning rate warmup for. "
    "E.g., 0.1 = 10% of training.")

flags.DEFINE_integer("save_checkpoints_steps", 1000,  # 保存检查点时的步数，达到1000时，保存一次模型
                     "How often to save the model checkpoint.")

flags.DEFINE_integer("iterations_per_loop", 1000,  # 在每个estimator调用中执行多少步骤
                     "How many steps to make in each estimator call.")

flags.DEFINE_bool("use_tpu", False, "Whether to use TPU or GPU/CPU.")

# TPU config：
tf.flags.DEFINE_string(
    "tpu_name", None,
    "The Cloud TPU to use for training. This should be either the name "
    "used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 "
    "url.")

tf.flags.DEFINE_string(
    "tpu_zone", None,
    "[Optional] GCE zone where the Cloud TPU is located in. If not "
    "specified, we will attempt to automatically detect the GCE project from "
    "metadata.")

tf.flags.DEFINE_string(
    "gcp_project", None,
    "[Optional] Project name for the Cloud TPU-enabled project. If not "
    "specified, we will attempt to automatically detect the GCE project from "
    "metadata.")

tf.flags.DEFINE_string("master", None, "[Optional] TensorFlow master URL.")

flags.DEFINE_integer(
    "num_tpu_cores", 8,
    "Only used if `use_tpu` is True. Total number of TPU cores to use.")


class InputExample(object):  # 每一行数据 to Inputexample对象
  """A single training/test example for simple sequence classification."""

  def __init__(self, guid, text_a, text_b=None, label=None):
    """Constructs a InputExample.

    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """
    self.guid = guid
    self.text_a = text_a
    self.text_b = text_b
    self.label = label


class PaddingInputExample(object):
"""当需要使用TPU训练时，eval和predict的数据需要是batch_size的整数倍，此类用于处理这类情况"""


class InputFeatures(object):
  """A single set of features of data."""

  def __init__(self,
               input_ids,  # 输入部分：token embedding：表示词向量，第一个词是CLS，分隔词有SEP，是单词本身
               input_mask,  # 输入部分：position embedding：为了令transformer感知词与词之间的位置关系
               segment_ids,  # 输入部分：segment embedding：text_a与text_b的句子关系
               label_id,  # 输出部分：标签，对应Y
               is_real_example=True):
    self.input_ids = input_ids
    self.input_mask = input_mask
    self.segment_ids = segment_ids
    self.label_id = label_id
    self.is_real_example = is_real_example


class DataProcessor(object):
  """Base class for data converters for sequence classification data sets."""

  def get_train_examples(self, data_dir):
    """Gets a collection of `InputExample`s for the train set."""
    raise NotImplementedError()

  def get_dev_examples(self, data_dir):
    """Gets a collection of `InputExample`s for the dev set."""
    raise NotImplementedError()

  def get_test_examples(self, data_dir):
    """Gets a collection of `InputExample`s for prediction."""
    raise NotImplementedError()

  def get_labels(self):
    """Gets the list of labels for this data set."""
    raise NotImplementedError()

  @classmethod
  def _read_tsv(cls, input_file, quotechar=None):
    """Reads a tab separated value file."""
    with tf.gfile.Open(input_file, "r") as f:
      reader = csv.reader(f, delimiter="\t", quotechar=quotechar)
      lines = []
      for line in reader:
        lines.append(line)
      return lines


class SentimentProcessor(DataProcessor):  # 自定义的类，用于处理二分类问题
    """Processor for the CoLA data set (GLUE version)."""
    def get_train_examples(self, data_dir):
        """See base class."""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):  # 二分类问题返回的标签值为0，1
        """See base class."""
        return ["0", "1"]

    def _create_examples(self, lines, set_type):
        """Creates examples for the training and dev sets."""
        examples = []
        for (i, line) in enumerate(lines):
            # Only the test set has a header
            guid = "%s-%s" % (set_type, i)
            text_a = tokenization.convert_to_unicode(line[1])
            label = tokenization.convert_to_unicode(line[0])
            examples.append(
                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
        return examples


def convert_single_example(ex_index, example, label_list, max_seq_length,
                           tokenizer):
  """Converts a single `InputExample` into a single `InputFeatures`."""

  if isinstance(example, PaddingInputExample):  # 相当于实例example为空，返回的数据
    return InputFeatures(
        input_ids=[0] * max_seq_length,
        input_mask=[0] * max_seq_length,
        segment_ids=[0] * max_seq_length,
        label_id=0,
        is_real_example=False)

  label_map = {}
  for (i, label) in enumerate(label_list):  # 标签映射
    label_map[label] = i

  tokens_a = tokenizer.tokenize(example.text_a)
  tokens_b = None
  
  if example.text_b:
    tokens_b = tokenizer.tokenize(example.text_b)

  if tokens_b:
    # Modifies `tokens_a` and `tokens_b` in place so that the total
    # length is less than the specified length.
    # Account for [CLS], [SEP], [SEP] with "- 3"
    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)  # 截断序列对：将序列截断至最大允许长度
  else:
    # Account for [CLS] and [SEP] with "- 2"
    if len(tokens_a) > max_seq_length - 2:
      tokens_a = tokens_a[0:(max_seq_length - 2)]

  # The convention in BERT is:
  # (a) For sequence pairs:
  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
  # (b) For single sequences:
  #  tokens:   [CLS] the dog is hairy . [SEP]
  #  type_ids: 0     0   0   0  0     0 0
  tokens = []
  segment_ids = []
  tokens.append("[CLS]")
  segment_ids.append(0)
  for token in tokens_a:
    tokens.append(token)
    segment_ids.append(0)
  tokens.append("[SEP]")
  segment_ids.append(0)

  if tokens_b:
    for token in tokens_b:
      tokens.append(token)
      segment_ids.append(1)
    tokens.append("[SEP]")
    segment_ids.append(1)

  # tokenizer ：是bert源码中提供的模块，其实主要作用就是将句子拆分成字，并且将字映射成id
  input_ids = tokenizer.convert_tokens_to_ids(tokens)  

  input_mask = [1] * len(input_ids)  # 暂时认为词与词之间的位置关系由索引决定就可以了[1,1,1...] --> index: 0,1,2...

  # Zero-pad up to the sequence length.
  while len(input_ids) < max_seq_length:
    input_ids.append(0)
    input_mask.append(0)
    segment_ids.append(0)

  assert len(input_ids) == max_seq_length
  assert len(input_mask) == max_seq_length
  assert len(segment_ids) == max_seq_length

  label_id = label_map[example.label]
  if ex_index < 5:
    tf.logging.info("*** Example ***")
    tf.logging.info("guid: %s" % (example.guid))
    tf.logging.info("tokens: %s" % " ".join(
        [tokenization.printable_text(x) for x in tokens]))
    tf.logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
    tf.logging.info("input_mask: %s" % " ".join([str(x) for x in input_mask]))
    tf.logging.info("segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
    tf.logging.info("label: %s (id = %d)" % (example.label, label_id))

  feature = InputFeatures(
      input_ids=input_ids,
      input_mask=input_mask,
      segment_ids=segment_ids,
      label_id=label_id,
      is_real_example=True)
  return feature


# 实现了俩功能：1.调用convert_single_example转化Input_example为Feature_example 2.转换为TFRecord格式，便于大型数据处理
def file_based_convert_examples_to_features(
    examples, label_list, max_seq_length, tokenizer, output_file):
  """Convert a set of `InputExample`s to a TFRecord file."""   # 写'train.tf_record'文件到output_dir下
                                                               # TFRecord内部采用二进制编码，加载快，对大型数据转换友好

# 此模块主要分为两个部分：1.TFRecord生成器, 2.Example模块
  writer = tf.python_io.TFRecordWriter(output_file)  # 最外层：是TFRecord生成器部分，内部需要传入tf_example

  for (ex_index, example) in enumerate(examples):
    if ex_index % 10000 == 0:
      tf.logging.info("Writing example %d of %d" % (ex_index, len(examples)))  # 跟踪examples转换进度
	# 把examples数据转化为features，用到前面的单次转换函数
    feature = convert_single_example(ex_index, example, label_list,
                                     max_seq_length, tokenizer)
    # 这是Example模块
    def create_int_feature(values):
      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
      return f
      
    features = collections.OrderedDict()  # 其实就是一个字典
    
    # 以下五句都是调用create_int_feature生成value往features字典中填充
    features["input_ids"] = create_int_feature(feature.input_ids)
    features["input_mask"] = create_int_feature(feature.input_mask)
    features["segment_ids"] = create_int_feature(feature.segment_ids)
    features["label_ids"] = create_int_feature([feature.label_id])
    features["is_real_example"] = create_int_feature(
        [int(feature.is_real_example)])

    tf_example = tf.train.Example(features=tf.train.Features(feature=features))  # 最外层是tf.train.Features()的实例，内层是feature的字典
    writer.write(tf_example.SerializeToString())
  writer.close()


# 这是一个闭包，外层函数返回内层函数的引用，内层函数使用外层函数的参数
def file_based_input_fn_builder(input_file, seq_length, is_training,  # 此input_file是TFRecord文件
                                drop_remainder):
  """Creates an `input_fn` closure to be passed to TPUEstimator."""  # 生成一个input_fn闭包传递给TPUEstimator

  name_to_features = {
      # 是tensorflow example协议中的一种解析，这里面，传入了shape和dtype
      "input_ids": tf.FixedLenFeature([seq_length], tf.int64),
      "input_mask": tf.FixedLenFeature([seq_length], tf.int64),
      "segment_ids": tf.FixedLenFeature([seq_length], tf.int64),
      "label_ids": tf.FixedLenFeature([], tf.int64),
      "is_real_example": tf.FixedLenFeature([], tf.int64),
  }

  def _decode_record(record, name_to_features):
    """Decodes a record to a TensorFlow example."""
    example = tf.parse_single_example(record, name_to_features)

    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
    # So cast all int64 to int32.
    for name in list(example.keys()):
      t = example[name]
      if t.dtype == tf.int64:
        t = tf.to_int32(t)
      example[name] = t

    return example

  def input_fn(params):
    """The actual input function."""
    batch_size = params["batch_size"]

    # 对于训练，我们需要大量的并行的读取和洗牌
    # 对于评估，我们不需要洗牌，并行的读取也无关紧要
    d = tf.data.TFRecordDataset(input_file)
    if is_training:
      d = d.repeat()  # 重复
      d = d.shuffle(buffer_size=100)  # 洗牌，缓冲区=100

    d = d.apply(
        tf.contrib.data.map_and_batch(
            # 调用_decode_record函数：1.解析TFRecord为example 2.int64 to int32
            lambda record: _decode_record(record, name_to_features),  
            batch_size=batch_size,
            drop_remainder=drop_remainder))

    return d

  return input_fn


def _truncate_seq_pair(tokens_a, tokens_b, max_length):
  """Truncates a sequence pair in place to the maximum length."""  # 将序列对截断到最大长度max_length

  # This is a simple heuristic which will always truncate the longer sequence
  # one token at a time. This makes more sense than truncating an equal percent
  # of tokens from each, since if one sequence is very short then each token
  # that's truncated likely contains more information than a longer sequence.
  while True:
    total_length = len(tokens_a) + len(tokens_b)
    if total_length <= max_length:
      break
    if len(tokens_a) > len(tokens_b):
      tokens_a.pop()
    else:
      tokens_b.pop()


# 做了两件事:1.使用modeling.py中的BerModel类创建模型 2.计算交叉熵损失loss
def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,  # 创建分类器模型
                 labels, num_labels, use_one_hot_embeddings):
  """Creates a classification model."""
  model = modeling.BertModel(
      config=bert_config,
      is_training=is_training,
      input_ids=input_ids,
      input_mask=input_mask,
      token_type_ids=segment_ids,
      use_one_hot_embeddings=use_one_hot_embeddings)

  # In the demo, we are doing a simple classification task on the entire
  # segment.
  #
  # If you want to use the token-level output, use model.get_sequence_output()
  # instead.
  output_layer = model.get_pooled_output()  # 输出层

  hidden_size = output_layer.shape[-1].value  # 隐藏层大小

  output_weights = tf.get_variable(  # 输出层权重
      "output_weights", [num_labels, hidden_size],
      initializer=tf.truncated_normal_initializer(stddev=0.02))

  output_bias = tf.get_variable(  # 输出层偏置
      "output_bias", [num_labels], initializer=tf.zeros_initializer())

  with tf.variable_scope("loss"):
    if is_training:
      # I.e., 0.1 dropout
      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)  # dropout：减小模型过拟合，保留90%的网络连接，随机drop 10%

    logits = tf.matmul(output_layer, output_weights, transpose_b=True)  # w*x的矩阵乘法
    logits = tf.nn.bias_add(logits, output_bias)  # w*x + b
    probabilities = tf.nn.softmax(logits, axis=-1)  # 把输出结果指数归一化映射到（0，1）区间
    log_probs = tf.nn.log_softmax(logits, axis=-1)  # 相当于对上式的每个值求log，落在 负无穷到0之间

	# 标签one_hot编码，相当于增加标签维度，变稀疏化
    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)
    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
    # 交叉熵损失：交叉熵的值越小，两个概率分布就越接近
    loss = tf.reduce_mean(per_example_loss)

    return (loss, per_example_loss, logits, probabilities)


def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings):
  """Returns `model_fn` closure for TPUEstimator."""  # 返回给TPUEstimator闭包model_fn

  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """The `model_fn` for TPUEstimator."""

    tf.logging.info("*** Features ***")
    for name in sorted(features.keys()):
      tf.logging.info("  name = %s, shape = %s" % (name, features[name].shape))

    input_ids = features["input_ids"]
    input_mask = features["input_mask"]
    segment_ids = features["segment_ids"]
    label_ids = features["label_ids"]
    is_real_example = None
    if "is_real_example" in features:
      is_real_example = tf.cast(features["is_real_example"], dtype=tf.float32)  # tf.cast()数据类型的转换
    else:
      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)  # 生成所有数字为1的tensor

    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

	# 1.创建bert的model 2.计算loss
    (total_loss, per_example_loss, logits, probabilities) = create_model( 
        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,
        num_labels, use_one_hot_embeddings)

    tvars = tf.trainable_variables()
    initialized_variable_names = {}
    scaffold_fn = None
    if init_checkpoint:
      (assignment_map, initialized_variable_names
      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
      if use_tpu:

        def tpu_scaffold():  # 用于创建或收集训练模型通常需要的部件
          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
          return tf.train.Scaffold()

        scaffold_fn = tpu_scaffold
      else:
        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

    tf.logging.info("**** Trainable Variables ****")
    for var in tvars:
      init_string = ""
      if var.name in initialized_variable_names:
        init_string = ", *INIT_FROM_CKPT*"
      tf.logging.info("  name = %s, shape = %s%s", var.name, var.shape,  # 得到可用于训练的变量名
                      init_string)

    output_spec = None
    if mode == tf.estimator.ModeKeys.TRAIN:

      train_op = optimization.create_optimizer(
          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,  # 模式为训练模式
          loss=total_loss,  # 损失
          train_op=train_op,  # 操作
          scaffold_fn=scaffold_fn)  # 训练模型需要的部件
    elif mode == tf.estimator.ModeKeys.EVAL:

      def metric_fn(per_example_loss, label_ids, logits, is_real_example):
        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)  # 返回array中，最大值的索引
        accuracy = tf.metrics.accuracy(
            labels=label_ids, predictions=predictions, weights=is_real_example)
        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)
        return {
            "eval_accuracy": accuracy,
            "eval_loss": loss,
        }

      eval_metrics = (metric_fn,
                      [per_example_loss, label_ids, logits, is_real_example])
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,  # 模式为评估模式
          loss=total_loss,
          eval_metrics=eval_metrics,  # 此处内含：accuracy和loss
          scaffold_fn=scaffold_fn)
    else:
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          # 此probabilities为softmax的计算的概率
          predictions={"probabilities": probabilities},
          scaffold_fn=scaffold_fn)
    return output_spec

  return model_fn


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  processors = {
      "cola": ColaProcessor,  # 填入自定义处理数据的类，必填项
      "mnli": MnliProcessor,
      "mrpc": MrpcProcessor,
      "xnli": XnliProcessor,
      "sentiment": SentimentProcessor
  }

  tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,  # 验证实例，匹配检查点
                                                FLAGS.init_checkpoint)

  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:
    raise ValueError(
        "At least one of `do_train`, `do_eval` or `do_predict' must be True.")

  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)
  # 上面三个判断至少有一个为真执行下面语句
  if FLAGS.max_seq_length > bert_config.max_position_embeddings:  # 语句长度设置的比如为128，不应大于bert官方训练时的长度
    raise ValueError(
        "Cannot use sequence length %d because the BERT model "
        "was only trained up to sequence length %d" %
        (FLAGS.max_seq_length, bert_config.max_position_embeddings))

  tf.gfile.MakeDirs(FLAGS.output_dir)  # 此语句会创建output_dir目录，所以，只要指定，无需再单独创建

  task_name = FLAGS.task_name.lower()  # 传入的自定义类Sentiment名称小写化为sentiment

  if task_name not in processors:
    raise ValueError("Task not found: %s" % (task_name))

  processor = processors[task_name]()  # 类()  -->  实例化

  label_list = processor.get_labels()  # 类.方法，此label_list为['0','1']类别标签集

  tokenizer = tokenization.FullTokenizer(
      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)  # 加载训练的中文词典，输入数据是否小写化

  tpu_cluster_resolver = None  # tpu集群处理
  if FLAGS.use_tpu and FLAGS.tpu_name:  # 使用tpu时创建tpu集群处理实例
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)

  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2  # per_host：每主机，XLnet中num_core_per_host指的是每主机核数
  run_config = tf.contrib.tpu.RunConfig(  # tpu的运行配置
      cluster=tpu_cluster_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.contrib.tpu.TPUConfig(  # tf.contrib模块是tf.nn的上一层的tf.layer的上层,主要提供一些图上的操作：如正则化，摘要操作。。。
          iterations_per_loop=FLAGS.iterations_per_loop,  # 在每个estimator调用中执行多少步，default中为1000步
          num_shards=FLAGS.num_tpu_cores,  # tpu核数，default为8
          per_host_input_for_training=is_per_host))

  train_examples = None
  num_train_steps = None
  num_warmup_steps = None



  if FLAGS.do_train:
    # 返回列表是一行一行的Inputexample对象，每行包括了guid，train_a,label..
    train_examples = processor.get_train_examples(FLAGS.data_dir)
    # 训练的次数：(训练集的样本数/每批次大小)*训练几轮
    num_train_steps = int(
        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)
        # 举例：num_train_steps = (20000/32)*3 = 1800次，
        # 也就是权重更新，loss下降1800次                                                            
            
    # 在预热学习中，线性地增加学习率                                               
    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)  

  model_fn = model_fn_builder(
      bert_config=bert_config,
      num_labels=len(label_list),
      init_checkpoint=FLAGS.init_checkpoint,
      learning_rate=FLAGS.learning_rate,
      num_train_steps=num_train_steps,
      num_warmup_steps=num_warmup_steps,
      use_tpu=FLAGS.use_tpu,
      use_one_hot_embeddings=FLAGS.use_tpu)

  # If TPU is not available, this will fall back to normal Estimator on CPU
  # or GPU.
  estimator = tf.contrib.tpu.TPUEstimator(  # 构建估计器对象
      use_tpu=FLAGS.use_tpu,
      model_fn=model_fn,
      config=run_config,
      train_batch_size=FLAGS.train_batch_size,
      eval_batch_size=FLAGS.eval_batch_size,
      predict_batch_size=FLAGS.predict_batch_size)

  if FLAGS.do_train:
    train_file = os.path.join(FLAGS.output_dir, "train.tf_record")  # record：记录，档案
    # 实现Input_example到Feature_example, TFRecord化
    file_based_convert_examples_to_features(
        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)
    tf.logging.info("***** Running training *****")
    tf.logging.info("  Num examples = %d", len(train_examples))
    tf.logging.info("  Batch size = %d", FLAGS.train_batch_size)
    tf.logging.info("  Num steps = %d", num_train_steps)
    # 调用此函数，完成：1.TFRecord to example 2.int64 to int32
    train_input_fn = file_based_input_fn_builder(
        input_file=train_file,
        seq_length=FLAGS.max_seq_length,
        is_training=True,
        drop_remainder=True)
    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)



  if FLAGS.do_eval:
    eval_examples = processor.get_dev_examples(FLAGS.data_dir)
    num_actual_eval_examples = len(eval_examples)
    if FLAGS.use_tpu:
      # 不为整数倍时，填充
      while len(eval_examples) % FLAGS.eval_batch_size != 0:
        eval_examples.append(PaddingInputExample())

    eval_file = os.path.join(FLAGS.output_dir, "eval.tf_record")
    file_based_convert_examples_to_features(
        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)

    tf.logging.info("***** Running evaluation *****")
    # 数量 =（实际的数量，填充的数量）
    tf.logging.info("  Num examples = %d (%d actual, %d padding)",
                    len(eval_examples), num_actual_eval_examples,
                    len(eval_examples) - num_actual_eval_examples)
    tf.logging.info("  Batch size = %d", FLAGS.eval_batch_size)

    # This tells the estimator to run through the entire set.
    eval_steps = None  # 遍历整个集合
    # However, if running eval on the TPU, you will need to specify the
    # number of steps.
    if FLAGS.use_tpu:
      # 假定使用TPU时，前面整除处理已经成功，将得到eval_steps为整数值
      assert len(eval_examples) % FLAGS.eval_batch_size == 0
      eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)
    # 如果使用tpu的话，删除剩余的部分（可能是无法整除的部分）
    eval_drop_remainder = True if FLAGS.use_tpu else False
    # 1.TFRecord to example 2.int64 to int32 为estimator提供输入
    eval_input_fn = file_based_input_fn_builder(
        input_file=eval_file,
        seq_length=FLAGS.max_seq_length,
        is_training=False,
        drop_remainder=eval_drop_remainder)

    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)
    
	# 生成验证集评估指标数据
    output_eval_file = os.path.join(FLAGS.output_dir, "eval_results.txt")
    with tf.gfile.GFile(output_eval_file, "w") as writer:
      tf.logging.info("***** Eval results *****")
      for key in sorted(result.keys()):
        tf.logging.info("  %s = %s", key, str(result[key]))
        writer.write("%s = %s\n" % (key, str(result[key])))

  if FLAGS.do_predict:
    predict_examples = processor.get_test_examples(FLAGS.data_dir)
    num_actual_predict_examples = len(predict_examples)
    if FLAGS.use_tpu:
      while len(predict_examples) % FLAGS.predict_batch_size != 0:
        predict_examples.append(PaddingInputExample())

    predict_file = os.path.join(FLAGS.output_dir, "predict.tf_record")
    # 1.调用convert_single_example转化Input_example为Feature_example
    # 2.转换为TFRecord格式，便于大型数据处理
    file_based_convert_examples_to_features(predict_examples, label_list,
                                                                           
                                            FLAGS.max_seq_length, tokenizer,
                                            predict_file)

    tf.logging.info("***** Running prediction*****")
    tf.logging.info("  Num examples = %d (%d actual, %d padding)",
                    len(predict_examples), num_actual_predict_examples,
                    len(predict_examples) - num_actual_predict_examples)
    tf.logging.info("  Batch size = %d", FLAGS.predict_batch_size)

    predict_drop_remainder = True if FLAGS.use_tpu else False
    # 1.TFRecord to example 2.int64 to int32 为estimator提供输入
    predict_input_fn = file_based_input_fn_builder(
        input_file=predict_file,
        seq_length=FLAGS.max_seq_length,
        is_training=False,
        drop_remainder=predict_drop_remainder)

    result = estimator.predict(input_fn=predict_input_fn)
	# 写测试集预测结果文件
    output_predict_file = os.path.join(FLAGS.output_dir, "test_results.tsv")
    with tf.gfile.GFile(output_predict_file, "w") as writer:
      num_written_lines = 0
      tf.logging.info("***** Predict results *****")
      for (i, prediction) in enumerate(result):
        # 写入probabilities的键值对，比如二分类：有预测为0的一列，预测为1的一列
        probabilities = prediction["probabilities"]
        if i >= num_actual_predict_examples:
          break
        output_line = "\t".join(
            str(class_probability)
            # 例如：某一行class_probability为(0.98,0.02)->('0.98','0.02')
            for class_probability in probabilities) + "\n"
        writer.write(output_line)
        num_written_lines += 1
    assert num_written_lines == num_actual_predict_examples


if __name__ == "__main__":
  flags.mark_flag_as_required("data_dir")
  flags.mark_flag_as_required("task_name")
  flags.mark_flag_as_required("vocab_file")
  flags.mark_flag_as_required("bert_config_file")
  flags.mark_flag_as_required("output_dir")
  tf.app.run()


```