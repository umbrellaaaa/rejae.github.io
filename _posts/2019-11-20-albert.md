---
layout:     post
title:      albert
subtitle:   summary
date:       2019-11-20
author:     RJ
header-img: 
catalog: true
tags:
    - DL

---
<p id = "build"></p>
---


论文地址：[ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://openreview.net/pdf?id=H1eA7AEtvS)

## abstract

Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a **self-supervised loss** that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having **fewer parameters** compared to BERT-large.


ALBERT主要对BERT做了两点改进，缩小了整体的参数量加快了训练速度，提高了模型效果。

## 1. INTRODUCTION
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120table1.jpg)
 Table 1 and Fig. 1 show

a typical example, where we simply increase the hidden size of BERT-large to be 2x larger and get
worse results with this BERT-xlarge model.
对特定的任务而言模型大不一定更好！

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120table2.jpg)

针对上述问题的现有解决方案包括模型并行化(Shoeybi et al.， 2019)和智能内存管理(Chen et al.， 2016;Gomez等人，2017)。这些解决方案解决了内存限制问题，但没有解决**通信开销和模型退化问题**。在本文中，我们通过设计一个比传统BERT体系结构参数少得多的Lite BERT (ALBERT)来解决所有上述问题。

ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models： 

- The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. (第一个技术是对嵌入参数进行因式分解。研究者将大的词汇嵌入矩阵分解为两个小的矩阵，从而将隐藏层的大小与词汇嵌入的大小分离开来。这种分离使得隐藏层的增加更加容易，同时不显著增加词汇嵌入的参数量。)
- The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.(第二种技术是跨层参数共享。这一技术可以避免参数量随着网络深度的增加而增加。两种技术都显著降低了 BERT 的参数量，同时不对其性能造成明显影响，从而提升了参数效率。ALBERT 的配置类似于 BERT-large，但参数量仅为后者的 1/18，训练速度却是后者的 1.7 倍。)


**To further improve the performance of ALBERT:**

 we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.

SOP关注句子间的连贯性 VS NSP下一句预测



## 2. RELATED WORK
### 2.1 SCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE
scaling up representation learning for natural language is not as easy as simply increasing model size. In addition, it is difficult to experiment with large models due to computational constraints, especially in terms of GPU/TPU memory limitations. Given that current state-of-the-art models often
have hundreds of millions or even billions of parameters, we can easily hit memory limits. To address this issue,：
- Chen et al. (2016) propose a method called gradient checkpointing to reduce the
memory requirement to be sublinear at the cost of an extra forward pass. 
- Gomez et al. (2017) propose a way to reconstruct each layer’s activations from the next layer so that they do not need to store the intermediate activations. Both methods reduce the memory consumption at the cost of speed.
- **In contrast, our parameter-reduction techniques reduce memory consumption and increase training
speed.**



### 2.2 CROSS-LAYER PARAMETER SHARING
The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting. Different from our observations, Dehghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard
transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard
transformer.

该Trick本质上就是对参数共享机制在Transformer内的探讨。在Transfor中有两大主要的组件：FFN与多头注意力机制。ALBERT主要是对这两大组件的共享机制进行探讨。
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120crosslayershare1.jpg)

论文里采用了四种方式： all-shared，shared-attention，shared-FFN以及 not-shared。我们首选关注一下参数量，not-shared与all-shared的参数量相差极为明显，因此可以得出共享机制才是参数量大幅减少的根本原因。然后，我们看到，**只共享Attention参数能够获得参数量与性能的权衡**。最后，很明显的就是，随着层数的加深，基于共享机制的 ALBERT 参数量与BERT参数量相比下降的更加明显。



### 2.3 SENTENCE ORDERING OBJECTIVES
ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. 

Several researchers have experimented with pretraining objectives that similarly relate to discourse coherence. Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979; Halliday & Hasan, 1976; Grosz et al., 1995). 
- Most objectives found effective in practice are quite simple. Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. 
- Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). 
- Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is defined on textual segments rather than sentences. BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and find that sentence ordering is a more challenging pretraining task and more useful for certain downstream tasks. 
- Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classification task rather than empirically comparing the two.



## 3 THE ELEMENTS OF ALBERT
In this section, we present the design decisions for ALBERT and provide quantified comparisons
against corresponding configurations of the original BERT architecture (Devlin et al., 2019).

### 3.1 MODEL ARCHITECTURE CHOICES
The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with **GELU** nonlinearities (Hendrycks & Gimpel, 2016).(GELUs正是在激活中引入了随机正则的思想，是一种对神经元输入的概率描述，直观上更符合自然的认识，同时实验效果要比Relus与ELUs都要好。)

We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H. Following Devlin et al. (2019), we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64. There are three main contributions that ALBERT makes over the design choices of BERT.
#### 嵌入向量参数化的因式分解

在 BERT 以及后续的 XLNet 和 RoBERTa 中，WordPiece 词嵌入大小 E 和隐藏层大小 H 是相等的，即 E ≡ H。由于建模和实际使用的原因，这个决策看起来可能并不是最优的。

- 从建模的角度来说，WordPiece 词嵌入的目标是学习上下文无关的表示，而隐藏层嵌入的目标是学习上下文相关的表示。通过上下文相关的实验，BERT 的表征能力很大一部分来自于使用上下文为学习过程提供上下文相关的表征信号。因此，将 WordPiece 词嵌入大小 E 从隐藏层大小 H 分离出来，可以更高效地利用总体的模型参数，其中 H 要远远大于 E。
- 从实践的角度，自然语言处理使用的词典大小 V 非常庞大，如果 E 恒等于 H，那么增加 H 将直接加大嵌入矩阵的大小，这种增加还会通过 V 进行放大。

因此，对于 ALBERT 而言，研究者对词嵌入参数进行了因式分解，将它们分解为两个小矩阵。研究者不再将 one-hot 向量直接映射到大小为 H 的隐藏空间，而是先将它们映射到一个低维词嵌入空间 E，然后再映射到隐藏空间。通过这种分解，研究者可以将词嵌入参数从 O(V × H) 降低到 O(V × E + E × H)，这在 H 远远大于 E 的时候，参数量减少得非常明显。

 O(V * H) to O(V * E + E * H)
 
 如以ALBert_xxlarge为例，V=30000, H=4096, E=128
   
 那么原先参数为V * H= 30000 * 4096 = 1.23亿个参数，现在则为V * E + E * H = 30000*128+128*4096 = 384万 + 52万 = 436万，
   
 词嵌入相关的参数变化前是变换后的28倍


在bert以及诸多bert的改进版中，embedding size都是等于hidden size的，这不一定是最优的。因为bert的token embedding是上下文无关的，而经过multi-head attention+ffn后的hidden embedding是上下文相关的，bert预训练的目的是提供更准确的hidden embedding，而不是token embedding，因此token embedding没有必要和hidden embedding一样大。albert将token embedding进行了分解，首先降低embedding size的大小，然后用一个Dense操作将低维的token embedding映射回hidden size的大小。bert的embedding size=hidden size，因此词向量的参数量是vocab size * hidden size，进行分解后的参数量是vocab size * embedding size + embedding size * hidden size，只要embedding size << hidden size，就能起到减少参数的效果。

#### 跨层权重共享

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120crosslayerstructure.jpg)
跨层参数共享，就是不管12层还是24层都只用一个transformer。来源于论文Universal Transformer.

跨层参数共享（参数量减少主要贡献）

- 这主要是为了减少参数量（性能轻微降低，参数大量减少，这整体上是好的事情）
- 论文里的消融实验的分数也说明no-share的分数是最高的

对于 ALBERT，研究者提出了另一种跨层参数共享机制来进一步提升参数效率。其实目前有很多方式来共享参数，例如只贡献前馈网络不同层之间的参数，或者只贡献注意力机制的参数，而 ALBERT 采用的是贡献所有层的所有参数。参数共享能显著减少参数。共享可以分为全连接层、注意力层的参数共享；**注意力层的参数对效果的减弱影响小一点。**

作者对比了每层输入输出的L2距离和相似度，发现了BERT的结果比较震荡，而ALBERT就很稳定，可见ALBERT有稳定网络参数的作用。

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120crosslayerfigure1.jpg)

#### 句间连贯性损失
后BERT时代很多研究(XLNet、RoBERTa)都发现next sentence prediction没什么用处，所以作者也审视了一下这个问题，认为NSP之所以没用是因为这个任务不仅包含了句间关系预测，也包含了主题预测，而主题预测显然更简单些（比如一句话来自新闻财经，一句话来自文学小说），模型会倾向于通过主题的关联去预测。因此换成了SOP(sentence order prediction)，预测两句话有没有被交换过顺序。实验显示新增的任务有1个点的提升：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120SOPtable.jpg)




### 3.2 MODEL SETUP
We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 2. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models. For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. If we set BERT to have an extra-large size with H = 2048, we end up with a model that has 1.27 billion parameters and under-performs (Fig. 1). In contrast, an ALBERT-xlarge configuration with H = 2048 has only 60M parameters, while an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERT-large’s parameters. Note that for ALBERTxxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive. This improvement in parameter efficiency is the most important advantage of ALBERT’s design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail.
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120parameterconfigurationtable2.jpg)
参数量级上的对比如上表所示，十分明显。这里需要提到的是ALBERT-xxlarge，它只有12层，但是隐层维度高达4096，这是考虑到深层网络的计算量问题，其本质上是一个浅而宽的网络。

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120contrastberttable3.jpg)
首先ALBERT-xxlarge的表现完全超过了BERT-large的表现，但是BERT-large的速度是要比ALBERT-xxlarge快3倍左右的。其次，BERT-xlarge的表现反而变差
## 4 EXPERIMENTAL RESULTS



## 5 DISCUSSION

- While ALBERT-xxlarge has **less parameters** than BERT-large and gets significantly better results, it is computationally more expensive due to its **larger structure**. 
- An important next step is thus to speed up the training and inference speed of ALBERT through methods like **sparse attention** (Child et al.,2019) and **block attention** (Shen et al., 2018). 
- An orthogonal line of research, which could provide additional representation power, includes **hard example mining** (Mikolov et al., 2013) and **more efficient language modeling training** (Yang et al., 2019). 
- Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be **more dimensions not yet captured** by the current self-supervised training losses that could create additional representation power for the resulting representations.


ALBERT 的目的论文开篇就提到，在预训练语言模型领域，增大模型往往能够到来不错的效果提升，但这种提升不是无止境的。预训练语言模型已经很大了，大到绝大多数的实验室和公司都没有资格参与这场游戏，对于大模型而言，一个很浅的idea就是：如何对大模型进行压缩？ ALBERT 本质就是对 BERT 模型压缩后的产物。模型压缩有很多手段，包括剪枝，参数共享，低秩分解，网络结构设计，知识蒸馏等。ALBERT 也没能逃出这一框架，它其实是一个相当工程化的思想，它的两大 压缩Trick 也很容易想到，下面就细聊一下。
- Factorized embedding parameterization - 低秩分解
- Cross-layer parameter sharing - 参数共享机制



## 参考

[ALBERT 告诉了我们什么？](https://zhuanlan.zhihu.com/p/92849070)  <br>
[ALBERT粗读](https://zhuanlan.zhihu.com/p/84273154)   <br>
[解读ALBERT](https://blog.csdn.net/weixin_37947156/article/details/101529943)  <br>
[Gaussian Error Linerar Units(GELUS)](https://arxiv.org/abs/1606.08415)<br>
[GELUS解读](https://blog.csdn.net/liruihongbob/article/details/86510622)<br>
[Universal Transformers](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1807.03819.pdf)
[UT解读](https://zhuanlan.zhihu.com/p/44655133)<br>

## 问答
机器之心：我们知道 ALBERT 中使用的参数化因式分解和参数共享两个方法，它们分别是怎样提升了模型的表现呢？

蓝振忠：这两个方法实际上都是将模型的参数进行降低。从架构的角度来看，可以把 BERT 或者 Transformer 的架构分为两个部分。其中一个是把单词投射到隐层。另一个部分是在隐层上做迭代的词向量变换。两块的模型参数都比较大。所以，我们如果想整体的去减少整个模型的参数量，就需要把两块的参数量都降下来。参数的因式分解和共享刚好对应的是这两个模块的降低参数量的方法。除了这两个方法可以降低参数量，从而降低模型的内存占用，我们还发现去掉 Dropout 以后可以显著的降低内存占用。

机器之心：为什么说去掉 Dropout 可以减少内存的占用呢？

蓝振忠：在计算的时候，除了参数会占用内存之外，还有临时变量也会占用内存。去掉 Dropout 可以显著减少临时变量对内存的占用，所以提升了性能。

机器之心：现在是否可以说，预训练模型已经可以应用在移动设备上了？如果不成熟的话，还有哪些技术难点需要解决？

蓝振忠：我觉得已经可以应用了。最近有一个这样研究，是 ChineseGLUE 的项目组做的，是 ALBERT 的中文版。他们训练了一个 ALBERT-Tiny 模型。我在谷歌的同事将它转成 tensorflow-lite 之后在手机端上做了一些测试。在 4 线程的 CPU 上的延时是 120 毫秒左右；这个是符合某些手机端的要求的。当然，我们最近还在做一些更新，还有进一步提升的空间。

