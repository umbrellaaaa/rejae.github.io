---
layout:     post
title:      albert
subtitle:   summary
date:       2019-11-20
author:     RJ
header-img: 
catalog: true
tags:
    - DL

---
<p id = "build"></p>
---


[ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://openreview.net/pdf?id=H1eA7AEtvS)

## abstract

Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a **self-supervised loss** that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having **fewer parameters** compared to BERT-large.


ALBERT主要对BERT做了两点改进，缩小了整体的参数量加快了训练速度，提高了模型效果。

## 1. INTRODUCTION
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120table1.jpg)
 Table 1 and Fig. 1 show

a typical example, where we simply increase the hidden size of BERT-large to be 2x larger and get
worse results with this BERT-xlarge model.
对特定的任务而言模型大不一定更好！

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120table2.jpg)

针对上述问题的现有解决方案包括模型并行化(Shoeybi et al.， 2019)和智能内存管理(Chen et al.， 2016;Gomez等人，2017)。这些解决方案解决了内存限制问题，但没有解决**通信开销和模型退化问题**。在本文中，我们通过设计一个比传统BERT体系结构参数少得多的Lite BERT (ALBERT)来解决所有上述问题。

ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models： 

1. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. 

第一个技术是对嵌入参数进行因式分解。研究者将大的词汇嵌入矩阵分解为两个小的矩阵，从而将隐藏层的大小与词汇嵌入的大小分离开来。这种分离使得隐藏层的增加更加容易，同时不显著增加词汇嵌入的参数量。


2. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.

第二种技术是跨层参数共享。这一技术可以避免参数量随着网络深度的增加而增加。两种技术都显著降低了 BERT 的参数量，同时不对其性能造成明显影响，从而提升了参数效率。ALBERT 的配置类似于 BERT-large，但参数量仅为后者的 1/18，训练速度却是后者的 1.7 倍。


**To further improve the performance of ALBERT:**

 we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.

SOP关注句子间的连贯性 VS NSP下一句预测



## 2. RELATED WORK







## 嵌入向量参数化的因式分解

在 BERT 以及后续的 XLNet 和 RoBERTa 中，WordPiece 词嵌入大小 E 和隐藏层大小 H 是相等的，即 E ≡ H。由于建模和实际使用的原因，这个决策看起来可能并不是最优的。

从建模的角度来说，WordPiece 词嵌入的目标是学习上下文无关的表示，而隐藏层嵌入的目标是学习上下文相关的表示。通过上下文相关的实验，BERT 的表征能力很大一部分来自于使用上下文为学习过程提供上下文相关的表征信号。因此，将 WordPiece 词嵌入大小 E 从隐藏层大小 H 分离出来，可以更高效地利用总体的模型参数，其中 H 要远远大于 E。

从实践的角度，自然语言处理使用的词典大小 V 非常庞大，如果 E 恒等于 H，那么增加 H 将直接加大嵌入矩阵的大小，这种增加还会通过 V 进行放大。

因此，对于 ALBERT 而言，研究者对词嵌入参数进行了因式分解，将它们分解为两个小矩阵。研究者不再将 one-hot 向量直接映射到大小为 H 的隐藏空间，而是先将它们映射到一个低维词嵌入空间 E，然后再映射到隐藏空间。通过这种分解，研究者可以将词嵌入参数从 O(V × H) 降低到 O(V × E + E × H)，这在 H 远远大于 E 的时候，参数量减少得非常明显。

 O(V * H) to O(V * E + E * H)
 
 如以ALBert_xxlarge为例，V=30000, H=4096, E=128
   
 那么原先参数为V * H= 30000 * 4096 = 1.23亿个参数，现在则为V * E + E * H = 30000*128+128*4096 = 384万 + 52万 = 436万，
   
 词嵌入相关的参数变化前是变换后的28倍
