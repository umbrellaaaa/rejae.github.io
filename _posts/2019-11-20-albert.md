---
layout:     post
title:      albert
subtitle:   summary
date:       2019-11-20
author:     RJ
header-img: 
catalog: true
tags:
    - DL

---
<p id = "build"></p>
---


[ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://openreview.net/pdf?id=H1eA7AEtvS)

## abstract

Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a **self-supervised loss** that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having **fewer parameters** compared to BERT-large.


ALBERT主要对BERT做了两点改进，缩小了整体的参数量加快了训练速度，提高了模型效果。

## 1. INTRODUCTION
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120table1.jpg)
 Table 1 and Fig. 1 show

a typical example, where we simply increase the hidden size of BERT-large to be 2x larger and get
worse results with this BERT-xlarge model.
对特定的任务而言模型大不一定更好！

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120table2.jpg)

针对上述问题的现有解决方案包括模型并行化(Shoeybi et al.， 2019)和智能内存管理(Chen et al.， 2016;Gomez等人，2017)。这些解决方案解决了内存限制问题，但没有解决**通信开销和模型退化问题**。在本文中，我们通过设计一个比传统BERT体系结构参数少得多的Lite BERT (ALBERT)来解决所有上述问题。

ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models： 

- The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. (第一个技术是对嵌入参数进行因式分解。研究者将大的词汇嵌入矩阵分解为两个小的矩阵，从而将隐藏层的大小与词汇嵌入的大小分离开来。这种分离使得隐藏层的增加更加容易，同时不显著增加词汇嵌入的参数量。)
- The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.(第二种技术是跨层参数共享。这一技术可以避免参数量随着网络深度的增加而增加。两种技术都显著降低了 BERT 的参数量，同时不对其性能造成明显影响，从而提升了参数效率。ALBERT 的配置类似于 BERT-large，但参数量仅为后者的 1/18，训练速度却是后者的 1.7 倍。)


**To further improve the performance of ALBERT:**

 we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.

SOP关注句子间的连贯性 VS NSP下一句预测



## 2. RELATED WORK
### 2.1 SCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE
scaling up representation learning for natural language is not as easy as simply increasing model size. In addition, it is difficult to experiment with large models due to computational constraints, especially in terms of GPU/TPU memory limitations. Given that current state-of-the-art models often
have hundreds of millions or even billions of parameters, we can easily hit memory limits. To address this issue,：
- Chen et al. (2016) propose a method called gradient checkpointing to reduce the
memory requirement to be sublinear at the cost of an extra forward pass. 
- Gomez et al. (2017) propose a way to reconstruct each layer’s activations from the next layer so that they do not need to store the intermediate activations. Both methods reduce the memory consumption at the cost of speed.
- **In contrast, our parameter-reduction techniques reduce memory consumption and increase training
speed.**



### 2.2 CROSS-LAYER PARAMETER SHARING
The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting. Different from our observations, Dehghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard
transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard
transformer.

该Trick本质上就是对参数共享机制在Transformer内的探讨。在Transfor中有两大主要的组件：FFN与多头注意力机制。ALBERT主要是对这两大组件的共享机制进行探讨。
![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120crosslayershare1.jpg)

论文里采用了四种方式： all-shared，shared-attention，shared-FFN以及 not-shared。我们首选关注一下参数量，not-shared与all-shared的参数量相差极为明显，因此可以得出共享机制才是参数量大幅减少的根本原因。然后，我们看到，**只共享Attention参数能够获得参数量与性能的权衡**。最后，很明显的就是，随着层数的加深，基于共享机制的 ALBERT 参数量与BERT参数量相比下降的更加明显。



### 2.3 SENTENCE ORDERING OBJECTIVES
后BERT时代很多研究(XLNet、RoBERTa)都发现next sentence prediction没什么用处，所以作者也审视了一下这个问题，认为NSP之所以没用是因为这个任务不仅包含了句间关系预测，也包含了主题预测，而主题预测显然更简单些（比如一句话来自新闻财经，一句话来自文学小说），模型会倾向于通过主题的关联去预测。因此换成了SOP(sentence order prediction)，预测两句话有没有被交换过顺序。实验显示新增的任务有1个点的提升：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191120SOPtable.jpg)


在auto-encoder的loss之外，bert使用了NSP的loss，用来提高bert在句对关系推理任务上的推理能力。而albert放弃了NSP的loss，使用了SOP的loss。NSP的loss是判断segment A和segment B之间的关系，其中0表示segment B是segment A的下一句，1表示segment A和segment B来自2篇不同的文本。SOP的loss是判断segment A和segment B的的顺序关系，0表示segment B是segment A的下一句，1表示segment A是segment B的下一句。






## 3 THE ELEMENTS OF ALBERT




ALBERT 的目的论文开篇就提到，在预训练语言模型领域，增大模型往往能够到来不错的效果提升，但这种提升是无止境的吗？[2]中进行了详细的实验，相当程度上解答了这一问题。这里先埋一个坑，过几天再填。预训练语言模型已经很大了，大到绝大多数的实验室和公司都没有资格参与这场游戏，对于大模型而言，一个很浅的idea就是：如何对大模型进行压缩？ ALBERT 本质就是对 BERT 模型压缩后的产物。如果对模型压缩有了解的同学肯定知道，模型压缩有很多手段，包括剪枝，参数共享，低秩分解，网络结构设计，知识蒸馏等（这是另外一个坑，先埋下）。ALBERT 也没能逃出这一框架，它其实是一个相当工程化的思想，它的两大 压缩Trick 也很容易想到，下面就细聊一下。

[](https://zhuanlan.zhihu.com/p/92849070)


## 嵌入向量参数化的因式分解

在 BERT 以及后续的 XLNet 和 RoBERTa 中，WordPiece 词嵌入大小 E 和隐藏层大小 H 是相等的，即 E ≡ H。由于建模和实际使用的原因，这个决策看起来可能并不是最优的。

- 从建模的角度来说，WordPiece 词嵌入的目标是学习上下文无关的表示，而隐藏层嵌入的目标是学习上下文相关的表示。通过上下文相关的实验，BERT 的表征能力很大一部分来自于使用上下文为学习过程提供上下文相关的表征信号。因此，将 WordPiece 词嵌入大小 E 从隐藏层大小 H 分离出来，可以更高效地利用总体的模型参数，其中 H 要远远大于 E。
- 从实践的角度，自然语言处理使用的词典大小 V 非常庞大，如果 E 恒等于 H，那么增加 H 将直接加大嵌入矩阵的大小，这种增加还会通过 V 进行放大。

因此，对于 ALBERT 而言，研究者对词嵌入参数进行了因式分解，将它们分解为两个小矩阵。研究者不再将 one-hot 向量直接映射到大小为 H 的隐藏空间，而是先将它们映射到一个低维词嵌入空间 E，然后再映射到隐藏空间。通过这种分解，研究者可以将词嵌入参数从 O(V × H) 降低到 O(V × E + E × H)，这在 H 远远大于 E 的时候，参数量减少得非常明显。

 O(V * H) to O(V * E + E * H)
 
 如以ALBert_xxlarge为例，V=30000, H=4096, E=128
   
 那么原先参数为V * H= 30000 * 4096 = 1.23亿个参数，现在则为V * E + E * H = 30000*128+128*4096 = 384万 + 52万 = 436万，
   
 词嵌入相关的参数变化前是变换后的28倍


在bert以及诸多bert的改进版中，embedding size都是等于hidden size的，这不一定是最优的。因为bert的token embedding是上下文无关的，而经过multi-head attention+ffn后的hidden embedding是上下文相关的，bert预训练的目的是提供更准确的hidden embedding，而不是token embedding，因此token embedding没有必要和hidden embedding一样大。albert将token embedding进行了分解，首先降低embedding size的大小，然后用一个Dense操作将低维的token embedding映射回hidden size的大小。bert的embedding size=hidden size，因此词向量的参数量是vocab size * hidden size，进行分解后的参数量是vocab size * embedding size + embedding size * hidden size，只要embedding size << hidden size，就能起到减少参数的效果。

## 参考
[ALBERT 告诉了我们什么？](https://zhuanlan.zhihu.com/p/92849070)
[ALBERT粗读](https://zhuanlan.zhihu.com/p/84273154)

## 问答
机器之心：我们知道 ALBERT 中使用的参数化因式分解和参数共享两个方法，它们分别是怎样提升了模型的表现呢？

蓝振忠：这两个方法实际上都是将模型的参数进行降低。从架构的角度来看，可以把 BERT 或者 Transformer 的架构分为两个部分。其中一个是把单词投射到隐层。另一个部分是在隐层上做迭代的词向量变换。两块的模型参数都比较大。所以，我们如果想整体的去减少整个模型的参数量，就需要把两块的参数量都降下来。参数的因式分解和共享刚好对应的是这两个模块的降低参数量的方法。除了这两个方法可以降低参数量，从而降低模型的内存占用，我们还发现去掉 Dropout 以后可以显著的降低内存占用。

机器之心：为什么说去掉 Dropout 可以减少内存的占用呢？

蓝振忠：在计算的时候，除了参数会占用内存之外，还有临时变量也会占用内存。去掉 Dropout 可以显著减少临时变量对内存的占用，所以提升了性能。

机器之心：现在是否可以说，预训练模型已经可以应用在移动设备上了？如果不成熟的话，还有哪些技术难点需要解决？

蓝振忠：我觉得已经可以应用了。最近有一个这样研究，是 ChineseGLUE 的项目组做的，是 ALBERT 的中文版。他们训练了一个 ALBERT-Tiny 模型。我在谷歌的同事将它转成 tensorflow-lite 之后在手机端上做了一些测试。在 4 线程的 CPU 上的延时是 120 毫秒左右；这个是符合某些手机端的要求的。当然，我们最近还在做一些更新，还有进一步提升的空间。

