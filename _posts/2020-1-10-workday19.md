---
layout:     post
title:      workday19
subtitle:   
date:       2020-1-10
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 前言


## 长文本截断策略，前后随机截断依照Bert

```python
def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng, wrong_tokens_a, wrong_tokens_b):
    """Truncates a pair of sequences to a maximum sequence length."""
    assert len(tokens_a) == len(wrong_tokens_a)
    try:
        assert len(tokens_b) == len(wrong_tokens_b)
    except:
        print(tokens_b)
        print(wrong_tokens_b)
        exit()

    while True:
        total_length = len(tokens_a) + len(tokens_b)
        if total_length <= max_num_tokens:
            break

        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b
        wrong_trunc_tokens = wrong_tokens_a if len(wrong_tokens_a) > len(wrong_tokens_b) else wrong_tokens_b
        assert len(trunc_tokens) >= 1

        # We want to sometimes truncate from the front and sometimes from the
        # back to add more randomness and avoid biases.
        if rng.random() < 0.5:
            del trunc_tokens[0]
            del wrong_trunc_tokens[0]
        else:
            trunc_tokens.pop()
            wrong_trunc_tokens.pop()
```

224行注释：
issue
```
想了解一下，这里的pretrain中的NSP，看了一下代码，并没有发现a_tokens和b_tokens有来自同一个句子的可能性，都是随机挑句子拼接成AAAABBBB，那么这里的NSP有什么用？毕竟Bert文中这样描述：

Specifically,
when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual
next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from
the corpus (labeled as NotNext).

请问作者能否为我指点迷津。谢谢### @eugene-yh @jwu26
```

## 训练样本实例


```python
instance = TrainingInstance(
    tokens=tokens, 
    segment_ids=segment_ids,
    is_random_next=is_random_next,
    masked_lm_positions=masked_lm_positions,
    masked_lm_labels=masked_lm_labels)
instances.append(instance)
```
以一个样本为例子：

tokens: <class 'list'>: ['[CLS]', '创', '好', '业', '了', '，', '然', '后', '再', '回', '(', '老', '家', ')', '[SEP]', '又', '有', '明', '星', '吸', '毒', '了', '[SEP]']

segment_ids: <class 'list'>: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]

is_random_next: True

masked_lm_positions: <class 'list'>: [10]

masked_lm_labels: <class 'list'>: ['（']

200kb(3575条数据) -->>  50M的tf_record文件（81266个instance）

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/instances_20200110094241.png)

重复使用该句后的Mask策略是什么？采用原Bert策略，加MASK_PROB覆盖。

## run_pretraining.py
预训练,配置一些必要参数，待调优一些训练参数。

```python
##Required parameters
flags.DEFINE_string(
    "bert_config_file", "../model/pre-trained/bert_config.json",
    "The config json file corresponding to the pre-trained BERT model. "
    "This specifies the model architecture.")

flags.DEFINE_string(
    "input_file", "../bert_modified/tf_examples_nlp_2w.tfrecord",
    "Input TF example files (can be a glob or comma separated).")

flags.DEFINE_string(
    "output_dir", "../model/fine-tuned",
    "The output directory where the model checkpoints will be written.")

##Other parameters
flags.DEFINE_string(
    "init_checkpoint", "../model/pre-trained/bert_model.ckpt",
    "Initial checkpoint (usually from a pre-trained BERT model).")
```

待调优参数
```python
flags.DEFINE_integer("train_batch_size", 32, "Total batch size for training.")

flags.DEFINE_integer("eval_batch_size", 8, "Total batch size for eval.")

flags.DEFINE_float("learning_rate", 1e-5, "The initial learning rate for Adam.")

flags.DEFINE_integer("num_train_steps", 40000, "Number of training steps.")

flags.DEFINE_integer("num_warmup_steps", 6000, "Number of warmup steps.")

flags.DEFINE_integer("save_checkpoints_steps", 5000,
                     "How often to save the model checkpoint.")

flags.DEFINE_integer("iterations_per_loop", 1000,
                     "How many steps to make in each estimator call.")

flags.DEFINE_integer("max_eval_steps", 100, "Maximum number of eval steps.")
```

## char_sim.py
发音距离度量：

```python
for pron_lan1, pron_lan2 in zip(pronunciations1, pronunciations2):
    if (pron_lan1 == 'null') or (pron_lan2 == 'null'):
        pass
    else:
        similarity_lan = 0.0
        for p1 in pron_lan1.split(','):
            for p2 in pron_lan2.split(','):
                tmp_sim = 1 - edit_distance(p1, p2) / max(len(p1), len(p2))
                similarity_lan = max(similarity_lan, tmp_sim)
        similarity += similarity_lan
        count += 1

return similarity / count

```

即，发音距离的度量是通过编辑距离计算。

U+5728	在	zai4;zoi6;CAY;ZAI;tại	⿸③⿱一丨一

U+518D	再	zai4;zoi3;CAY;SAI,SA;tái	⿱一⿵丨一丨一

U+5B85	宅	zhai2;zaak6;TAYK,THAYK;TAKU;null
```
For character pair (在, 再):
    v-sim = 0.375
    p-sim = 0.8166666666666668

For character pair (在, 宅):
    v-sim = 0.11111111111111116
    p-sim = 0.4375
```
考虑到此项目的标语：中文拼写检错 / 中文拼写纠错 / 中文拼写检查，其中增加的：

粤语，日语，韩文，越南语

如论文中说的，使发音相似度更具备健壮性。


## faspeel.py

### 曲线绘制，调节confidence和similarity过滤候选
```python
class Curves(object):
    def __init__(self):
        pass

    @staticmethod
    def curve_null(confidence, similarity):
        """This curve is used when no filter is applied"""
        return True

    @staticmethod
    def curve_full(confidence, similarity):
        """This curve is used to filter out everything"""
        return False

    @staticmethod
    def curve_01(confidence, similarity):
        """
        we provide an example of how to write a curve. Empirically, curves are all convex upwards.
        Thus we can approximate the filtering effect of a curve using its tangent lines.
        """
        flag1 = 20 / 3 * confidence + similarity - 21.2 / 3 > 0
        flag2 = 0.1 * confidence + similarity - 0.6 > 0
        if flag1 or flag2:
            return True

        return False
```




## 参考
[中文语音合成基础](https://mtts.readthedocs.io/zh_CN/stable/mtts_implement/hmm_training.html)

[语音合成音素](https://auzxb.tech/2018/11/End-To-End-TTS-Overview/)