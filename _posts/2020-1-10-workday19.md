---
layout:     post
title:      workday19
subtitle:   
date:       2020-1-10
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 前言


## 长文本截断策略，前后随机截断依照Bert

```python
def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng, wrong_tokens_a, wrong_tokens_b):
    """Truncates a pair of sequences to a maximum sequence length."""
    assert len(tokens_a) == len(wrong_tokens_a)
    try:
        assert len(tokens_b) == len(wrong_tokens_b)
    except:
        print(tokens_b)
        print(wrong_tokens_b)
        exit()

    while True:
        total_length = len(tokens_a) + len(tokens_b)
        if total_length <= max_num_tokens:
            break

        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b
        wrong_trunc_tokens = wrong_tokens_a if len(wrong_tokens_a) > len(wrong_tokens_b) else wrong_tokens_b
        assert len(trunc_tokens) >= 1

        # We want to sometimes truncate from the front and sometimes from the
        # back to add more randomness and avoid biases.
        if rng.random() < 0.5:
            del trunc_tokens[0]
            del wrong_trunc_tokens[0]
        else:
            trunc_tokens.pop()
            wrong_trunc_tokens.pop()
```

224行注释：

```
想了解一下，这里的pretrain中的NSP，看了一下代码，并没有发现a_tokens和b_tokens有来自同一个句子的可能性，都是随机挑句子拼接成AAAABBBB，那么这里的NSP有什么用？毕竟Bert文中这样描述：

Specifically,
when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual
next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from
the corpus (labeled as NotNext).

请问作者能否为我指点迷津。谢谢### @eugene-yh @jwu26
```

## 训练样本实例


```python
instance = TrainingInstance(
    tokens=tokens, 
    segment_ids=segment_ids,
    is_random_next=is_random_next,
    masked_lm_positions=masked_lm_positions,
    masked_lm_labels=masked_lm_labels)
instances.append(instance)
```
以一个样本为例子：

tokens: <class 'list'>: ['[CLS]', '创', '好', '业', '了', '，', '然', '后', '再', '回', '(', '老', '家', ')', '[SEP]', '又', '有', '明', '星', '吸', '毒', '了', '[SEP]']

segment_ids: <class 'list'>: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]

is_random_next: True

masked_lm_positions: <class 'list'>: [10]

masked_lm_labels: <class 'list'>: ['（']

200kb(3575条数据) -->>  50M的tf_record文件（81266个instance）

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/instances_20200110094241.png)

重复使用该句后的Mask策略是什么？采用原Bert策略，加MASK_PROB覆盖。

## run_pretraining.py
开始进行预训练：

```python
##Required parameters
flags.DEFINE_string(
    "bert_config_file", "../model/pre-trained/bert_config.json",
    "The config json file corresponding to the pre-trained BERT model. "
    "This specifies the model architecture.")

flags.DEFINE_string(
    "input_file", "../bert_modified/tf_examples_nlp_2w.tfrecord",
    "Input TF example files (can be a glob or comma separated).")

flags.DEFINE_string(
    "output_dir", "../model/fine-tuned",
    "The output directory where the model checkpoints will be written.")

##Other parameters
flags.DEFINE_string(
    "init_checkpoint", "../model/pre-trained/bert_model.ckpt",
    "Initial checkpoint (usually from a pre-trained BERT model).")
```