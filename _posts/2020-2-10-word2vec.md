---
layout:     post
title:      word2vec
subtitle:   
date:       2020-2-10
author:     RJ
header-img: 
catalog: true
tags:
    -  nlp

---
<p id = "build"></p>
---


## word2vec
[参考](https://www.jianshu.com/p/972d0db609f2)
参数：

```python
  sentences=None
  size=100
  alpha=0.025
  window=5
  min_count=5
  max_vocab_size=None
  sample=1e-3
  seed=1
  workers=3
  min_alpha=0.0001
  sg=0
  hs=0
  negative=5
  cbow_mean=1
  hashfxn=hash
  iter=5
  null_word=0
  trim_rule=None
  sorted_vocab=1
  batch_words=MAX_WORDS_IN_BATCH

sg：这个是训练时用的算法，当为0时采用的是CBOW算法，当为1时会采用skip-gram
size：这个是定义训练的向量的长度
window：是在一个句子中，当前词和预测词的最大距离
alpha：是学习率，是控制梯度下降算法的下降速度的
seed：用于随机数发生器。与初始化词向量有关
min_count： 字典截断.，词频少于min_count次数的单词会被丢弃掉
max_vocab_size：词向量构建期间的RAM限制。如果所有不重复单词个数超过这个值，则就消除掉其中最不频繁的一个,None表示没有限制
sample：高频词汇的随机负采样的配置阈值，默认为1e-3，范围是(0,1e-5)
workers：设置多线程训练模型，机器的核数越多，训练越快
hs：如果为1则会采用hierarchica·softmax策略，Hierarchical Softmax是一种对输出层进行优化的策略，输出层从原始模型的利用softmax计算概率值改为了利用Huffman树计算概率值。如果设置为0（默认值），则负采样策略会被使用
negative：如果大于0，那就会采用负采样，此时该值的大小就表示有多少个“noise words”会被使用，通常设置在（5-20），默认是5，如果该值设置成0，那就表示不采用负采样
cbow_mean：在采用cbow模型时，此值如果是0，就会使用上下文词向量的和，如果是1（默认值），就会采用均值
hashfxn：hash函数来初始化权重。默认使用python的hash函数
iter： 迭代次数，默认为5
trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受(word, count, min_count)并返回utils.RULE_DISCARD，utils.RULE_KEEP或者utils.RULE_DEFAULT，这个设置只会用在构建词典的时候，不会成为模型的一部分
sorted_vocab： 如果为1（default），则在分配word index 的时候会先对单词基于频率降序排序。
batch_words：每一批传递给每个线程单词的数量，默认为10000，如果超过该值，则会被截断
```


用法：
```python
  from gensim.models import word2vec
  sentences = word2vec.Text8Corpus("C:/traindataw2v.txt")  # 加载语料
  model = word2vec.Word2Vec(sentences, size=200)  # 训练skip-gram模型; 默认window=5
  #获取“学习”的词向量
  print("学习：" + model["学习"])
  # 计算两个词的相似度/相关程度
  y1 = model.similarity("不错", "好")
  # 计算某个词的相关词列表
  y2 = model.most_similar("书", topn=20)  # 20个最相关的
  # 寻找对应关系
  print("书-不错，质量-")
  y3 = model.most_similar(['质量', '不错'], ['书'], topn=3)
  # 寻找不合群的词
  y4 = model.doesnt_match("书 书籍 教材 很".split())
  # 保存模型，以便重用
  model.save("db.model")
  # 对应的加载方式
  model = word2vec.Word2Vec.load("db.model")



```










































## 相关资料下载：

中文维基百科下载地址：https://dumps.wikimedia.org/zhwiki/

WikiExtractor项目git地址：https://github.com/attardi/wikiextractor

OpenCC项目git地址：https://github.com/BYVoid/OpenCC

中文分词jieba项目git地址：https://github.com/fxsjy/jieba

gensim官网地址：https://radimrehurek.com/gensim/install.html

## 一、语料库的下载
我下载是20190401文件，1.5G左右是一个压缩包，下载的时候需要注意文件的名称。

## 二、语料库文章的提取
下载完成之后，解压缩得到的是一个xml文件，里面包含了许多的文章，也有许多的日志信息。所以，我们只需要提取xml文件里面的文章就可以了。我们通过WikiExtractor来提取xml文件中的文章，它是一个意大利人写的一个Python脚本专门用来提取维基百科语料库中的文章，将每个文件分割的大小为500M，它是一个通过cmd命令来设置一些参数提取文章，提取步骤如下：


a、WikiExtractor的安装

将整个WikiExtractor项目clone或者下载到本地，打开cmd窗口，

b、维基百科语料库文章的提取

使用WikiExtractor来提取语料库中的文章，还需要使用到WikiExtractor.py脚本，通过以下命令来提取语料库中的文章

python WikiExtractor.py -b 500M -o zhwiki zhwiki-20190401-pages-articles-multistream.xml.bz2
使用WikiExtractor提取文章，会在指定目录下产生一个AA的文件夹，里面会包含很多的文件。



c、中文简体和繁体的转换

因为维基百科语料库中的文章内容里面的简体和繁体是混乱的，所以我们需要将所有的繁体字转换成为简体。这里我们利用OpenCC来进行转换。


OpenCC的使用教程请参考下篇：OpenCC中文简体和繁体互转


d、正则表达式提取文章内容并进行分词

使用WikiExtractor提取的文章，会包含许多的<doc></doc>，所以我们需要将这些不相关的内容通过正则表达式来去除。然后再通过jieba对文章进行分词，在分词的时候还需要将一些没有实际意义的词进行去除，所以在分词的之后加了一个停用词的去除。将分割之后的文章保存到文件中，每一行表示一篇文章，每个词之间使用空格进行分隔。

import logging,jieba,os,re

def get_stopwords():
    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',level=logging.INFO)
    #加载停用词表
    stopword_set = set()
    with open("../stop_words/stopwords.txt",'r',encoding="utf-8") as stopwords:
        for stopword in stopwords:
            stopword_set.add(stopword.strip("\n"))
    return stopword_set

'''
使用正则表达式解析文本
'''
def parse_zhwiki(read_file_path,save_file_path):
    #过滤掉<doc>
    regex_str = "[^<doc.*>$]|[^</doc>$]"
    file = open(read_file_path,"r",encoding="utf-8")
    #写文件
    output = open(save_file_path,"w+",encoding="utf-8")
    content_line = file.readline()
    #获取停用词表
    stopwords = get_stopwords()
     #定义一个字符串变量，表示一篇文章的分词结果
    article_contents = ""
    while content_line:
        match_obj = re.match(regex_str,content_line)
        content_line = content_line.strip("\n")
        if len(content_line) > 0:
            if match_obj:
                #使用jieba进行分词
                words = jieba.cut(content_line,cut_all=False)
                for word in words:
                    if word not in stopwords:
                        article_contents += word+" "
            else:
                if len(article_contents) > 0:
                    output.write(article_contents+"\n")
                    article_contents = ""
        content_line = file.readline()
    output.close()
e、将分词后的文件合并为一个

将分词后的多个文件合并为一个文件，便于word2vec模型的训练

'''
合并分词后的文件
'''
def merge_corpus():
    output = open("../dataset/zhwiki/BB/wiki_corpus","w",encoding="utf-8")
    input = "../dataset/zhwiki/BB"
    for i in range(3):
        file_path = os.path.join(input,str("wiki_corpus0%s"%str(i)))
        file = open(file_path,"r",encoding="utf-8")
        line = file.readline()
        while line:
            output.writelines(line)
            line = file.readline()
        file.close()
    output.close()


## 三、word2vec模型的训练



训练word2vec模型的时候，需要使用到gensim库，安装教程请参考官网，通过pip命令就可以进行安装。训练过程需要30分钟到1个小时，具体训练时间与电脑的配置相关。

import logging
from gensim.models import word2vec

def main():
    logging.basicConfig(format="%(asctime)s:%(levelname)s:%(message)s",level=logging.INFO)
    sentences = word2vec.LineSentence("../dataset/zhwiki/BB/wiki_corpus")
    # size：单词向量的维度。
    model = word2vec.Word2Vec(sentences,size=250)
    #保存模型
    model.save("../model/wiki_corpus.bin")
    # model.save("../model/wiki_corpus.model")
    # model.wv.save_word2vec_format("./sogou_word2vec/min_count-1/sogou.wor2vec.txt")

if __name__ == "__main__":
    main()


各种信息资料已上传百度云盘

链接:https://pan.baidu.com/s/1sxwpnGXj4SREcyBMP8HppQ  密码:9avd



四、word2vec模型的使用
训练完成之后，我们可以利用训练好的模型来做一些词的预测，主要包括三个方面的应用。

1、找出与指定词相似的词

返回的结果是一个列表，列表中包含了制定个数的元组，每个元组的键是词，值这个词语指定词的相似度。

 logging.basicConfig(format="%(asctime)s:%(levelname)s:%(message)s",level=logging.INFO)
    model = models.Word2Vec.load("../wiki_chinese/model/wiki_corpus.bin")
    #输入一个词找出相似的前10个词
    one_corpus = ["人工智能"]

    # ''' 词库中与one_corpus最相似的10个词'''
    result = model.most_similar(one_corpus[0],topn=10)
    print(result)
[('人工智慧', 0.8270298838615417), 

('AI', 0.7743903994560242), 

('专家系统', 0.6860651969909668), 

('智能', 0.6649989485740662), 

('虚拟现实', 0.6449255347251892), 

('计算机', 0.6375125646591187), 

('模式识别', 0.6328349113464355), 

('人工神经网络', 0.6263511776924133), 

('计算能力', 0.6243234276771545), 

('认知科学', 0.6234999299049377)]



2、计算两个词的相似度

  # 两个词的相似度
    # #输入两个词计算相似度
    two_corpus = ["腾讯","阿里巴巴"]
    res = model.similarity(two_corpus[0],two_corpus[1])
    print("similarity:%.4f"%res)


similarity:0.7268



# KeyError: "word '报了' not in vocabulary"
# 错误：“单词'报了'不在词汇中”


3、新数据与已知数据类比，分类

例如：

已知数据(原数据)

[{"intent":"天气", "words":"杭州天气怎么样"},
{"intent": "年龄", "words": "你今年几岁了"}]



新数据：

北京天气怎么样
和天气句子最相似，最后会分类问为：天气



001、用word2vec+平均词向量的方式生成句子向量

from jieba_tokenizer.tokenizer import JiebaTokenizer
from gensim import models

logger = logging.getLogger(__name__)

sys.path.append("./")

class AvgWord2vec(object):
    """用word2vec+平均词向量的方式生成句子向量"""

    def __init__(self,
                 w2v_model_path="../wiki_chinese/model/wiki_corpus.bin"):

        # self._load_w2v_model(w2v_model_path)
        self.w2v_model = models.Word2Vec.load(w2v_model_path)

        self.tokenizer = JiebaTokenizer()

    def _load_w2v_model(self, path):
        """加载w2v模型"""

        w2v_path = glob.glob("**{}".format(path), recursive=True)
        if len(w2v_path) == 0:
            logger.error("can not find w2v model")
        else:
            logger.info("Loading word2vec model file at {}".format(w2v_path[0]))
            self.w2v_model = models.Word2Vec.load(w2v_path[0], binary=True)
        return

    def transfrom_sentence_to_vec(self, sentence):
        """把句子转换成句子向量"""

        cutted_sentence = self.tokenizer.cut_sentence(sentence)
        # words_list = cutted_sentence.split(" ")
        # print(cutted_sentence)
        vec = self.seg_text_to_vector(cutted_sentence)
        # sentence
        return vec

    def seg_text_to_vector(self, sentence):
        splited_text = sentence.split(" ")
        # size：单词向量的维度。与训练时保持一致
        vector = np.zeros(250)
        num_of_word = 0
        for word in splited_text:
            try:
                a = self.w2v_model[word]
                for q in a:
                    if np.isnan(q):
                        continue
            except:
                # print(j+"is not in vocabulary")
                continue

            # 不分词语的权重
            # vector += a

            # 词语带权重
            words_weight_dict = self.word_weight_dict(sentence)
            weight = words_weight_dict.get(word)
            if weight is None:
                weight = 1.0
            vector += (a * weight)

            num_of_word += 1

        if (num_of_word == 0) is True:
            return np.zeros(250)
        else:
            vector = vector / num_of_word
            return vector

    def word_weight_dict(self, sentence):
        """根据输入sentence返回一个词权重查询词典"""

        tf_idf_list = jieba.analyse.extract_tags(sentence, topK=None, withWeight=True)
        weight_dict = {}
        for word_weight in tf_idf_list:
            weight_dict[word_weight[0]] = word_weight[1]

        return weight_dict


if __name__ == '__main__':
    sentence = "这是一句测试用的句子，通过这个文件，可以把一句话转换成一个句子向量"
    s2v = AvgWord2vec()
    vec = s2v.transfrom_sentence_to_vec(sentence)
    print(vec)
    exit(0)


注意：单词向量的维度。与训练时保持一致（我用的是250维的）



句子对比使用

from example.avg_w2v import AvgWord2vec
from scipy.spatial.distance import cosine as cos

# 夹角越小越好
best_score = 0.3


def words_base(words_list):
    '''原数据的训练保存'''
    words_infos = []

    for words_info in words_list:
        sentence = words_info.get("words")
        s2v = AvgWord2vec()
        words_vec = s2v.transfrom_sentence_to_vec(sentence)

        words_dict = {"intent": words_info.get("intent"), "words_vec": words_vec}
        words_infos.append(words_dict)

    return words_infos

def words_score(sentence, words_infos):
    '''新数据与老数据对比，分类'''
    s2v = AvgWord2vec()
    words_vec = s2v.transfrom_sentence_to_vec(sentence)

    for words_info in words_infos:
        score = cos(words_vec, words_info.get("words_vec"))
        print(score)

        # 夹角越小越相似
        if score < best_score:
            return words_info.get("intent")

    else:
        return "匹配失败"

if __name__ == '__main__':
    words_list = [
                    {"intent":"天气", "words":"杭州天气怎么样"},
                    {"intent": "年龄", "words": "你今年几岁了"}
                    ]

    words_infos = words_base(words_list)
    # print(words_infos)

    sentence = "北京天气怎么样"
    result = words_score(sentence, words_infos)
    print(result)


相似度夹角：0.1741155833744904

分类：天气



