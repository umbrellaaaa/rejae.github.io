---
layout:     post
title:      workday7
subtitle:   
date:       2019-12-23
author:     RJ
header-img: 
catalog: true
tags:
    - Job
---
<p id = "build"></p>
---

## 如何提高模型的精度

- 控制过拟合
- 激活函数的选择
- 正确使用Transformer结构

## 控制过拟合
- [warm up](https://www.zhihu.com/question/338066667)
- 梯度裁剪
- 权重衰减
- 批量归一化与层归一化

## activation 
模型的FFNN激活为None默认线性激活：
```python 
activation: Activation function. Set it to None to maintain a
    linear activation.

def get(identifier):
  if identifier is None:
    return linear
```
transformer的FFNN使用的是 relu
```python
def ff(inputs, num_units, scope="positionwise_feedforward"):
    '''position-wise feed forward net. See 3.3
    
    inputs: A 3d tensor with shape of [N, T, C].
    num_units: A list of two integers.
    scope: Optional scope for `variable_scope`.

    Returns:
      A 3d tensor with the same shape and dtype as inputs
    '''
    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
        # Inner layer
        outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)

        # Outer layer
        outputs = tf.layers.dense(outputs, num_units[1])

        # Residual connection
        outputs += inputs

        # Normalize
        outputs = ln(outputs)

    return outputs
```
而google bert的激活函数实现是 gelu：


```
We use a gelu activation (Hendrycks and Gimpel, 2016) rather than
the standard relu, following OpenAI GPT.
```

### GELUs 
[Gelu 解析](https://www.cnblogs.com/shiyublog/p/11121839.html)

GELU: Gaussian Error Linear Unit 高斯误差线性单元

**Abstract:**

The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron’s input.

GELU非线性的实现是对神经网络的输入进行随机正则化的变化，为输入匹配一个或0或1的随机值。

与ReLU的不同：GELU为其按照输入的magnitude（等级）为inputs加权值的；ReLUs是根据inputs的sign（正负）来gate（加门限）的。

论文实验证明GELU在多项计算机视觉， 自然语言处理，语音任务上效果优于ReLU， ELU。

**Introduction:**

1) 以往的激活函数为神经网络进入了非线性（binary threshold, sigmoid, ReLU, ELU, 及特点和优劣）

2) 另外神经网络中需要在网络层中加入一些noise,或通过加入dropout等方式进行随机正则化。

3) 以往的非线性和随机正则化这两部分基本都是互不相关的，因为辅助非线性变换的那些随机正则化器是与输入无关的。

4) GELU将非线性与随机正则化结合，是Adaptive Dropout的修改。

**GELU Formulation：**

GELU与ReLU, dropout, zoneout 之间的联系与区别

1) dropout 与 ReLU：ReLU中Input乘以一个0或者1，所乘的值是确定的；dropout也会乘以一个0或者1，所乘的值是随机的；

2) zoneout：zoneout是一个RNN正则化器，它会为inputs随机乘1.

3) GELU：GELU也会为inputs乘以0或者1，但不同于以上的或有明确值或随机，GELU所加的0-1mask的值是随机的，同时是依赖于inputs的分布的。可以理解为：GELU的权值取决于当前的输入input有多大的概率大于其余的inputs.
bert源码中的近似计算更简单:

```python
def gelu(input_tensor):
	cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
	return input_tesnsor*cdf

```


## 课外阅读

[target dropout](https://www.chainnews.com/articles/371844811546.htm)