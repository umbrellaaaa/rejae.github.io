---
layout:     post
title:      workday8
subtitle:   
date:       2019-12-25
author:     RJ
header-img: 
catalog: true
tags:
    - Job
---
<p id = "build"></p>
---

## 如何提高模型的精度

- 控制过拟合
- 激活函数的选择
- 正确使用Transformer结构

### 控制过拟合
- [warm up](https://www.zhihu.com/question/338066667)
- 梯度裁剪
- 权重衰减
- 批量归一化与层归一化

### activation 
模型的FFNN激活为None默认线性激活：
```python 
activation: Activation function. Set it to None to maintain a
    linear activation.

def get(identifier):
  if identifier is None:
    return linear
```
transformer的FFNN使用的是 relu
```python
def ff(inputs, num_units, scope="positionwise_feedforward"):
    '''position-wise feed forward net. See 3.3
    
    inputs: A 3d tensor with shape of [N, T, C].
    num_units: A list of two integers.
    scope: Optional scope for `variable_scope`.

    Returns:
      A 3d tensor with the same shape and dtype as inputs
    '''
    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
        # Inner layer
        outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)

        # Outer layer
        outputs = tf.layers.dense(outputs, num_units[1])

        # Residual connection
        outputs += inputs

        # Normalize
        outputs = ln(outputs)

    return outputs
```
而google bert的激活函数实现是 gelu：


```
We use a gelu activation (Hendrycks and Gimpel, 2016) rather than
the standard relu, following OpenAI GPT.
```

### GELUs 
[Gelu 解析](https://www.cnblogs.com/shiyublog/p/11121839.html)

GELU: Gaussian Error Linear Unit 高斯误差线性单元

**Abstract:**

The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron’s input.

GELU非线性的实现是对神经网络的输入进行随机正则化的变化，为输入匹配一个或0或1的随机值。

与ReLU的不同：GELU为其按照输入的magnitude（等级）为inputs加权值的；ReLUs是根据inputs的sign（正负）来gate（加门限）的。

论文实验证明GELU在多项计算机视觉， 自然语言处理，语音任务上效果优于ReLU， ELU。

**Introduction:**

1) 以往的激活函数为神经网络进入了非线性（binary threshold, sigmoid, ReLU, ELU, 及特点和优劣）

2) 另外神经网络中需要在网络层中加入一些noise,或通过加入dropout等方式进行随机正则化。

3) 以往的非线性和随机正则化这两部分基本都是互不相关的，因为辅助非线性变换的那些随机正则化器是与输入无关的。

4) GELU将非线性与随机正则化结合，是Adaptive Dropout的修改。

**GELU Formulation：**

GELU与ReLU, dropout, zoneout 之间的联系与区别

1) dropout 与 ReLU：ReLU中Input乘以一个0或者1，所乘的值是确定的；dropout也会乘以一个0或者1，所乘的值是随机的；

2) zoneout：zoneout是一个RNN正则化器，它会为inputs随机乘1.

3) GELU：GELU也会为inputs乘以0或者1，但不同于以上的或有明确值或随机，GELU所加的0-1mask的值是随机的，同时是依赖于inputs的分布的。可以理解为：GELU的权值取决于当前的输入input有多大的概率大于其余的inputs.
bert源码中的近似计算更简单:

```python
def gelu(input_tensor):
	cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
	return input_tesnsor*cdf

```



## 重构代码
遇到问题： 
source code中的每一批数据的长度都不相同，但是重构代码在model的init中就进行了positional encoding操作，导致seq_len无法获取。

然而要想拿到每个batch的seq_len，需要在train的时候获得batch，而model又是train之前init的。所以这里需要解决seq_len的动态变换 或者 固定长度padding 的问题。

仔细查看positional encoding代码：
```python
def embedding(inputs,
              vocab_size,
              num_units,
              zero_pad=True,
              scale=True,
              scope="embedding",
              reuse=None):
    with tf.variable_scope(scope, reuse=reuse):
        lookup_table = tf.get_variable('lookup_table',
                                       dtype=tf.float32,
                                       shape=[vocab_size, num_units],
                                       initializer=tf.contrib.layers.xavier_initializer())
        if zero_pad:
            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),
                                      lookup_table[1:, :]), 0)
        outputs = tf.nn.embedding_lookup(lookup_table, inputs)

        if scale:
            outputs = outputs * (num_units ** 0.5)

    return outputs


self.emb = embedding(self.x,
                        vocab_size=self.input_vocab_size, num_units=self.hidden_units, scale=True,
                        scope="enc_embed")

self.enc = self.emb + embedding(
    tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),
    vocab_size=self.max_length,
    num_units=self.hidden_units,
    zero_pad=False,
    scale=False,
    scope="enc_pe")
```
其中，位置编码复用lookup_table表进行查询，即输入[0,1,2,3...]位置序号进行查表，这会与embedding的输入inputs的id混淆，即inputs的id为0,1,2,...seq_len的拼音会受到位置训练的影响。

如果采用transformer的原本实现，那么需要取得输入的seq_len, 即需要Padding成固定的长度，所以原始的取每个batch得最大长度来padding失效，采用统一Padding长度。

修改process_file代码。加上padding到长度为100


正则化损失函数，由于参考代码是分类模型，所以对应的label是固定的n个分类中的一个，所以参考模型拿到网路的outputs后，直接将seq_len那一维度拆开，拼接到embedding那一维度，进行concat，相当于将二维的table转换成一个向量，然后进行sparse_softmax_cross_entropy_with_logits，其loss计算如下：
```python
    def cal_loss(self):
        """
        计算损失，支持二分类和多分类
        :return:
        """
        with tf.name_scope("loss"):
            losses = 0.0
            if self.config["num_classes"] == 1:
                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,
                                                                 labels=tf.reshape(self.labels, [-1, 1]))
            elif self.config["num_classes"] > 1:
                self.labels = tf.cast(self.labels, dtype=tf.int32)
                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,
                                                                        labels=self.labels)
            loss = tf.reduce_mean(losses)
            return loss
```
查看其数据输入发现：
```python
        self.inputs = tf.placeholder(tf.int32, [None, None], name="inputs")#数据输入

        self.labels = tf.placeholder(tf.float32, [None], name="labels")#标签
```
我们这里的标签输入是二维的数据，所以这里有差异，导致sparse_softmax_cross_entropy_with_logits过程中出现了问题。

详细分析一下：网络模型输出[batch,seq_len,num_units]的张量，seq_len[i]对应第i个拼音，拿这一个拼音就可以做l2_loss的计算：
```python
        #全连接层的输出
with tf.name_scope("output"):
    output_w = tf.get_variable(
        "output_w",
        shape=[output_size, self.vocab_size],
        initializer=tf.contrib.layers.xavier_initializer())
    output_b = tf.Variable(tf.constant(0.1, shape=[self.vocab_size]), name="output_b")
    self.l2_loss += tf.nn.l2_loss(output_w)
    self.l2_loss += tf.nn.l2_loss(output_b)
    self.logits = tf.nn.xw_plus_b(outputs, output_w, output_b, name="logits")
    self.predictions = self.get_predictions()
```
所以我们一个输入，就可以做其seq_len次loss计算。但是项目的源代码是怎样计算loss的呢？

```python
if self.is_training:
    # Loss
    check_onehot = tf.one_hot(self.y, depth=self.label_vocab_size)
    self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=self.label_vocab_size))
    self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)
    self.mean_loss = tf.reduce_sum(self.loss * self.istarget) / (tf.reduce_sum(self.istarget))
```
首先将label输入根据vocab转换为one_hot表示，那么将原来的shape从[batch,seq_len]==>>[batch,seq_len,vocab_size]。

debug一下logits和labels的shape:

logits：  Tensor("dense/BiasAdd:0", shape=(?, ?, 2664), dtype=float32)

labels：  Tensor("add_1:0", shape=(?, ?, 2664), dtype=float32)

这里的正则化函数实现起来是最后一步的l2_loss。而且这里对于项目而言不太适配，查阅l2_loss:

```
TensorFlow - regularization with L2 loss, how to apply to all weights, not just last one?

A shorter and scalable way of doing this would be ;

vars   = tf.trainable_variables() 
lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001
```


## 课外了解

[target dropout](https://www.chainnews.com/articles/371844811546.htm)