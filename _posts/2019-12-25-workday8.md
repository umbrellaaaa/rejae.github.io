---
layout:     post
title:      workday8
subtitle:   
date:       2019-12-25
author:     RJ
header-img: 
catalog: true
tags:
    - Job
---
<p id = "build"></p>
---

## 如何提高模型的精度

- 控制过拟合
- 激活函数的选择
- 正确使用Transformer结构

### 控制过拟合
- [warm up](https://www.zhihu.com/question/338066667)
- 梯度裁剪
- 权重衰减
- 批量归一化与层归一化

### activation 
模型的FFNN激活为None默认线性激活：
```python 
activation: Activation function. Set it to None to maintain a
    linear activation.

def get(identifier):
  if identifier is None:
    return linear
```
transformer的FFNN使用的是 relu
```python
def ff(inputs, num_units, scope="positionwise_feedforward"):
    '''position-wise feed forward net. See 3.3
    
    inputs: A 3d tensor with shape of [N, T, C].
    num_units: A list of two integers.
    scope: Optional scope for `variable_scope`.

    Returns:
      A 3d tensor with the same shape and dtype as inputs
    '''
    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
        # Inner layer
        outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)

        # Outer layer
        outputs = tf.layers.dense(outputs, num_units[1])

        # Residual connection
        outputs += inputs

        # Normalize
        outputs = ln(outputs)

    return outputs
```
而google bert的激活函数实现是 gelu：


```
We use a gelu activation (Hendrycks and Gimpel, 2016) rather than
the standard relu, following OpenAI GPT.
```

### GELUs 
[Gelu 解析](https://www.cnblogs.com/shiyublog/p/11121839.html)

GELU: Gaussian Error Linear Unit 高斯误差线性单元

**Abstract:**

The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron’s input.

GELU非线性的实现是对神经网络的输入进行随机正则化的变化，为输入匹配一个或0或1的随机值。

与ReLU的不同：GELU为其按照输入的magnitude（等级）为inputs加权值的；ReLUs是根据inputs的sign（正负）来gate（加门限）的。

论文实验证明GELU在多项计算机视觉， 自然语言处理，语音任务上效果优于ReLU， ELU。

**Introduction:**

1) 以往的激活函数为神经网络进入了非线性（binary threshold, sigmoid, ReLU, ELU, 及特点和优劣）

2) 另外神经网络中需要在网络层中加入一些noise,或通过加入dropout等方式进行随机正则化。

3) 以往的非线性和随机正则化这两部分基本都是互不相关的，因为辅助非线性变换的那些随机正则化器是与输入无关的。

4) GELU将非线性与随机正则化结合，是Adaptive Dropout的修改。

**GELU Formulation：**

GELU与ReLU, dropout, zoneout 之间的联系与区别

1) dropout 与 ReLU：ReLU中Input乘以一个0或者1，所乘的值是确定的；dropout也会乘以一个0或者1，所乘的值是随机的；

2) zoneout：zoneout是一个RNN正则化器，它会为inputs随机乘1.

3) GELU：GELU也会为inputs乘以0或者1，但不同于以上的或有明确值或随机，GELU所加的0-1mask的值是随机的，同时是依赖于inputs的分布的。可以理解为：GELU的权值取决于当前的输入input有多大的概率大于其余的inputs.
bert源码中的近似计算更简单:

```python
def gelu(input_tensor):
	cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
	return input_tesnsor*cdf

```



## 重构代码
遇到问题： 
source code中的每一批数据的长度都不相同，但是重构代码在model的init中就进行了positional encoding操作，导致seq_len无法获取。

然而要想拿到每个batch的seq_len，需要在train的时候获得batch，而model又是train之前init的。所以这里需要解决seq_len的动态变换 或者 固定长度padding 的问题。

仔细查看positional encoding代码：
```python
def embedding(inputs,
              vocab_size,
              num_units,
              zero_pad=True,
              scale=True,
              scope="embedding",
              reuse=None):
    with tf.variable_scope(scope, reuse=reuse):
        lookup_table = tf.get_variable('lookup_table',
                                       dtype=tf.float32,
                                       shape=[vocab_size, num_units],
                                       initializer=tf.contrib.layers.xavier_initializer())
        if zero_pad:
            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),
                                      lookup_table[1:, :]), 0)
        outputs = tf.nn.embedding_lookup(lookup_table, inputs)

        if scale:
            outputs = outputs * (num_units ** 0.5)

    return outputs


self.emb = embedding(self.x,
                        vocab_size=self.input_vocab_size, num_units=self.hidden_units, scale=True,
                        scope="enc_embed")

self.enc = self.emb + embedding(
    tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),
    vocab_size=self.max_length,
    num_units=self.hidden_units,
    zero_pad=False,
    scale=False,
    scope="enc_pe")
```
其中，位置编码复用lookup_table表进行查询，即输入[0,1,2,3...]位置序号进行查表，这会与embedding的输入inputs的id混淆，即inputs的id为0,1,2,...seq_len的拼音会受到位置训练的影响。

如果采用transformer的原本实现，那么需要取得输入的seq_len, 即需要Padding成固定的长度，所以原始的取每个batch得最大长度来padding失效，采用统一Padding长度。

修改process_file代码。加上padding到长度为100


## 正则化模型权重
正则化损失函数，由于参考代码是分类模型，所以对应的label是固定的n个分类中的一个，所以参考模型拿到网路的outputs后，直接将seq_len那一维度拆开，拼接到embedding那一维度，进行concat，相当于将二维的table转换成一个向量，然后进行sparse_softmax_cross_entropy_with_logits，其loss计算如下：
```python
    def cal_loss(self):
        """
        计算损失，支持二分类和多分类
        :return:
        """
        with tf.name_scope("loss"):
            losses = 0.0
            if self.config["num_classes"] == 1:
                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,
                                                                 labels=tf.reshape(self.labels, [-1, 1]))
            elif self.config["num_classes"] > 1:
                self.labels = tf.cast(self.labels, dtype=tf.int32)
                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,
                                                                        labels=self.labels)
            loss = tf.reduce_mean(losses)
            return loss
```
查看其数据输入发现：
```python
        self.inputs = tf.placeholder(tf.int32, [None, None], name="inputs")#数据输入

        self.labels = tf.placeholder(tf.float32, [None], name="labels")#标签
```
我们这里的标签输入是二维的数据，所以这里有差异，导致sparse_softmax_cross_entropy_with_logits过程中出现了问题。

详细分析一下：网络模型输出[batch,seq_len,num_units]的张量，seq_len[i]对应第i个拼音，拿这一个拼音就可以做l2_loss的计算：
```python
        #全连接层的输出
with tf.name_scope("output"):
    output_w = tf.get_variable(
        "output_w",
        shape=[output_size, self.vocab_size],
        initializer=tf.contrib.layers.xavier_initializer())
    output_b = tf.Variable(tf.constant(0.1, shape=[self.vocab_size]), name="output_b")
    self.l2_loss += tf.nn.l2_loss(output_w)
    self.l2_loss += tf.nn.l2_loss(output_b)
    self.logits = tf.nn.xw_plus_b(outputs, output_w, output_b, name="logits")
    self.predictions = self.get_predictions()
```
所以我们一个输入，就可以做其seq_len次loss计算。但是项目的源代码是怎样计算loss的呢？

```python
if self.is_training:
    # Loss
    check_onehot = tf.one_hot(self.y, depth=self.label_vocab_size)
    self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=self.label_vocab_size))
    self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)
    self.mean_loss = tf.reduce_sum(self.loss * self.istarget) / (tf.reduce_sum(self.istarget))
```
首先将label输入根据vocab转换为one_hot表示，那么将原来的shape从[batch,seq_len]==>>[batch,seq_len,vocab_size]。

debug一下logits和labels的shape:

logits：  Tensor("dense/BiasAdd:0", shape=(?, ?, 2664), dtype=float32)

labels：  Tensor("add_1:0", shape=(?, ?, 2664), dtype=float32)

这里的正则化函数实现起来是最后一步的l2_loss。而且这里对于项目而言不太适配，查阅l2_loss:
[tensorflow l2 loss](https://stackoverflow.com/questions/38286717/tensorflow-regularization-with-l2-loss-how-to-apply-to-all-weights-not-just)
```
TensorFlow - regularization with L2 loss, how to apply to all weights, not just last one?

A shorter and scalable way of doing this would be ;

vars   = tf.trainable_variables() 
lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001
```


## 课外了解

[target dropout](https://www.chainnews.com/articles/371844811546.htm)


## 重构代码块

transformer_model
```python
import tensorflow as tf
import numpy as np
from base import BaseModel


class TransformerModel(BaseModel):
    def __init__(self, config, vocab_size, word_vectors):
        super(TransformerModel, self).__init__(config=config, vocab_size=vocab_size, word_vectors=word_vectors)

        self.build_model()
        self.init_saver()

    def build_model(self):

        # 词嵌入层
        with tf.name_scope("embedding"):
            # 利用预训练的词向量初始化词嵌入矩阵
            if self.word_vectors is not None:
                embedding_w = tf.Variable(tf.cast(self.word_vectors, dtype=tf.float32, name="word2vec"),
                                          name="embedding_w")
            else:
                embedding_w = tf.get_variable("embedding_w", shape=[self.vocab_size, self.config["embedding_size"]],
                                              initializer=tf.contrib.layers.xavier_initializer())
            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]
            embedded_words = tf.nn.embedding_lookup(embedding_w, self.inputs)

        with tf.name_scope("positionEmbedding"):
            embedded_position = self._position_embedding()

        embedded_representation = embedded_words + embedded_position

        with tf.name_scope("transformer"):
            for i in range(self.config["num_blocks"]):
                with tf.name_scope("transformer-{}".format(i + 1)):
                    with tf.name_scope("multi_head_atten"):
                        # 维度[batch_size, sequence_length, embedding_size]
                        multihead_atten = self._multihead_attention(inputs=self.inputs,
                                                                    queries=embedded_representation,
                                                                    keys=embedded_representation)
                    with tf.name_scope("feed_forward"):
                        # 维度[batch_size, sequence_length, embedding_size]
                        embedded_representation = self._feed_forward(multihead_atten,
                                                                     self.config["filters"])

            # outputs = tf.reshape(embedded_representation,
            #                      [-1, self.config['sequence_length'] * self.config["embedding_size"]])  ## 将矩阵转化为一个向量
            outputs = embedded_representation
        output_size = outputs.get_shape()[-2].value

        with tf.name_scope("dropout"):
            outputs = tf.nn.dropout(outputs, keep_prob=self.keep_prob)

        ############################################################
        self.logits = tf.layers.dense(outputs, self.vocab_size)
        self.preds = tf.to_int32(tf.argmax(self.logits, axis=-1))
        self.istarget = tf.to_float(tf.not_equal(self.labels, 0))  # 该函数将返回一个 bool 类型的张量.
        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.labels)) * self.istarget) / (
            tf.reduce_sum(self.istarget))
        tf.summary.scalar('acc', self.acc)

        # Loss
        ##self.y_smoothed = self._label_smoothing(tf.one_hot(self.labels, depth=self.vocab_size))
        # self.istarget = tf.to_float(tf.not_equal(self.labels, 0))  # 该函数将返回一个 bool 类型的张量.
        # self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)
        # self.mean_loss = tf.reduce_sum(self.loss * self.istarget) / (tf.reduce_sum(self.istarget))
        #
        # # Training Scheme
        # self.global_step = tf.Variable(0, name='global_step', trainable=False)
        # self.optimizer = tf.train.AdamOptimizer(learning_rate=self.config["learning_rate"], beta1=0.9, beta2=0.98,
        #                                             epsilon=1e-8)
        # self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)
        #
        # # Summary
        # tf.summary.scalar('mean_loss', self.mean_loss)
        # self.merged = tf.summary.merge_all()
        #####################################################################################
        # 全连接层的输出
        with tf.name_scope("output"):
            output_w = tf.get_variable(
                "output_w",
                shape=[output_size, self.vocab_size],
                initializer=tf.contrib.layers.xavier_initializer())
            output_b = tf.Variable(tf.constant(0.1, shape=[self.vocab_size]), name="output_b")
            self.l2_loss += tf.nn.l2_loss(output_w)
            self.l2_loss += tf.nn.l2_loss(output_b)
            self.logits = tf.nn.xw_plus_b(outputs, output_w, output_b, name="logits")
            self.predictions = self.get_predictions()

        # 计算交叉熵损失
#        self.loss = self.cal_loss() + self.config["l2_reg_lambda"] * self.l2_loss
        # 获得训练入口
        self.train_op, self.summary_op = self.get_train_op()

    def _layer_normalization(self, inputs):
        """
        对最后维度的结果做归一化，也就是说对每个样本每个时间步输出的向量做归一化
        :param inputs:
        :return:
        """
        epsilon = self.config["ln_epsilon"]

        inputs_shape = inputs.get_shape()  # [batch_size, sequence_length, embedding_size]
        params_shape = inputs_shape[-1]

        # LayerNorm是在最后的维度上计算输入的数据的均值和方差，BN层是考虑所有维度的
        # mean, variance的维度都是[batch_size, sequence_len, 1]
        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)

        beta = tf.Variable(tf.zeros(params_shape), dtype=tf.float32)

        gamma = tf.Variable(tf.ones(params_shape), dtype=tf.float32)
        normalized = (inputs - mean) / ((variance + epsilon) ** .5)

        outputs = gamma * normalized + beta

        return outputs

    def _multihead_attention(self, inputs, queries, keys, num_units=None):
        """
        计算多头注意力
        :param inputs: 原始输入，用于计算mask
        :param queries: 添加了位置向量的词向量
        :param keys: 添加了位置向量的词向量
        :param num_units: 计算多头注意力后的向量长度，如果为None，则取embedding_size
        :return:
        """
        num_heads = self.config["num_heads"]  # multi head 的头数

        if num_units is None:  # 若是没传入值，直接去输入数据的最后一维，即embedding size.
            num_units = queries.get_shape().as_list()[-1]

        # tf.layers.dense可以做多维tensor数据的非线性映射，在计算self-Attention时，一定要对这三个值进行非线性映射，
        # 其实这一步就是论文中Multi-Head Attention中的对分割后的数据进行权重映射的步骤，我们在这里先映射后分割，原则上是一样的。
        # Q, K, V的维度都是[batch_size, sequence_length, embedding_size]
        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu)
        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)
        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu)

        # 将数据按最后一维分割成num_heads个, 然后按照第一维拼接
        # Q, K, V 的维度都是[batch_size * numHeads, sequence_length, embedding_size/numHeads]
        Q_ = tf.concat(tf.split(Q, num_heads, axis=-1), axis=0)
        K_ = tf.concat(tf.split(K, num_heads, axis=-1), axis=0)
        V_ = tf.concat(tf.split(V, num_heads, axis=-1), axis=0)

        # 计算keys和queries之间的点积，维度[batch_size * numHeads, queries_len, key_len], 后两维是queries和keys的序列长度
        similarity = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))

        # 对计算的点积进行缩放处理，除以向量长度的根号值
        similarity = similarity / (K_.get_shape().as_list()[-1] ** 0.5)

        # 在我们输入的序列中会存在padding这个样的填充词，这种词应该对最终的结果是毫无帮助的，原则上说当padding都是输入0时，
        # 计算出来的权重应该也是0，但是在transformer中引入了位置向量，当和位置向量相加之后，其值就不为0了，因此在添加位置向量
        # 之前，我们需要将其mask为0。在这里我们不仅要对keys做mask，还要对querys做mask
        # 具体关于key mask的介绍可以看看这里： https://github.com/Kyubyong/transformer/issues/3

        # 利用tf，tile进行张量扩张， 维度[batch_size * numHeads, keys_len] keys_len = keys 的序列长度
        mask = tf.tile(inputs, [num_heads, 1])

        # 增加一个维度，并进行扩张，得到维度[batch_size * numHeads, queries_len, keys_len]
        key_masks = tf.tile(tf.expand_dims(mask, 1), [1, tf.shape(queries)[1], 1])

        # tf.ones_like生成元素全为1，维度和similarity相同的tensor, 然后得到负无穷大的值
        paddings = tf.ones_like(similarity) * (-2 ** 32 + 1)

        # tf.where(condition, x, y),condition中的元素为bool值，其中对应的True用x中的元素替换，对应的False用y中的元素替换
        # 因此condition,x,y的维度是一样的。下面就是keyMasks中的值为0就用paddings中的值替换
        masked_similarity = tf.where(tf.equal(key_masks, 0), paddings,
                                     similarity)  # 维度[batch_size * numHeads, queries_len, key_len]

        # 通过softmax计算权重系数，维度 [batch_size * numHeads, queries_len, keys_len]
        weights = tf.nn.softmax(masked_similarity)

        # 因为key和query是相同的输入，当存在padding时，计算出来的相似度矩阵应该是行和列都存在mask的部分，上面的key_masks是
        # 对相似度矩阵中的列mask，mask完之后，还要对行做mask，列mask时用负无穷来使得softmax（在这里的softmax是对行来做的）
        # 计算出来的非mask部分的值相加还是为1，行mask就直接去掉就行了，以上的分析均针对batch_size等于1.
        """
        mask的相似度矩阵：[[0.5, 0.5, 0], [0.5, 0.5, 0], [0, 0, 0]]
        初始的相似度矩阵:[[1, 1, 1], [1, 1, 1], [1, 1, 1]]
        一，key_masks + 行softmax：[[0.5, 0.5, 0], [0.5, 0.5, 0], [0.5, 0.5, 0]]
        二，query_masks后：[[0.5, 0.5, 0], [0.5, 0.5, 0], [0, 0, 0]]
        """
        query_masks = tf.tile(tf.expand_dims(mask, -1), [1, 1, tf.shape(keys)[1]])
        mask_weights = tf.where(tf.equal(query_masks, 0), paddings,
                                weights)  # 维度[batch_size * numHeads, queries_len, key_len]

        # 加权和得到输出值, 维度[batch_size * numHeads, sequence_length, embedding_size/numHeads]
        outputs = tf.matmul(mask_weights, V_)

        # 将多头Attention计算的得到的输出重组成最初的维度[batch_size, sequence_length, embedding_size]
        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)

        outputs = tf.nn.dropout(outputs, keep_prob=self.keep_prob)

        # 对每个subLayers建立残差连接，即H(x) = F(x) + x
        outputs += queries
        # normalization 层
        outputs = self._layer_normalization(outputs)
        return outputs

    def _feed_forward(self, inputs, filters):

        outputs = tf.layers.dense(inputs, filters[0], activation=tf.nn.relu)

        outputs = tf.layers.dense(outputs, filters[1])

        outputs += inputs

        outputs = self._layer_normalization(outputs)

        return outputs

    def _position_embedding(self):
        """
        生成位置向量
        :return:
        """
        batch_size = self.config["batch_size"]
        sequence_length = self.config["sequence_length"]
        embedding_size = self.config["embedding_size"]

        # 生成位置的索引，并扩张到batch中所有的样本上
        position_index = tf.tile(tf.expand_dims(tf.range(tf.shape(self.inputs)[1]), 0), [batch_size, 1])
        # 根据正弦和余弦函数来获得每个位置上的embedding的第一部分
        position_embedding = np.array([[pos / np.power(10000, (i - i % 2) / embedding_size)
                                        for i in range(embedding_size)]
                                       for pos in range(sequence_length)])

        # 然后根据奇偶性分别用sin和cos函数来包装
        position_embedding[:, 0::2] = np.sin(position_embedding[:, 0::2])
        position_embedding[:, 1::2] = np.cos(position_embedding[:, 1::2])

        # 将positionEmbedding转换成tensor的格式
        position_embedding = tf.cast(position_embedding, dtype=tf.float32)

        # 得到三维的矩阵[batchSize, sequenceLen, embeddingSize]
        embedded_position = tf.nn.embedding_lookup(position_embedding, position_index)

        return embedded_position

    def _label_smoothing(self, inputs, epsilon=0.1):
        num_channels = inputs.get_shape().as_list()[-1]  # number of channels
        return ((1 - epsilon) * inputs) + (epsilon / num_channels)

```
train.py
```python
import json
import os
import argparse
import tensorflow as tf
from transformer_model import TransformerModel
from metrics import accuracy, mean
from data_loader import mk_lm_pny_vocab, mk_lm_han_vocab, process_file, read_file, next_batch


class Trainer(object):
    def __init__(self, args):
        self.args = args
        with open(os.path.join(os.path.abspath(os.path.dirname(os.getcwd())), args.config_path), "r") as fr:
            self.config = json.load(fr)
        print(self.config)
        self.load_data()
        self.word_vectors = None
        self.model = TransformerModel(config=self.config, vocab_size=self.vocab_size, word_vectors=self.word_vectors)

    def load_data(self):
        # 加载数据集
        train_path = 'data/train.tsv'
        dev_path = 'data/dev.tsv'
        test_path = 'data/test.tsv'
        pny_list, han_list = read_file(train_path)
        pny_dict_w2id, pny_dict_id2w = mk_lm_pny_vocab(pny_list)
        han_dict_w2id, han_dict_id2w = mk_lm_han_vocab(han_list)

        self.train_inputs, self.train_labels = process_file(train_path, pny_dict_w2id, han_dict_w2id,
                                                            self.config['sequence_length'])
        self.eval_inputs, self.eval_labels = process_file(dev_path, pny_dict_w2id, han_dict_w2id,
                                                          self.config['sequence_length'])

        self.vocab_size = len(pny_dict_w2id)

    def train(self):

        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9, allow_growth=True)
        sess_config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True, gpu_options=gpu_options)
        with tf.Session(config=sess_config) as sess:
            # 初始化变量值
            sess.run(tf.global_variables_initializer())
            current_step = 0

            # 创建train和eval的summary路径和写入对象
            train_summary_path = os.path.join(os.path.abspath(os.path.dirname(os.getcwd())),
                                              self.config["output_path"] + "/summary/train")
            if not os.path.exists(train_summary_path):
                os.makedirs(train_summary_path)
            train_summary_writer = tf.summary.FileWriter(train_summary_path, sess.graph)

            eval_summary_path = os.path.join(os.path.abspath(os.path.dirname(os.getcwd())),
                                             self.config["output_path"] + "/summary/eval")
            if not os.path.exists(eval_summary_path):
                os.makedirs(eval_summary_path)
            eval_summary_writer = tf.summary.FileWriter(eval_summary_path, sess.graph)

            for epoch in range(self.config["epochs"]):
                print("----- Epoch {}/{} -----".format(epoch + 1, self.config["epochs"]))

                for batch in next_batch(self.train_inputs, self.train_labels,
                                        self.config["batch_size"]):
                    self.config["sequence_length"] = batch['sequence_length']
                    summary, loss, predictions = self.model.train(sess, batch, self.config["keep_prob"])
                    acc = accuracy(pred_y=predictions, true_y=batch["y"])
                    train_summary_writer.add_summary(summary)
                    print("train: step: {}, loss: {}, acc: {}".format(current_step, loss, acc))
                    current_step += 1

                    if current_step % self.config["checkpoint_every"] == 0:
                        eval_losses = []
                        for eval_batch in next_batch(self.eval_inputs, self.eval_labels,
                                                     self.config["batch_size"]):
                            eval_summary, eval_loss, eval_predictions = self.model.eval(sess, eval_batch)
                            eval_summary_writer.add_summary(eval_summary)
                            acc = accuracy(pred_y=eval_predictions, true_y=batch["y"])

                            eval_losses.append(eval_loss)
                        print("\n")
                        print("eval:  loss: {} ,acc: {}".format(mean(eval_losses), acc))
                        print("\n")

                        if self.config["ckpt_model_path"]:
                            save_path = os.path.join(os.path.abspath(os.path.dirname(os.getcwd())),
                                                     self.config["ckpt_model_path"])
                            if not os.path.exists(save_path):
                                os.makedirs(save_path)
                            model_save_path = os.path.join(save_path, self.config["model_name"])
                            self.model.saver.save(sess, model_save_path, global_step=current_step)

            # inputs = {"inputs": tf.saved_model.utils.build_tensor_info(self.model.inputs),
            #           "keep_prob": tf.saved_model.utils.build_tensor_info(self.model.keep_prob)}
            #
            # outputs = {"predictions": tf.saved_model.utils.build_tensor_info(self.model.predictions)}
            #
            # # method_name决定了之后的url应该是predict还是classifier或者regress
            # prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs,
            #                                                                               outputs=outputs,
            #                                                                               method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)
            # legacy_init_op = tf.group(tf.tables_initializer(), name="legacy_init_op")
            # self.builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],
            #                                           signature_def_map={"classifier": prediction_signature},
            #                                           legacy_init_op=legacy_init_op)
            #
            # self.builder.save()


if __name__ == "__main__":
    # 读取用户在命令行输入的信息
    parser = argparse.ArgumentParser()
    path = r'D:\anaconda\WORKSPACE\DeepSpeechRecognition-master\DeepSpeechRecognition-master\model_language\transformer_config.json'
    parser.add_argument("--config_path", help="config path of model", default=path)
    args = parser.parse_args()
    trainer = Trainer(args)
    trainer.train()

```

base.py
```python
import tensorflow as tf


class BaseModel(object):
    def __init__(self, config, vocab_size=None, word_vectors=None):

        self.config = config
        self.vocab_size = vocab_size
        self.word_vectors = word_vectors

        self.inputs = tf.placeholder(tf.int32, [None, None], name="inputs")  # 数据输入
        self.labels = tf.placeholder(tf.int32, [None, None], name="labels")  # 标签
        self.keep_prob = tf.placeholder(tf.float32, name="keep_prob")  # dropout

        self.l2_loss = tf.constant(0.0)  # 定义l2损失
        self.loss = 0.0  # 损失
        self.train_op = None  # 训练入口
        self.summary_op = None
        self.logits = None  # 模型最后一层的输出
        self.predictions = None  # 预测结果
        self.saver = None  # 保存为ckpt模型的对象

    def cal_loss(self):

        with tf.name_scope("loss"):

            m_logits = self.logits
            m_labels = self.labels
            self.labels = tf.cast(self.labels, dtype=tf.int32)
            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,
                                                                    labels=self.labels)
            loss = tf.reduce_mean(losses)
            return loss

    def get_optimizer(self):
        optimizer = None
        if self.config["optimization"] == "adam":
            optimizer = tf.train.AdamOptimizer(self.config["learning_rate"])
        if self.config["optimization"] == "rmsprop":
            optimizer = tf.train.RMSPropOptimizer(self.config["learning_rate"])
        if self.config["optimization"] == "sgd":
            optimizer = tf.train.GradientDescentOptimizer(self.config["learning_rate"])
        return optimizer

    def get_train_op(self):
        # 定义优化器
        optimizer = self.get_optimizer()

        trainable_params = tf.trainable_variables()
        gradients = tf.gradients(self.loss, trainable_params)
        # 对梯度进行梯度截断
        clip_gradients, _ = tf.clip_by_global_norm(gradients, self.config["max_grad_norm"])
        train_op = optimizer.apply_gradients(zip(clip_gradients, trainable_params))

        tf.summary.scalar("loss", self.loss)
        summary_op = tf.summary.merge_all()

        return train_op, summary_op

    def get_predictions(self):
        logits_list = self.logits  # 按seq_len拆分
        predictions = [tf.argmax(logit, axis=-1) for logit in logits_list]
        return predictions

    def init_saver(self):
        self.saver = tf.train.Saver(tf.global_variables())

    def train(self, sess, batch, dropout_prob):

        feed_dict = {self.inputs: batch["x"],
                     self.labels: batch["y"],
                     self.keep_prob: dropout_prob}

        # 训练模型
        _, summary, loss, predictions = sess.run([self.train_op, self.summary_op, self.loss, self.predictions],
                                                 feed_dict=feed_dict)
        return summary, loss, predictions

    def eval(self, sess, batch):
        feed_dict = {self.inputs: batch["x"],
                     self.labels: batch["y"],
                     self.keep_prob: 1.0}

        summary, loss, predictions = sess.run([self.summary_op, self.loss, self.predictions], feed_dict=feed_dict)
        return summary, loss, predictions

    def infer(self, sess, inputs):

        feed_dict = {self.inputs: inputs,
                     self.keep_prob: 1.0}

        predict = sess.run(self.predictions, feed_dict=feed_dict)

        return predict

```
data_loader.py
```python
import numpy as np
import tqdm
from collections import defaultdict
import random


def read_file(filename):
    """读取文件数据"""

    with open(filename, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        pny_list = []
        han_list = []
        for line in lines:
            pny_temp = []

            wav_file, pny, han = line.split('<SEP>')
            han = han.strip()
            for pny_item in pny.split(' '):
                pny_temp.append(pny_item)

            pny_list.append(pny_temp)
            han_list.append([item for item in han])

        han_list = [han for han in han_list]
    return pny_list, han_list


def mk_lm_pny_vocab(data):
    vocab = ['<PAD>']
    shuffle_vocab = []
    for line in data:
        for pny in line:
            if pny not in shuffle_vocab:
                shuffle_vocab.append(pny)
    random.shuffle(shuffle_vocab)
    vocab.extend(shuffle_vocab)
    pny_dict_w2id = defaultdict(int)
    for index, item in enumerate(vocab):
        pny_dict_w2id[item] = index

    pny_dict_id2w = {v: k for k, v in pny_dict_w2id.items()}

    return pny_dict_w2id, pny_dict_id2w


def mk_lm_han_vocab(data):
    vocab = ['<PAD>']
    shuffle_vocab = []
    for line in data:
        # line = ''.join(line.split(' '))
        for han in line:
            if han not in shuffle_vocab:
                shuffle_vocab.append(han)

    random.shuffle(shuffle_vocab)
    vocab.extend(shuffle_vocab)

    han_dict_w2id = defaultdict(int)
    for index, item in enumerate(vocab):
        han_dict_w2id[item] = index

    han_dict_id2w = {v: k for k, v in han_dict_w2id.items()}

    return han_dict_w2id, han_dict_id2w


def process_file(filename, pny_dict_w2id, han_dict_w2id,seq_len=None):
    """将文件转换为id表示"""
    pny_list, han_list = read_file(filename)
    pny_id_list = []
    han_id_list = []
    for i in range(len(han_list)):

        pny_id_list.append([pny_dict_w2id[x] for x in pny_list[i]]+ [0]*(seq_len-len(pny_list[i])))
        han_id_list.append([han_dict_w2id[x] for x in han_list[i]]+ [0]*(seq_len-len(pny_list[i])))

        pny_id_list.append([pny_dict_w2id[x] for x in pny_list[i]])
        han_id_list.append([han_dict_w2id[x] for x in han_list[i]])

    return pny_id_list, han_id_list


# def batch_iter(x, y, batch_size=64):
#     """生成批次数据"""
#     data_len = len(x)
#     num_batch = int((data_len - 1) / batch_size) + 1
#
#     indices = np.random.permutation(np.arange(data_len))
#     x_shuffle = x[indices]
#     y_shuffle = y[indices]
#
#     for i in range(num_batch):
#         start_id = i * batch_size
#         end_id = min((i + 1) * batch_size, data_len)
#         yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]

def next_batch(self, x, y, batch_size):
    perm = np.arange(len(x))
    np.random.shuffle(perm)
    x = x[perm]
    y = y[perm]

    num_batches = len(x) // batch_size

    for i in range(num_batches):
        start = i * batch_size
        end = start + batch_size
        batch_x = np.array(x[start: end], dtype="int64")
        batch_y = np.array(y[start: end], dtype="float32")

        max_len = max([len(line) for line in batch_x])

        input_batch = np.array(
            [line + [0] * (max_len - len(line)) for line in batch_x])
        label_batch = np.array(
            [line + [0] * (max_len - len(line)) for line in batch_y])

        yield dict(x=input_batch, y=label_batch, sequence_length=max_len)

```