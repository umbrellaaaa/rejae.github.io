---
layout:     post
title:      work day  1 and 2
subtitle:   
date:       2019-12-17
author:     RJ
header-img: 
catalog: true
tags:
    - NLP
---
<p id = "build"></p>
---

## 环境搭建

1. putty
2. winscp
3. 环境配置

conda install ipykernel

source activate 环境名称

python -m ipykernel install --user --name 环境名称 --display-name "Python (环境名称)"



start_jupyter.sh

nohup jupyter notebook --ip=192.168.100.76 --allow-root &

http://192.168.100.xxx:8889/?token=xxx


[服务器外部jupyter访问](https://blog.csdn.net/mmc2015/article/details/52439212)

### 问题与解决
由于本地下载速度较慢，配置环境后才能在本地进行debug。所以在调试test.py文件的时候遇到:

FileNotFoundError: [Errno 2] No such file or directory: 'data/data_thchs30/train/A11_0.wav'

第一时间不能调试代码发现错误，回溯代码发现缺少thchs30文件，遂查找到文件：

http://www.openslr.org/18/

原始文件大小6G有多，在下载过程中，阅读数据说明。

## 讨论和学习
和前一个实习的同学交流了一下，明确了自己接下来需要做的事情：

1. 数据清洗
2. Transformer 掌握其原理，熟练应用此模型。
3. Transformer 添加拼音到汉字的纠错功能。



## Transformer资料参考
[illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)

[transformer.ipynb](https://github.com/tensorflow/docs/blob/master/site/zh-cn/tutorials/text/transformer.ipynb)

[Texar](https://github.com/asyml/texar/tree/master/texar/tf/modules)

[tensor2tensor](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models)

[official transformer](https://github.com/tensorflow/models)


## 项目中的Transformer代码调试

```python
##emb得到句子x通过lookuptabel映射得到三维[batch, seq_len  100, embedding_size  512]
self.emb = embedding(self.x, vocab_size=self.input_vocab_size, num_units=self.hidden_units, scale=True, scope="enc_embed")

##tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0),[tf.shape(self.x)[0], 1]) 
##完成输入input shape=[vocab_size,embedding_size]的positional embedding的维度配对，并且shape[1]中的数据是0,1,...shape[1]-1的位置编码

position_emb = embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0),[tf.shape(self.x)[0], 1]),                                                                   vocab_size=self.max_length,
                                            num_units=self.hidden_units,
                                            zero_pad=False,
                                            scale=False,
                                            scope="enc_pe")

self.enc = self.emb + position_emb

## Dropout
self.enc = tf.layers.dropout(self.enc, 
                            rate=self.dropout_rate, 
                            training=tf.convert_to_tensor(self.is_training))


```

## 代码相关问题
1. 为什么不用原文的sin,cos位置编码，而使用一般的整数序列编码？

2. 思考embedding+position_emb之后就使用dropout的意义在哪里？Why should we use (or not) dropout on the input layer?

```
why we do:
People generally avoid using dropout at the input layer itself. But wouldn't it be better to use it?

Adding dropout (given that it's randomized it will probably end up acting like another regularizer) should make the model more robust. It will make it more independent of a given set of features, which matter always, and let the NN find other patterns too, and then the model generalizes better even though we might be missing some important features, but that's randomly decided per epoch.

Is this an incorrect interpretation? What am I missing?

Isn't this equivalent to what we generally do by removing features one by one and then rebuilding the non-NN-based model to see the importance of it?

why not:

Why not, because the risks outweigh the benefits.

It might work in images, where loss of pixels / voxels could be somewhat "reconstructed" by other layers, also pixel/voxel loss is somewhat common in image processing. But if you use it on other problems like NLP or tabular data, dropping columns of data randomly won't improve performance and you will risk losing important information randomly. It's like running a lottery to throw away data and hope other layers can reconstruct the data.

In the case of NLP you might be throwing away important key words or in the case of tabular data, you might be throwing away data that cannot be replicated anyway else, like gens in a genome, numeric or factors in a table, etc.

I guess this could work if you are using an input-dropout-hidden layer model as you described as part of a larger ensemble though, so that the model focuses on other, less evident features of the data. However, in theory, this is already achieved by dropout after hidden layers.

```

3. key mask和 query mask的意义？

```
        #Key Masking
        key_masks = tf.sign(tf.abs(tf.reduce_sum(emb, axis=-1))) # (N, T_k)   
        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)
        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)
        
        paddings = tf.ones_like(outputs)*(-2**32+1)
        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)

针对emb,将最后embedding_size一维reduce_sum,emb的shape就转为[batch,seq], 将batch维度tile成multi_head的倍数，这样相当于[batch,(w1,w2,...,wn)]其中由于sign将w1,w2,...替换成了1，0，-1，当wi是[PAD]时候，wi被padding。key mask就是为了不受 补全短句的positional encoding的影响。 query mask只需要变换一下维度直接与keymask对应相乘就好了。
```


4. 为什么不用concat(embedding, PE), 而使用 add(embedding, PE)? In a Transformer model, why does one sum positional encoding to the embedding rather than concatenate it?


[知乎参考](https://www.zhihu.com/question/350116316/answer/860242432)

[why-does-one-sum-positional-encoding-to-the-embedding](https://datascience.stackexchange.com/questions/55901/in-a-transformer-model-why-does-one-sum-positional-encoding-to-the-embedding-ra)


![](![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191217transformer_pe.png)

Based on the graphs I have seen wrt what the encoding looks like, that means that :

- the first few bits of the embedding are completely unusable by the network because the position encoding will distort them a lot
- while there is also a large amount of positions in the embedding that are only slightly affected by the positional encoding (when you move further towards the end).

So, why not instead have smaller word embeddings (reduce memory usage) and a smaller positional encoding retaining only the most important bits of the encoding, and instead of summing the positional encoding of words keep it concatenated to word embeddings?



## Multihead_attention

Q_,K_,V_

Q_ multi K_  

-->> equal  keymask padding

-->>> * querymask   

-->>> multi V_

--->> 残差连接outputs+queries  

--->>>  normalize(outputs)  

--->>>  FFNN



## Tensor2Tensor 可视化attention


[Tensor2Tensor Colab](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)


## save_model的权重参数


[TensorFlow中对训练后的神经网络参数（权重、偏置）提取](https://blog.csdn.net/leviopku/article/details/78510977)


## 调通模型得到一下结果

```python
 the  0 th example.
WARNING:tensorflow:From /root/anaconda3/envs/tensorflow14/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 dong4
原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2
原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然
识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎动

 the  1 th example.
文本结果： ta1 jin3 ping2 yao1 bu4 de li4 liang4 zai4 yong3 dao4 shang4 xia4 fan1 teng2 yong3 dong4 she2 xing2 zhuang4 ru2 hai3 tun2 yi4 zhi2 yi3 yi1 tou2 de you1 shi4 ling3 xian1
原文结果： ta1 jin3 ping2 yao1 bu4 de li4 liang4 zai4 yong3 dao4 shang4 xia4 fan1 teng2 yong3 dong4 she2 xing2 zhuang4 ru2 hai3 tun2 yi4 zhi2 yi3 yi1 tou2 de you1 shi4 ling3 xian1
原文汉字： 他仅凭腰部的力量在泳道上下翻腾蛹动蛇行状如海豚一直以一头的优势领先
识别结果： 他仅凭腰部的力量在蛹道上下翻腾蛹动蛇行状如海豚一直以一头的优势领先

 the  2 th example.
文本结果： pao4 yan3 da3 hao3 le zha4 yao4 zen3 me zhuang1 yue4 zheng4 cai2 yao3 le yao3 ya2 shu1 di4 tuo1 qu4 yi1 fu2 guang1 bang3 zi chong1 jin4 le shui3 cuan4 dong4
原文结果： pao4 yan3 da3 hao3 le zha4 yao4 zen3 me zhuang1 yue4 zheng4 cai2 yao3 le yao3 ya2 shu1 di4 tuo1 qu4 yi1 fu2 guang1 bang3 zi chong1 jin4 le shui3 cuan4 dong4
原文汉字： 炮眼打好了炸药怎么装岳正才咬了咬牙倏地脱去衣服光膀子冲进了水窜洞
识别结果： 炮眼打好了炸药怎么装岳正才咬了咬牙倏地脱去衣服光膀子冲进了水窜洞

 the  3 th example.
文本结果： ke3 shei2 zhi1 wen2 wan2 hou4 ta1 yi1 zhao4 jing4 zi zhi1 jian4 zuo3 xia4 yan3 jian3 de xian4 you4 cu1 you4 hei1 yu3 you4 ce4 ming2 shi1 shui3 ang4 ran2
原文结果： ke3 shei2 zhi1 wen2 wan2 hou4 ta1 yi1 zhao4 jing4 zi zhi1 jian4 zuo3 xia4 yan3 jian3 de xian4 you4 cu1 you4 hei1 yu3 you4 ce4 ming2 xian3 bu2 dui4 cheng1
原文汉字： 可谁知纹完后她一照镜子只见左下眼睑的线又粗又黑与右侧明显不对称
识别结果： 可谁知纹完后她一照镜子知见左下眼睑的线右粗右黑与右侧明诗水盎然

 the  4 th example.
文本结果： yi1 jin4 men2 wo3 bei4 jing1 dai1 le zhe4 hu4 ming2 jiao4 pang2 ji2 de lao3 nong2 shi4 kang4 mei3 yuan2 chao2 fu4 shang1 hui2 xiang1 de lao3 bing1 qi1 zi3 chang2 nian2 you3 bing4 jia1 tu2 si4 bi4 yi1 pin2 ru2 xi3
原文结果： yi1 jin4 men2 wo3 bei4 jing1 dai1 le zhe4 hu4 ming2 jiao4 pang2 ji2 de lao3 nong2 shi4 kang4 mei3 yuan2 chao2 fu4 shang1 hui2 xiang1 de lao3 bing1 qi1 zi3 chang2 nian2 you3 bing4 jia1 tu2 si4 bi4 yi1 pin2 ru2 xi3
原文汉字： 一进门我被惊呆了这户名叫庞吉的老农是抗美援朝负伤回乡的老兵妻子长年有病家徒四壁一贫如洗
识别结果： 一进门我被惊呆了这户名叫庞吉的老农是抗美援朝负伤回乡的老兵妻子长年有病家徒四壁一贫如洗

 the  5 th example.
文本结果： zou3 chu1 cun1 zi lao3 yuan3 lao3 yuan3 wo3 hai2 hui2 tou2 zhang1 wang4 na4 ge4 an1 ning2 tian2 jing4 de xiao3 yuan4 na4 ge4 shi3 wo3 zhong1 shen1 nan2 wang4 shui3 cuan4 xian1
原文结果： zou3 chu1 cun1 zi lao3 yuan3 lao3 yuan3 wo3 hai2 hui2 tou2 zhang1 wang4 na4 ge4 an1 ning2 tian2 jing4 de xiao3 yuan4 na4 ge4 shi3 wo3 zhong1 shen1 nan2 wang4 de xiao3 yuan4
原文汉字： 走出村子老远老远我还回头张望那个安宁恬静的小院那个使我终身难忘的小院
识别结果： 走出村子老远老远我还回头张望那个安宁恬静的小院那个使我终身难望水窜先

 the  6 th example.
文本结果： er4 yue4 si4 ri4 zhu4 jin4 xin1 xi1 men2 wai4 luo2 jia1 nian3 wang2 jia1 gang1 zhu1 zi4 qing1 wen2 xun4 te4 di4 cong2 dong1 men2 wai4 gan3 lai2 qing4 he4
原文结果： er4 yue4 si4 ri4 zhu4 jin4 xin1 xi1 men2 wai4 luo2 jia1 nian3 wang2 jia1 gang1 zhu1 zi4 qing1 wen2 xun4 te4 di4 cong2 dong1 men2 wai4 gan3 lai2 qing4 he4
原文汉字： 二月四日住进新西门外罗家碾王家冈朱自清闻讯特地从东门外赶来庆贺
识别结果： 二月四日住进新西门外罗家碾王家冈朱自清闻讯特地从东门外赶来庆贺

 the  7 th example.
文本结果： dan1 wei4 bu2 shi4 wo3 lao3 die1 kai1 de ping2 shen2 me yao4 yi1 ci4 er4 ci4 zhao4 gu4 wo3 wo3 bu4 neng2 ba3 zi4 ji3 de bao1 fu2 wang3 xue2 xiao4 qu4
原文结果： dan1 wei4 bu2 shi4 wo3 lao3 die1 kai1 de ping2 shen2 me yao4 yi1 ci4 er4 ci4 zhao4 gu4 wo3 wo3 bu4 neng2 ba3 zi4 ji3 de bao1 fu2 wang3 xue2 xiao4 shuai3
原文汉字： 单位不是我老爹开的凭什么要一次二次照顾我我不能把自己的包袱往学校甩
识别结果： 单位不是我老爹开的凭什么要一次二次照顾我我不能把自己的包袱往学校去

 the  8 th example.
文本结果： dou1 yong4 cao3 mao4 huo4 ge1 bo zhou3 hu4 zhe wan3 lie4 lie4 qie ju1 chuan1 guo4 lan4 ni2 tang2 ban1 de yuan4 ba4 pao3 hui2 zi4 ji3 de su4 she4 chu1
原文结果： dou1 yong4 cao3 mao4 huo4 ge1 bo zhou3 hu4 zhe wan3 lie4 lie4 qie ju1 chuan1 guo4 lan4 ni2 tang2 ban1 de yuan4 ba4 pao3 hui2 zi4 ji3 de su4 she4 qu4 le
原文汉字： 都用草帽或胳膊肘护着碗趔趔趄趄穿过烂泥塘般的院坝跑回自己的宿舍去了
识别结果： 都用草帽或胳膊肘护着碗趔趔趄趄穿过烂泥塘般的院坝跑回自己的宿舍出

 the  9 th example.
文本结果： xiang1 gang3 yan3 yi4 quan1 huan1 ying2 mao2 a1 min3 jia1 meng2 wu2 xian4 tai2 yu3 hua2 xing1 yi1 xie1 zhong4 da4 de yan3 chang4 huo2 dong4 dou1 yao1 qing3 ta1 chu1 chang3 you3 ji3 ci4 hai2 te4 yi4 an1 pai2 ya1 zhou4 yan3 ru2 xi3
原文结果： xiang1 gang3 yan3 yi4 quan1 huan1 ying2 mao2 a1 min3 jia1 meng2 wu2 xian4 tai2 yu3 hua2 xing1 yi1 xie1 zhong4 da4 de yan3 chang4 huo2 dong4 dou1 yao1 qing3 ta1 chu1 chang3 you3 ji3 ci4 hai2 te4 yi4 an1 pai2 ya1 zhou4 yan3 chu1
原文汉字： 香港演艺圈欢迎毛阿敏加盟无线台与华星一些重大的演唱活动都邀请她出场有几次还特意安排压轴演出
识别结果： 香港演意圈欢迎毛阿敏加盟无线台与华星一些重大的演唱活动都邀请她出场有几次还特意安排压轴演如洗
词错误率： 0.05507246376811594

```