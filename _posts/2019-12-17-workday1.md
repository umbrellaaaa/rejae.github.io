---
layout:     post
title:      work day1
subtitle:   
date:       2019-12-17
author:     RJ
header-img: 
catalog: true
tags:
    - NLP
---
<p id = "build"></p>
---

## 环境搭建

1. putty
2. winscp
3. 环境配置

conda install ipykernel

source activate 环境名称

python -m ipykernel install --user --name 环境名称 --display-name "Python (环境名称)"



start_jupyter.sh

nohup jupyter notebook --ip=192.168.100.76 --allow-root &

http://192.168.100.xxx:8889/?token=xxx


[服务器外部jupyter访问](https://blog.csdn.net/mmc2015/article/details/52439212)

### 问题与解决
由于本地下载速度较慢，配置环境后才能在本地进行debug。所以在调试test.py文件的时候遇到:

FileNotFoundError: [Errno 2] No such file or directory: 'data/data_thchs30/train/A11_0.wav'

第一时间不能调试代码发现错误，回溯代码发现缺少thchs30文件，遂查找到文件：

http://www.openslr.org/18/

原始文件大小6G有多，在下载过程中，阅读数据说明。

## 讨论和学习
和前一个实习的同学交流了一下，明确了自己接下来需要做的事情：

1. 数据清洗
2. Transformer 掌握其原理，熟练应用此模型。
3. Transformer 添加拼音到汉字的纠错功能。



## Transformer资料参考
[illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)

[transformer.ipynb](https://github.com/tensorflow/docs/blob/master/site/zh-cn/tutorials/text/transformer.ipynb)

[Texar](https://github.com/asyml/texar/tree/master/texar/tf/modules)

[tensor2tensor](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models)

[official transformer](https://github.com/tensorflow/models)


## 项目中的Transformer代码调试

```python
##emb得到句子x通过lookuptabel映射得到三维[batch, seq_len  100, embedding_size  512]
self.emb = embedding(self.x, vocab_size=self.input_vocab_size, num_units=self.hidden_units, scale=True, scope="enc_embed")

##tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0),[tf.shape(self.x)[0], 1]) 
##完成输入input shape=[vocab_size,embedding_size]的positional embedding的维度配对，并且shape[1]中的数据是0,1,...shape[1]-1的位置编码

position_emb = embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0),[tf.shape(self.x)[0], 1]),                                                                   vocab_size=self.max_length,
                                            num_units=self.hidden_units,
                                            zero_pad=False,
                                            scale=False,
                                            scope="enc_pe")

self.enc = self.emb + position_emb

## Dropout
self.enc = tf.layers.dropout(self.enc, 
                            rate=self.dropout_rate, 
                            training=tf.convert_to_tensor(self.is_training))


```

## 代码相关问题
1. 为什么不用原文的sin,cos位置编码，而使用一般的整数序列编码？

2. 思考embedding+position_emb之后就使用dropout的意义在哪里？Why should we use (or not) dropout on the input layer?

```
why we do:
People generally avoid using dropout at the input layer itself. But wouldn't it be better to use it?

Adding dropout (given that it's randomized it will probably end up acting like another regularizer) should make the model more robust. It will make it more independent of a given set of features, which matter always, and let the NN find other patterns too, and then the model generalizes better even though we might be missing some important features, but that's randomly decided per epoch.

Is this an incorrect interpretation? What am I missing?

Isn't this equivalent to what we generally do by removing features one by one and then rebuilding the non-NN-based model to see the importance of it?

why not:

Why not, because the risks outweigh the benefits.

It might work in images, where loss of pixels / voxels could be somewhat "reconstructed" by other layers, also pixel/voxel loss is somewhat common in image processing. But if you use it on other problems like NLP or tabular data, dropping columns of data randomly won't improve performance and you will risk losing important information randomly. It's like running a lottery to throw away data and hope other layers can reconstruct the data.

In the case of NLP you might be throwing away important key words or in the case of tabular data, you might be throwing away data that cannot be replicated anyway else, like gens in a genome, numeric or factors in a table, etc.

I guess this could work if you are using an input-dropout-hidden layer model as you described as part of a larger ensemble though, so that the model focuses on other, less evident features of the data. However, in theory, this is already achieved by dropout after hidden layers.

```

3. key mask和 query mask的意义？
```
        #Key Masking
        key_masks = tf.sign(tf.abs(tf.reduce_sum(emb, axis=-1))) # (N, T_k)   
        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)
        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)
        
        paddings = tf.ones_like(outputs)*(-2**32+1)
        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)

针对emb,将最后embedding_size一维reduce_sum,emb的shape就转为[batch,seq], 将batch维度tile成multi_head的倍数，这样相当于[batch,(w1,w2,...,wn)]其中由于sign将w1,w2,...替换成了1，0，-1，当wi是[PAD]时候，wi被padding。key mask就是为了不受 补全短句的positional encoding的影响。 query mask只需要变换一下维度直接与keymask对应相乘就好了。
```

4. 为什么不用concat(embedding, PE), 而使用 add(embedding, PE)? In a Transformer model, why does one sum positional encoding to the embedding rather than concatenate it?
[知乎参考](https://www.zhihu.com/question/350116316/answer/860242432)
[why-does-one-sum-positional-encoding-to-the-embedding](https://datascience.stackexchange.com/questions/55901/in-a-transformer-model-why-does-one-sum-positional-encoding-to-the-embedding-ra)
![](![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191217transformer_pe.png)

Based on the graphs I have seen wrt what the encoding looks like, that means that :

- the first few bits of the embedding are completely unusable by the network because the position encoding will distort them a lot
- while there is also a large amount of positions in the embedding that are only slightly affected by the positional encoding (when you move further towards the end).

So, why not instead have smaller word embeddings (reduce memory usage) and a smaller positional encoding retaining only the most important bits of the encoding, and instead of summing the positional encoding of words keep it concatenated to word embeddings?



## Multihead_attention

Q_,K_,V_

Q_ multi K_  

-->> equal  keymask padding

-->>> * querymask   

-->>> multi V_

--->> 残差连接outputs+queries  

--->>>  normalize(outputs)  

--->>>  FFNN



## Tensor2Tensor 可视化attention

[Tensor2Tensor Colab](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)


## save_model的权重参数
[TensorFlow中对训练后的神经网络参数（权重、偏置）提取](https://blog.csdn.net/leviopku/article/details/78510977)