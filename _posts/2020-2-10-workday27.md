---
layout:     post
title:      workday27
subtitle:   
date:       2020-2-10
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 相似度计算
example:

正确句子： 甚至出现交易几乎停滞的情况 

纠错句子： 甚至出现交易几乎停止的情况

出错位置： 9
```python
[(0, '甚至'),
 (0, '甚至'),
 (2, '出现'),
 (2, '出现'),
 (4, '交易'),
 (4, '交易'),
 (6, '几乎'),
 (6, '几乎'),
 (8, '停止'),
 (8, '停止'),
 (10, '的'),
 (11, '情况'),
 (11, '情况')]
```
取第9个元组，判断其index与原正确句子是否相同，不同则纠错失败， 相同则进行相似度计算。

关键：由纠错后的分词来直接对原句进行分词，避免了分词方式不同，带来的不匹配问题。（但由于过长切分，如：但因为 与 是因为 则会引入误差，此情况应该较少）


```python

import jieba

def get_cut_data(corr_series,origin_series):
    corr_iterator_list = [iterator for iterator in corr_series.apply(jieba.cut)]
    cut_corr_data = []
    cut_origin_data = []
    for index,item in enumerate(corr_iterator_list):
        
        corr_data = []
        origin_data = []
        outer_index = 0
        origin_temp_data = ''
        for i in item:
            length = len(i)
            origin = origin_series[index]
            origin_temp_data=origin[outer_index:outer_index+length]
            for inner_index in range(outer_index,outer_index+length):

                corr_data.append((outer_index,i))
                origin_data.append((outer_index,origin_temp_data))
            outer_index = outer_index+length
            
        cut_corr_data.append(corr_data)
        cut_origin_data.append(origin_data)
        break
    return cut_corr_data,cut_origin_data


([[(0, '甚至'),
   (0, '甚至'),
   (2, '出现'),
   (2, '出现'),
   (4, '交易'),
   (4, '交易'),
   (6, '几乎'),
   (6, '几乎'),
   (8, '停止'),
   (8, '停止'),
   (10, '的'),
   (11, '情况'),
   (11, '情况')]],
 [[(0, '甚至'),
   (0, '甚至'),
   (2, '出现'),
   (2, '出现'),
   (4, '交易'),
   (4, '交易'),
   (6, '几乎'),
   (6, '几乎'),
   (8, '停滞'),
   (8, '停滞'),
   (10, '的'),
   (11, '情况'),
   (11, '情况')]])
```
根据error_location_list，取出对应纠错位置的id, 根据 id 取出对应的元组词， 拿到一个句子中的多个词后，根据元组的第一个参数去重。

```python
## 根据error_location_list来
compare_tuple_set = set()
for index,sentence in enumerate(df['error_location_list']):
    for item in sentence:
        print(cut_origin_data[index][item])
        print(cut_corr_data[index][item])
        compare_tuple_set.add((cut_origin_data[index][item][1],cut_corr_data[index][item][1]))

#得到结果共12484对：
('新房', '新房')
('俩', '还')
('一步', '以步')
('按', '她')
('乖', '乖')
('非常', '施长')
('美的', '美帝')
('过任', '国计')
('成飞', '乘飞')
('潜移默化', '诙言默画')
('再也', '在于')
('低迷', '低迷')
('了', '们')
('友', '有')
('预计', '认计')
('基地', '机种')
('下调', '下调')
...
...
```

## 根据word2vec计算tuple的相似度
参考：


中文维基百科下载地址：https://dumps.wikimedia.org/zhwiki/



WikiExtractor项目git地址：https://github.com/attardi/wikiextractor



OpenCC项目git地址：https://github.com/BYVoid/OpenCC



中文分词jieba项目git地址：https://github.com/fxsjy/jieba



gensim官网地址：https://radimrehurek.com/gensim/install.html

使用wikidata训练word2vec模型，并计算相似度。




