layout:     post
title:      workday10
subtitle:   
date:       2019-12-26
author:     RJ
header-img: 
catalog: true
tags:
    - Job
---
<p id = "build"></p>
---

昨天由于生病，工作落下了。

昨天参考transformer的文本分类模型，调通，发现loss和acc都不理想。所以今天直接修改源代码，一步一步的测试模型效果。


## step1: 修改数据加载项

修改完数据加载项后，训练一个epoch，在这个epoch的每过100个batch，输出acc和loss。对比发现新改的数据加载项运行结果缓慢，仔细分析。在数据输入前我对每一个数据都padding到100的长度，导致训练过程中，所有短句变为了长句，导致训练速度很慢。修改数据加载项，将w2id操作放到next_batch中padding为每一个batch的最长长度。训练速度恢复正常。

源代码：
```
enter epoch training
average acc= 0.6059002131223679 cost= 3.7502832889556883
average acc= 0.6581291913986206 cost= 3.1885115623474123
average acc= 0.724135673046112 cost= 2.3097871780395507
average acc= 0.6739206731319427 cost= 2.5840892791748047
average acc= 0.6878040134906769 cost= 2.6681422710418703
average acc= 0.7554251432418824 cost= 2.388796317577362
average acc= 0.7260848224163056 cost= 2.361169147491455
average acc= 0.6045062601566314 cost= 2.725984263420105
average acc= 0.7070521354675293 cost= 2.408917474746704
average acc= 0.8062520086765289 cost= 1.9536547422409059
average acc= 0.8010896265506744 cost= 1.784736156463623
average acc= 0.7367690324783325 cost= 2.001241993904114
average acc= 0.8395167052745819 cost= 1.6710567474365234
average acc= 0.8303306341171265 cost= 1.8605268120765686
average acc= 0.8221604883670807 cost= 1.8159384369850158
average acc= 0.7634373545646668 cost= 2.188371109962463
average acc= 0.6557466387748718 cost= 2.628843832015991
average acc= 0.8208128988742829 cost= 1.7290584564208984
average acc= 0.8139382004737854 cost= 1.7837854862213134
average acc= 0.8292209208011627 cost= 1.7022520184516907
average acc= 0.7794299602508545 cost= 2.0221943855285645
average acc= 0.7298405587673187 cost= 2.011440062522888
average acc= 0.7441449224948883 cost= 2.228485584259033
average acc= 0.669456559419632 cost= 2.441757607460022
average acc= 0.8126269102096557 cost= 1.7742337703704834
average acc= 0.8129978597164154 cost= 1.7185113072395324
average acc= 0.7926573336124421 cost= 1.8906293034553527
average acc= 0.7716241836547851 cost= 2.032231020927429
average acc= 0.7966776430606842 cost= 1.8886690616607666
average acc= 0.82615424990654 cost= 1.7719403028488159
average acc= 0.844136768579483 cost= 1.7313872933387757
average acc= 0.7397358596324921 cost= 2.3039756417274475
average acc= 0.6944393754005432 cost= 2.42236750125885
average acc= 0.7634942889213562 cost= 2.023843014240265
average acc= 0.8075268507003784 cost= 1.7727800488471985
batch  3582 : average loss =  2.221133840580088 average acc =  0.7493348719429405
```

修改代码
```
enter epoch training
acc= 0.6439542263746262 cost= 3.7598569869995115
acc= 0.7476384818553925 cost= 2.671954298019409
acc= 0.7039936006069183 cost= 2.507528781890869
acc= 0.7772251725196838 cost= 2.09833961725235
acc= 0.7534208416938781 cost= 2.1019667983055115
acc= 0.7369805693626403 cost= 2.1480162262916567
acc= 0.7357022523880005 cost= 2.1401012420654295
acc= 0.7458811044692993 cost= 2.107592487335205
acc= 0.7412943959236145 cost= 2.1013749122619627
acc= 0.7722874879837036 cost= 1.9295800089836121
acc= 0.7727415800094605 cost= 1.9270888090133667
acc= 0.7450265705585479 cost= 1.9679744124412537
acc= 0.7664263606071472 cost= 1.9043409705162049
acc= 0.7283282041549682 cost= 2.0105485320091248
acc= 0.7505691885948181 cost= 2.074885904788971
acc= 0.7826587498188019 cost= 1.8962715864181519
acc= 0.7777101159095764 cost= 1.9475908041000367
acc= 0.7667924344539643 cost= 1.9721711158752442
acc= 0.7969763040542602 cost= 1.8858129024505614
acc= 0.7758720457553864 cost= 1.9573217630386353
acc= 0.8044050395488739 cost= 1.8326051235198975
acc= 0.8146495819091797 cost= 1.785447323322296
acc= 0.8069379687309265 cost= 1.8106085777282714
acc= 0.789966082572937 cost= 1.9375856757164
acc= 0.764629191160202 cost= 2.024051821231842
acc= 0.7998378992080688 cost= 1.8837562084197998
acc= 0.8248593151569367 cost= 1.7742550849914551
acc= 0.7721165299415589 cost= 1.9447824954986572
acc= 0.8111102461814881 cost= 1.7535919070243835
acc= 0.7755769252777099 cost= 1.8635669231414795
acc= 0.7804965674877167 cost= 1.9126514792442322
acc= 0.8115160465240479 cost= 1.9189164757728576
acc= 0.7766305863857269 cost= 1.9317401051521301
acc= 0.784650593996048 cost= 1.9555382966995238
acc= 0.7843230664730072 cost= 1.9279656291007996
batch  3582 : average loss =  2.1111554148742453 average acc =  0.7610840393478429
```

对比发现，在ai_shell dev文件下，训练一个epoch二者的平均acc和loss都非常接近。第一步数据加载项修改完成。


## step2: 修改acc和loss的计算为移动均值，为训练过程加入evalue操作：

```python
        for batch in next_batch(train_inputs, train_labels, lm_args.batch_size, pny_dict_w2id, han_dict_w2id):
            input_batch, label_batch = batch['x'], batch['y']
            try:
                feed = {lm.x: input_batch, lm.y: label_batch}
                acc, cost, _ = sess.run([lm.acc, lm.mean_loss, lm.train_op], feed_dict=feed)
                total_acc.append(acc)
                total_loss.append(cost)
                i = i + 1
            except Exception as e:
                print(e, 'batch_num:', i)
                print(input_batch, label_batch)

            if i % 100 == 0:
                print('acc=', sum(total_acc[-10:]) / 10, 'cost=', sum(total_loss[-10:]) / 10)
                ##############################################evaluate
                print('evaluate:')
                eval_total_acc = []
                eval_total_loss = []
                j = 0
                for batch in next_batch(eval_inputs, eval_labels, lm_args.batch_size, pny_dict_w2id, han_dict_w2id):

                    input_batch, label_batch = batch['x'], batch['y']
                    feed = {lm.x: input_batch, lm.y: label_batch}
                    acc, cost, _ = sess.run([lm.acc, lm.mean_loss, lm.train_op], feed_dict=feed)
                    eval_total_acc.append(acc)
                    eval_total_loss.append(cost)
                    j = j + 1
                    if j == 100:
                        print('eval_total_acc:', sum(eval_total_acc) / (len(eval_total_acc)))
                        print('eval_total_loss:', sum(eval_total_loss) / (len(eval_total_loss)))
                        break
                ###############################################################################
            if (epoch * batch_num + i) % 10 == 0:
                rs = sess.run(merged, feed_dict=feed)
                writer.add_summary(rs, epoch * batch_num + i)
        print('batch ', i + 1, ': average loss = ', sum(total_loss) / batch_num, 'average acc = ',
              sum(total_acc) / batch_num)

```
```
enter epoch training
acc= 0.7031409722566605 cost= 2.7487846922874453
eval_total_acc: 0.6634967997670174 eval_total_loss: 2.945201554298401
acc= 0.741613584458828 cost= 2.1059571850299834
eval_total_acc: 0.7127553337812423 eval_total_loss: 2.268597273826599
acc= 0.7388679391145706 cost= 2.00633419752121
eval_total_acc: 0.729096200466156 eval_total_loss: 2.1392676961421966
acc= 0.735886612534523 cost= 1.9900279927253723
eval_total_acc: 0.7359545642137527 eval_total_loss: 2.044731003046036
acc= 0.766111388206482 cost= 1.9145163249969483
eval_total_acc: 0.7392314738035202 eval_total_loss: 2.019765703678131
acc= 0.7687549012899398 cost= 1.911255375146866
eval_total_acc: 0.7672740387916565 eval_total_loss: 1.9357702004909516
acc= 0.7854423177242279 cost= 1.858759068250656
eval_total_acc: 0.7768335145711899 eval_total_loss: 1.8816068220138549
acc= 0.7971291065216064 cost= 1.830035809278488
eval_total_acc: 0.8027953463792801 eval_total_loss: 1.8393085634708404
acc= 0.8028074550628662 cost= 1.8198472630977631
eval_total_acc: 0.8144273710250854 eval_total_loss: 1.7980825436115264
acc= 0.8135244083404541 cost= 1.8038136553764343
eval_total_acc: 0.8197382593154907 eval_total_loss: 1.8015415394306182
acc= 0.8043141216039658 cost= 1.8085044372081756
eval_total_acc: 0.8369180256128311 eval_total_loss: 1.734100157022476
batch  3582 : average loss =  2.0912751216057526 average acc =  0.7550594171005125
```

## step3： 替换positional_encoding为标准的sin,cos方式编码。

```python
def _position_embedding(inputs, max_length, hidden_units):
    batch_size = tf.shape(inputs)[0]
    sequence_length = max_length
    embedding_size = hidden_units

    # 生成位置的索引，并扩张到batch中所有的样本上
    position_index = tf.tile(tf.expand_dims(tf.range(tf.shape(inputs)[1]), 0), [batch_size, 1])
    # 根据正弦和余弦函数来获得每个位置上的embedding的第一部分
    position_embedding = np.array([[pos / np.power(10000, (i - i % 2) / embedding_size)
                                    for i in range(embedding_size)]
                                   for pos in range(sequence_length)])

    # 然后根据奇偶性分别用sin和cos函数来包装
    position_embedding[:, 0::2] = np.sin(position_embedding[:, 0::2])
    position_embedding[:, 1::2] = np.cos(position_embedding[:, 1::2])

    # 将positionEmbedding转换成tensor的格式
    position_embedding = tf.cast(position_embedding, dtype=tf.float32)

    # 得到三维的矩阵[batchSize, sequenceLen, embeddingSize]
    embedded_position = tf.nn.embedding_lookup(position_embedding, position_index)

    return embedded_position
```

```
enter epoch training
acc= 0.7074218249320984 cost= 2.7479679334163665
evaluate:
eval_total_acc: 0.6627151066064835
eval_total_loss: 2.9718770956993104
acc= 0.7209131747484208 cost= 2.1679107892513274
evaluate:
eval_total_acc: 0.7081493377685547
eval_total_loss: 2.2933158612251283
acc= 0.7351829922199249 cost= 2.007067748308182
evaluate:
eval_total_acc: 0.7217354756593705
eval_total_loss: 2.1222776114940642
acc= 0.7606028217077255 cost= 1.9197818338871002
evaluate:
eval_total_acc: 0.7431985080242157
eval_total_loss: 2.0257984149456023
acc= 0.7682702279090882 cost= 1.8844773709774016
evaluate:
eval_total_acc: 0.7535757833719253
eval_total_loss: 1.987589212656021
acc= 0.7825768518447876 cost= 1.8662899148464203
evaluate:
eval_total_acc: 0.7654590773582458
eval_total_loss: 1.9492652547359466
acc= 0.7841318231821061 cost= 1.8638798320293426
evaluate:
eval_total_acc: 0.7866767108440399
eval_total_loss: 1.8718446803092956
acc= 0.7919829595088959 cost= 1.860753870010376
evaluate:
eval_total_acc: 0.7959450846910476
eval_total_loss: 1.846416128873825
acc= 0.7940960311889649 cost= 1.86176926612854
evaluate:
eval_total_acc: 0.8120561575889588
eval_total_loss: 1.8015945649147034
acc= 0.7997326129674911 cost= 1.8440140140056611
evaluate:
eval_total_acc: 0.8183250111341477
eval_total_loss: 1.7869949519634247
acc= 0.8069122511148453 cost= 1.8145265185832977
evaluate:
eval_total_acc: 0.8277986997365951
eval_total_loss: 1.765822777748108
batch  3582 : average loss =  2.09671976885227 average acc =  0.7551557793188606

```
这里很容易的就能切换两种位置编码，但是由于目前是在简单测试模型和整改模型，所以这里只需要保留两种编码的调用，在整改完成之后，用更多数据来测试哪种位置编码效果更好。


## step4: test,测试模型，发现文字完全无法对号
由于在构建vocab的时候，我对vocab进行了shuffle，所以按源代码test执行的时候，也会重建一次vocab，但是两次shuffle的结果是不一样的。由于词表只需要创建一次，test重复操作也是无意义的，所以在train构建词表的过程中，可以将词表保存下来，test直接拿到词表作一个映射即可。

为词表创建，构建字典并存入Json文件。
```python
    def mk_lm_han_vocab(self, data):
        final_vocab = ['<PAD>']
        vocab = []
        for line in tqdm(data):
            # line = ''.join(line.split(' '))
            for han in line:
                if han not in vocab:
                    vocab.append(han)
        final_vocab.extend(vocab)
        han_dict = dict(zip(final_vocab, range(len(final_vocab))))

        with open('vocab/han_vocab.json', 'a', encoding='utf-8') as f:
            json.dump(han_dict, f, ensure_ascii=False)
            f.write('\n')
        return final_vocab
```

读取json文件，用于test词表映射。在这个过程中，发现原项目的train,dev数据加载是分开的，由于创建字典过程中使用了shuffle，所以二者的字典表不一样，为此改用random.Random(4).shuffle(x)，使用相同的随机种子。

从这里的复杂过程可以见得，代码冗余度很高，新改写的数据加载根本不会出现这些问题。

在更改train和dev文件后，遇到问题：
```python
indices[2,0] = 1056 is not in [0, 1042) [[{{node enc_embed/embedding_lookup}}]]
```
这个错误说的是训练集的词表范围是[0,1042],而验证集在查表操作的时候有index=1056超过了lookup_table。最根本的原因就是train和dev都进行了词表创建，不合理。

调试更改后的模型，出现error:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [2664] rhs shape= [2998]
```
经过详细分析，发现han的字典表中，一共有2664个汉字，但是测试集的数据有2998个汉字，所以出现了这样的错误。

修改test的数据加载，出现测试结果完全不匹配的现象，查找问题：

1. 从json文件load字典，不是原来的default_dict,这里需要进行转换
```python
def defaultdict_from_dict(d):
    nd = lambda: defaultdict(int)
    ni = nd()
    ni.update(d)
    return ni
```

很难思考这个问题，同样的模型，同样的词表怎么会出现这样的情况。从代码逻辑和实现上来分析，都没有问题。要找到这个原因只有查看以前的Test方式，深入其内部原理仔细分析了。



## step5: 为每一个multi_head_attention block添加FFNN

