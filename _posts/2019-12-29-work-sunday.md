---
layout:     post
title:      work sunday
subtitle:   
date:       2019-12-29
author:     RJ
header-img: 
catalog: true
tags:
    - Job


---
<p id = "build"></p>
---

## 前言
周五遇到Test完全不匹配的问题，调试发现，pny和han的字典映射完全无误。训练过程的acc和loss都处于正常范围。

出现问题的原因在于，我修改了数据的加载方式，改为字典映射。不再是以前的拼音汉字对应生成List，使用.index取得其id编号。按理说这种影响是不应该有巨大波动的。

## check1
 查看原來list，index()方式构建vocab映射，检查pny_list和han_list顺序构建的偶合程度：

 ![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20191229check.jpg)


发现id对应重叠的很小，排除此错误。

## check2
仔细分析Test结果，查看不同句子中，相同单词的id映射：
```
 the  7171 th example.
[2693, 796, 1739, 2787, 1319, 1538, 2753, 2626, 2148, 796, 1551, 1254, 2196, 2888]
原文汉字： ['向', '一', '家', '金', '融', '公', '司', '贷', '款', '一', '千', '多', '万', '元']
识别结果： 料训阴鑫尖沂树汇评训荡浙吉坎

 the  7172 th example.
[1538, 2753, 30, 1577, 2626, 2388, 156, 368, 1441, 507, 1538, 2753, 2072, 1139, 2433, 275]
原文汉字： ['公', '司', '承', '诺', '贷', '款', '本', '息', '都', '由', '公', '司', '负', '责', '偿', '还']
识别结果： 沂树廊妮汇仰冒规流少沂树甯凡刮含

 the  7173 th example.
[1538, 2753, 484, 502, 1475, 671, 1719, 2787, 904, 725]
原文汉字： ['公', '司', '却', '遇', '到', '了', '资', '金', '困', '难']
识别结果： 沂树篷喜玻茅又鑫窗澎
```
其中 “公司” 二字，都映射向  "沂树" 二字，这绝非偶然发生。

相同的数据，顺序测试第二次，结果又发生了改变，但公司二字却仍然对应： 暇红
```
 the  7171 th example.
原文汉字id: 444, 2108, 2719, 1769, 2672, 1153, 581, 1191, 596, 2108, 945, 1657, 1157, 1984
原文汉字： ['向', '一', '家', '金', '融', '公', '司', '贷', '款', '一', '千', '多', '万', '元']
识别结果id: [813, 1310, 1484, 1644, 655, 2829, 2375, 1143, 2065, 1310, 596, 2351, 1430, 2856]
识别结果汉字：： 层悬通哗威捧红膏闫悬款带蛋咖

 the  7172 th example.
原文汉字id: 1153, 581, 2215, 2425, 1191, 596, 1767, 2618, 2529, 2186, 1153, 581, 2736, 62, 2045, 2447
原文汉字： ['公', '司', '承', '诺', '贷', '款', '本', '息', '都', '由', '公', '司', '负', '责', '偿', '还']
识别结果id: [847, 2375, 1042, 1786, 1143, 2065, 2459, 975, 1554, 917, 847, 2375, 2438, 1536, 1064, 711]
识别结果汉字：： 暇红颖孚膏闫鹏璞秀课暇红伐装萤似

 the  7173 th example.
原文汉字id: 1153, 581, 940, 2973, 1581, 988, 1211, 1769, 49, 2828
原文汉字： ['公', '司', '却', '遇', '到', '了', '资', '金', '困', '难']
识别结果id: [847, 2375, 1715, 1002, 429, 1127, 2233, 1644, 1383, 43]
识别结果汉字：： 暇红衡傅想炫警哗锏拼
```

最终发现，Test调用load_data的时候，也重建了词表。

### 解决问题后，最终测试5个epoch，采用_position_ecoding结果：

```
 the  7175 th example.
原文汉字id: 621, 624, 1673, 1538, 1786, 1398, 523, 939, 394, 0, 600, 980, 80
原文汉字： ['这', '令', '被', '贷', '款', '的', '员', '工', '们', '寝', '食', '难', '安']
识别结果id: [621, 624, 1673, 2173, 1786, 1398, 523, 939, 394, 1373, 364, 1308, 80]
识别结果汉字：： 这令被代款的员工们角时男安
词错误率： 0.20199494105855964
```


训练5个epoch，每一个epoch得到：
```
batch  3582 : average loss =  2.092415084322216 average acc =  0.7557373287083924
batch  3582 : average loss =  1.7106554837441252 average acc =  0.8430870531558058
batch  3582 : average loss =  1.6238320626815714 average acc =  0.8755341079340937
batch  3582 : average loss =  1.568590122972311 average acc =  0.8943321122300255
batch  3582 : average loss =  1.5339341708915388 average acc =  0.9060538436440375
```

Test:
```
词错误率： 0.14045721376413878
```

### 原embedding的5个epoch结果：

```
batch  3582 : average loss =  2.05024021046683 average acc =  0.764345135698935
batch  3582 : average loss =  1.724920180482393 average acc =  0.837439579675398
batch  3582 : average loss =  1.6563430748249492 average acc =  0.8614693903550193
batch  3582 : average loss =  1.6109621875680915 average acc =  0.8780007405142584
batch  3582 : average loss =  1.5772555215303474 average acc =  0.8884211289453227
```

Test
```
词错误率： 0.172252183458216
```

对比实验发现，Transformer原 position_encoding效果更好。


## multi_head_attention
二者的实现方式相同，区别在于input原项目代码采用的是embedding作为输入，而transformer代码采用的是原input[ None, None]作为输入，其中的mask就有一定的区别，比如input输入需要tile一个embedding_size的维度。

## 接下来就是每个block接FFNN了
```
batch  3582 : average loss =  2.173763842630639 average acc =  0.7275526706606287
batch  3582 : average loss =  1.7754060251025041 average acc =  0.812886719529238
batch  3582 : average loss =  1.6961933201300103 average acc =  0.8431309958937048
batch  3582 : average loss =  1.6961933201300103 average acc =  0.8431309958937048
batch  3582 : average loss =  1.6370686905173009 average acc =  0.8637264239258754
```
看来，每个block都接FFNN好像效果差了一些，具体原因还需要进一步分析。

Test:
```
词错误率： 0.18039421562544744
```


## 将FFNN中的激活函数替换成gelu测试效果

```python
def gelu(input_tensor):
	cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
	return input_tesnsor*cdf

```