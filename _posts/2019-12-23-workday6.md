---
layout:     post
title:      work saturday
subtitle:   
date:       2019-12-23
author:     RJ
header-img: 
catalog: true
tags:
    - Job
---
<p id = "build"></p>
---

## 前言
使用清华的数据，完成了一个tiny baseline. 由于数据集较小，所以OOV严重，得到的测试结果的错误率为50%。今天尝试一下使用所有数据集，训练20个epoch看看效果有多大的提升。

由于是在source code上直接修改的代码，所以代码结构较为混乱，所以今天的任务是：
- 重构一下代码
- 参数保存对比
- 阿里论文

明天的任务是：
- 模型过拟合控制
- ckpt模型参数分析

## 重构代码



## 训练模型：

### error1
训练过程中出现错误，根本原因是源代码构建的han_vocab和pny_vocab不够robust，需要将其更改为defaultdict类型，然后使用这个更改后的类型，得到输入拼音和汉字的  id. 但是为什么在训练集构建vocab后，使用训练集to id 会出现OOV？，那一定是在构建vocab的时候忽略了' '，即空格字符。此问题出现的原因是数据集清洗的不够标准，数据集不是由汉字紧凑型构成。

解决：将输入的汉字，strip()一下。
```
  File "transformer_train.py", line 105, in <module>
    input_batch, label_batch = next(batch)
  File "../utils.py", line 128, in get_lm_batch
    [self.han2id(line, self.han_vocab) + [0] * (max_len - len(line)) for line in label_batch])
  File "../utils.py", line 128, in <listcomp>
    [self.han2id(line, self.han_vocab) + [0] * (max_len - len(line)) for line in label_batch])
  File "../utils.py", line 135, in han2id
    return [vocab.index(han) for han in line]
  File "../utils.py", line 135, in <listcomp>
    return [vocab.index(han) for han in line]
ValueError: ' ' is not in list

```

### error2
```
setting an array element with a sequence
```
输入的batch数据出现了问题，但是只用清华数据就没有问题！！！，说明数据的格式还存在问题。

经过调试发现：
```python
[[  58 1044  281  406  140  141  233   76   68    0    0    0    0]
 [ 180  142  956  339    3    3   11 1190  352  208  103 1080 1080]
 [ 246  757   32  635  434  279  930  103  260   32   44    0    0]
 [ 172  299  138  291  569  141  231  268  187   78  169   59    0]] 

[list([61, 2675, 552, 505, 151, 152, 2869, 81, 72, 0, 0, 0, 0])
 list([195, 153, 2450, 394, 1111, 1142, 11, 3951, 3951, 414, 2441, 113, 2880, 2880])
 list([276, 1253, 109, 62, 557, 329, 1988, 113, 293, 109, 45, 0, 0])
 list([1743, 342, 148, 333, 797, 152, 254, 926, 203, 83, 183, 62, 0])]


setting an array element with a sequence.
[[  99    2  466  420  278   30  596   99   61  944   38  103 1075    0]
 [ 279    2  188  445  471  191  636 1190  180  740  267  493  135  169]
 [ 231  145  268   61  148  669  603  375   11  104  661  246    0    0]
 [ 295  127  466   82  339  567   33  299  499  141  136    0    0    0]] 

[list([109, 2, 599, 530, 318, 905, 2654, 109, 191, 2055, 38, 113, 2849, 0])
 list([329, 2, 204, 2189, 608, 207, 927, 3951, 3951, 195, 1207, 302, 645, 145, 183])
 list([254, 157, 303, 191, 427, 993, 867, 1199, 11, 526, 1329, 340, 0, 0])
 list([337, 137, 599, 88, 394, 1059, 1606, 342, 50, 152, 146, 0, 0, 0])]
```
**拼音长度和汉字长度不一致。**

对几个数据集进行分析：
```python
pny_lst = []
han_lst = []
with open(stcmd, 'r', encoding='utf8') as f:
    data = f.readlines()  
    for line in tqdm(data):
        wav_file, pny, han = line.split('\t')
        #self.wav_lst.append(wav_file)
        if len(pny.split(' '))!= len(han.strip()):
            print(len(pny.split(' ')),'------',pny)
            print(len(han.strip()),'------',han)
        pny_lst.append(pny.split(' '))
        han_lst.append(han.strip())
```
**最终发现stcmd.txt数据出现问题，原因是：  空白字符的处理不到位。**
```
14 ------ qun2 li3 huo2 dong4 liang4 zui4 jin4 you3 suo3 xia4 hua2 a  
12 ------ 群里活动量最近有所下滑啊 

12 ------ wo3 cheng2 can2 hai4 mei4 zi de yuan2 xiong1 ge1  
10 ------ 我成残害妹子的元凶咯 

18 ------ wei4 shen2 me zhong4 dian3 yi1 yuan4 xiao1 liang4 quan2 bu4 dou1 zai4 xia4 hua2 ne  
16 ------ 为什么重点医院销量全部都在下滑呢 

11 ------ bu4   yun3 xu3 ren4 he2 ren2 qu4 po4 huai4
10 ------ 不 允许任何人去破坏
```

**处理好了拼音的问题，又遇到了汉字空白的问题。**
```
11 ------ ['ba3', 'jie2', 'wa2', 'er2', 'he2', 'er4', 'ya1', 'de', 'qq', 'gei3', 'wo3']
12 ------ 把洁娃儿和二丫的qq给我

10 ------ ['zu3', 'zhang3', 'qing3', 'kan4', 'huang2', 'hua2', 'zu1', 'fang2', 'he2', 'tong2']
11 ------ 组 长请看黄骅租房合同

100%|██████████| 102600/102600 [00:00<00:00, 126052.73it/s][A
12 ------ ['cha1', 'er3', 'ji1', 'kan4', 'dian4', 'shi4', 'mo2', 'ya2', 'hai2', 'ba1', 'ji1', 'zui3']
13 ------ 插耳机看电视 磨牙还吧唧嘴
```
修改代码之后，还遭遇了qq 恶魔：
```
11 ------ ['ba3', 'jie2', 'wa2', 'er2', 'he2', 'er4', 'ya1', 'de', 'qq', 'gei3', 'wo3']
12 ------ ['把', '洁', '娃', '儿', '和', '二', '丫', '的', 'q', 'q', '给', '我']
```
遇到这种情况，怕只有try,catch了。



## CTC介绍
[ctc-explained](https://xiaodu.io/ctc-explained/)


## Tensorflow保存恢复模型及微调

[Tensorflow保存恢复模型及微调](https://zhuanlan.zhihu.com/p/53814653)

- meta:.meta 文件用于保存网络结构，且以 protocol buffer 格式进行保存。protocol buffer是Google 公司内部使用的一种轻便高效的数据描述语言。类似于XML能够将结构化数据序列化，protocol buffer也可用于序列化结构化数据，并将其用于数据存储、通信协议等方面。相较于XML，protocol buffer更小、更快、也更简单。

- data: tensorflow 0.11之后，将ckpt文件拆分为了.data-00000-of-00001 和 .index 两个文件。.ckpt是二进制文件，保存了所有变量的值及变量的名称。拆分后的.data-00000-of-00001 保存的是变量值。

- index: .index文件保存的是.data文件中数据和 .meta文件中结构图之间的对应关系（也就是变量的名称）。

### 保存模型

```python
import tensorflow as tf
from six.moves import xrange
import os

w1 = tf.Variable(tf.random_normal(shape=[2]), name='w11')#变量w1在内存中的名字是w11；恢复变量时应该与name的名字保持一致
w2 = tf.Variable(tf.random_normal(shape=[5]), name='w22')
w3 = tf.Variable(tf.random_normal(shape=[5]), name='w33')

#保存一部分变量[w1,w2];只保存最近的5个模型文件;每2小时保存一次模型
saver = tf.train.Saver([w1, w2],max_to_keep=5, keep_checkpoint_every_n_hours=2)
save_path = './checkpoint_dir/MyModel'#定义模型保存的路径./checkpoint_dir/及模型名称MyModel

# Launch the graph and train, saving the model every 1,000 steps.
with tf.Session() as sess:
	sess.run(tf.global_variables_initializer())
	for step in xrange(100):
		if step % 10 == 0:
			# 每隔step=10步保存一次模型（ keep_checkpoint_every_n_hours与global_step可同时使用，表示'与'，通常任选一个就够了）；
			#每次会在保存的模型名称后面加上global_step的值作为后缀
			# write_meta_graph=False表示不保存图
			saver.save(sess, save_path, global_step=step, write_meta_graph=False)
			# 如果模型文件中没有保存网络图，则使用如下语句保存一张网络图（由于网络图不变，只保存一次就行）
			if not os.path.exists('./checkpoint_dir/MyModel.meta'):
				# saver.export_meta_graph(filename=None, collection_list=None,as_text=False,export_scope=None,clear_devices=False)
				# saver.export_meta_graph()仅仅保存网络图；参数filename表示网络图保存的路径即网络图名称
				saver.export_meta_graph('./checkpoint_dir/MyModel.meta')#定义网络图保存的路径./checkpoint_dir/及网络图名称MyModel.meta
                                #注意：tf.train.export_meta_graph()等价于tf.train.Saver.export_meta_graph()
```

### 恢复模型

```python
#首先恢复graph
saver = tf.train.import_meta_graph('./checkpoint_dir/MyModel.meta')
#恢复参数有两种方式，如下：
with tf.Session() as sess:
    #恢复最新保存的权重
    saver.restore(sess, tf.train.latest_checkpoint('./checkpoint_dir'))
    #指定一个权重恢复
    saver.restore(sess, './checkpoint_dir/MyModel-50')#注意不要加文件后缀名。若权重保存为.ckpt则需要加上后缀
```

## 权重读取
老版本ckpt类型：
```python
import tensorflow as tf
import numpy as np
reader = tf.train.NewCheckpointReader('llw/MNIST_model/mnist_model-29001')
all_variables = reader.get_variable_to_shape_map()
w1 = reader.get_tensor("layer1/weights")
print(type(w1))
print(w1.shape)
print(w1[0])  
```

新版本方法：
```python

checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)
graph = tf.Graph()
with graph.as_default():
    session_conf = tf.ConfigProto(
      allow_soft_placement=FLAGS.allow_soft_placement,
      log_device_placement=FLAGS.log_device_placement)
    sess = tf.Session(config=session_conf)
    with sess.as_default():
        #Load the saved meta graph and restore variables
        saver = tf.train.import_meta_graph("{}.meta".format(checkpoint_file))
        saver.restore(sess, checkpoint_file)


        #Get the placeholders from the graph by name
        w = graph.get_operation_by_name("word_embedding/W").outputs[0]
        print sess.run(w)





#模型中 关于W的定义：

self.W = tf.Variable(10.0, name = "W")


In TensorFlow, trained weights are represented by tf.Variable objects. 

If you created a tf.Variable—e.g. called v—yourself, you can get its value as a NumPy array by calling sess.run(v) (where sess is a tf.Session).

If you do not currently have a pointer to the tf.Variable, you can get a list of the trainable variables in the current graph by calling tf.trainable_variables(). This function returns a list of all trainable tf.Variable objects in the current graph, and you can select the one that you want by matching the v.name property. For example:

#Desired variable is called "tower_2/filter:0".
var = [v for v in tf.trainable_variables() if v.name == "tower_2/filter:0"][0]

```

