---
layout:     post
title:      LDA
subtitle:   
date:       2020-4-15
author:     RJ
header-img: 
catalog: true
tags:
    - nlp

---
<p id = "build"></p>
---



## Familia --A Toolkit for Industrial Topic Modeling

Familia 开源项目包含文档主题推断工具、语义匹配计算工具以及基于工业级语料训练的三种主题模型：
- Latent Dirichlet Allocation(LDA)
- SentenceLDA 
- Topical Word Embedding(TWE)

 支持用户以“拿来即用”的方式进行文本分类、文本聚类、个性化推荐等多种场景的调研和应用。考虑到主题模型训练成本较高以及开源主题模型资源有限的现状，我们会陆续开放基于工业级语料训练的多个垂直领域的主题模型，以及这些模型在工业界的典型应用方式，助力主题模型技术的科研和落地。


[2003 Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)

Abstract

We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of
discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each
item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in
turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of
text modeling, the topic probabilities provide an explicit representation of a document. We present
efficient approximate inference techniques based on variational methods and an EM algorithm for
empirical Bayes parameter estimation. We report results in document modeling, text classification,
and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI
model.

While the tf-idf reduction has some appealing features—notably in its basic identification of sets
of words that are discriminative for documents in the collection—the approach also provides a relatively small amount of reduction in description length and reveals little in the way of inter- or intradocument statistical structure. To address these shortcomings, IR researchers have proposed several
other dimensionality reduction techniques, most notably latent semantic indexing (LSI) (Deerwester
et al., 1990). 

**LSI uses a singular value decomposition of the X matrix to identify a linear subspace in the space of tf-idf features that captures most of the variance in the collection.**

This approach can achieve significant compression in large collections. Furthermore, Deerwester et al. argue that the derived features of LSI, which are linear combinations of the original tf-idf features, can capture
some aspects of basic linguistic notions such as synonymy and polysemy.


[topic model (LSA、PLSA、LDA)](https://blog.csdn.net/lmm6895071/article/details/74999129)
LFM、LSI、PLSI、LDA都是隐含语义分析技术，是同一类概念；在本质上是相通的，都是找出潜在的主题或特征。这些技术首先在文本挖掘领域中被提出来，近些年也被不断应用到其他领域中，并得到了不错的应用效果。
在推荐系统中它能够基于用户的行为对item进行自动聚类，也就是把item划分到不同类别/主题，这些主题/类别可以理解为用户的兴趣。对文本信息进行隐含主题发掘以提取必要特征，譬如LDA获得主题分布之后，可以实现对文档的降维。在论文推荐领域，次LDA+PMF模型实现协同主题回归模型（CTR)。