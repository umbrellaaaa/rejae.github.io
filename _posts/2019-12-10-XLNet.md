---
layout:     post
title:      XLNet
subtitle:   
date:       2019-12-10
author:     RJ
header-img: 
catalog: true
tags:
    - NLP
---
<p id = "build"></p>
---

<h2>XLNet: Generalized Autoregressive Pretrainingfor Language Understanding</h2>

## 前言 
此blog在于理解总结预训练语言模型相关内容，主要记录XLNet, 但也涉及： Bert,XLNet,Transformer,Transformer-XL等内容。


## 摘要

与基于自回归语言模型的预训练方法相比，Bert等基于自编码的预训练方法具有更好的双向上下文建模能力。但是，BERT依赖于使用mask破坏输入，因此忽略了mask位置之间的依赖性，并且存在预训练-微调差异。鉴于这些优点和缺点，我们提出了XLNet，这是一种广义的自回归预训练方法，该方法
- （1）通过最大化所有对象的预期可能性来实现双向上下文的学习。
- （2）克服了BERT的局限性，这归功于它的自回归公式。

此外，XLNet将来自最先进的自回归模型Transformer-XL的思想整合到预训练中。 从经验上讲，XLNet在20个任务上通常比BERT表现出色，而且在包括答疑，自然语言推理，情感分析和文档排名在内的18项任务上取得了最新的成果。

## 1导言
无监督表示学习在自然语言处理领域非常成功。通常，这些方法首先会大规模预训练神经网络未标记的文本语料库，然后微调下游任务的模型或表示。 在这种共同的高级观念下，文献中探索了不同的无监督预训练目标。 

其中，自回归（AR）语言建模和自编码（AE）是最成功的两个预训练目标。AR语言建模旨在使用自动回归模型来估计文本语料库的概率分布。
- AR语言建模将可能性分解为前向或后项乘积。训练参数模型（例如神经网络）以对每个条件分布进行建模。 由于AR语言模型仅受过训练以编码单向上下文（向前或向后），因此在建模深度双向上下文时无效。 相反，下游语言理解任务通常需要双向上下文信息。 这导致AR语言建模与有效的预训练之间存在差距。
- AE的预训练不执行显式密度估计，而是旨在从损坏的输入中重建原始数据。 BERT是一个著名的例子，它是最先进的预训练方法。给定输入token序列，将token的某些部分替换为特殊符号[MASK]，并训练模型以从损坏的版本中恢复原始token。由于密度估计不是目标的一部分，因此允许BERT利用双向上下文进行重建。作为直接好处，这消除了AR语言建模中的上述双向信息鸿沟，从而提高了性能。但是，在微调期间，真实数据中缺少BERT在预训练期间使用的人工符号，例如[MASK]，导致预训练与预调整之间存在差异。此外，由于预测的token在输入中被屏蔽，因此BERT无法像AR语言建模那样使用乘积规则对联合概率进行建模。换句话说，给定未屏蔽的token，BERT假定预测的token彼此独立，这被简化为自然语言中普遍存在的高阶，长距离依赖性。

面对现有语言预训练目标的利弊，在这项工作中，我们提出XLNet，这是一种通用的自动回归方法，它充分利用了AR语言建模和AE的优点，同时避免了它们的局限性。

- 首先，XLNet代替了常规AR模型中使用的固定前向或后向分解顺序，而使XLNet最大化了序列w.r.t的预期对数可能性。 分解阶数的所有可能排列。 多亏了permutation操作，每个位置的上下文都可以由左右两个标记组成。 期望地，每个位置学会从所有位置利用上下文信息，即捕获双向上下文。

- 其次，作为通用的AR语言模型，XLNet不依赖数据损坏。 因此，XLNet不会遭受BERT所受的预训练-微调差异的困扰。 同时，自回归目标还提供了一种自然的方式，可以使用乘积规则将预测token的联合概率分解为因数，从而消除了BERT中的独立性假设。

除了一个新的预训练目标，XLNet改进了预训练的架构设计。
- 受AR语言建模最新进展的启发，XLNet将分段递归机制和Transformer-XL的相对编码方案集成到预训练中，从而从经验上提高了性能，尤其是对于涉及较长文本序列的任务。
- 仅仅将Transformer（-XL）体系结构应用于基于permutation的语言建模不起作用，因为分解顺序是任意的，并且目标是模棱两可的。 作为解决方案，我们建议重新配置Transformer（-XL）网络以消除歧义。

根据经验，XLNet在18项任务上获得了最先进的结果，即7项GLUE语言理解任务，3项阅读理解任务（包括SQuAD和RACE），7项文本分类任务（包括Yelp和IMDB）以及ClueWeb09-B文档排名任务 。 通过一系列公平的比较实验，XLNet在多个基准测试中始终优于BERT。

**相关工作** 

已经探索了基于permutation的AR建模的思想，但是存在几个关键差异。 先前的模型是无序的，而XLNet本质上是使用位置编码的顺序感知。 这对于语言理解很重要，因为无序模型缺乏基本的表达能力，简直就是一句话。 上述差异是由动机上的根本差异引起的。以前的模型旨在通过在模型中添加“无序”归纳偏差来改善密度估计，而XLNet是通过使AR语言模型能够学习双向上下文来激发的。








## Glue
通用语言理解评价 (GLUE General Language Understanding Evaluation) 基准（Wang et al.(2018)）是对多种自然语言理解任务的集合。大多数 GLUE 数据集已经存在多年，但 GLUE 的用途是：

1. 以分离的训练集、验证集和测试集的标准形式发布这些数据集
2. 建立一个评估服务器来缓解评估不一致和过度拟合测试集的问题

GLUE 不发布测试集的标签，用户必须将他们的预测上传到GLUE 服务器进行评估，并对提交的数量进行限制。
GLUE 基准包括以下数据集，其描述最初在 Wang et al.(2018)中总结:

- MNLI 多类型的自然语言推理（Multi-Genre Natural Language Inference）是一项大规模的、众包的蕴
含分类任务（Williams et al.， 2018）。给定一对句子，目的是预测第二个句子相对于第一个句子是暗
含的、矛盾的还是中立的关系。
- QQP Quora问题对（Quora Question Pairs）是一个二元分类任务，目的是确定两个问题在Quora上问
的语义是否相等 （Chen et al., 2018）。
- QNLI 问题自然语言推理（Question Natural Language Inference）是斯坦福问题回答数据集
（Rajpurkar et al., 2016）已经转换为二进制分类任务的一个版本 Wang et al.(2018)。正类的例子是
（问题，句子）对，句子中包含正确的答案，和负类的例子是来自同一段的（问题，句子）对，句子
中不包含正确的答案。
- SST-2 斯坦福情感语义树（Stanford Sentiment Treebank）数据集是一个二元单句分类任务，数据由
电影评论中提取的句子组成，并对由人工对这些句子进行标注（Socher et al., 2013）。
- CoLA 语言可接受性单句二元分类任务语料库（Corpus of Linguistic Acceptability），它的目的是预测
一个英语句子在语言学上是否 “可接受”（Warstadt et al., 2018）。
- STS-B 文本语义相似度基准（Semantic Textual Similarity Bench-mark ）是从新闻标题中和其它来源
里提取的句子对的集合（Cer et al., 2017）。他们用从 1 到 5 的分数标注，表示这两个句子在语义上
是多么相似。
- MRPC 微软研究释义语料库（Microsoft Research Paraphrase Corpus）从在线新闻中自动提取的句子
对组成，并用人工注解来说明这两个句子在语义上是否相等（Dolan and Brockett, 2005.）。
- RTE 识别文本蕴含（Recognizing Textual Entailment）是一个与 MNLI 相似的二元蕴含任务，只是
RTE 的训练数据更少 Bentivogli et al., 2009。
- WNLI 威诺格拉德自然语言推理（Winograd NLI）是一个来自（Levesque et al., 2011) ）的小型自然
语言推理数据集。GLUE网页提示到这个数据集的构造存在问题，每一个被提交给 GLUE 的经过训练
的系统在预测多数类时都低于 65.1 这个基线准确度。因此，出于对 OpenAI GPT 的公平考虑，我们排
除了这一数据集。对于我们的 GLUE 提交，我们总是预测多数类。