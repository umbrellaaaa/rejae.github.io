---
layout:     post
title:      workday28
subtitle:   
date:       2020-2-11
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 今日工作内容
1. 使用wikidata训练word2vec模型
2. 使用word2vec模型，计算origin和corr词间相似度，统计纠错率。

## 纠错率计算
之前数据预处理，得到12484组(origin,corr)数据:
```
国主权 国主陷
产品 成品
将 将
付款 付款
季报 季报
上限为 金限为
主意 主意
任何 任何
失败 失败
大奖 大奖
情妇 行复
部品 药品
总额 总国
妥协 土鞋
圆 也
第 地
澡 走
三轴 三周
呈阳 沈阳
...
...
...
```


对这分词后的数据使用wikidata训练的word2vec模型进行相似度计算，取相似度大于50%的作为正确的纠错，得到2933条数据。


```PYTHON

import logging
 
from gensim import models
from gensim.models import word2vec


def compute_similarity():
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    sentences = word2vec.LineSentence("compute_similarity.txt")
#     model = word2vec.Word2Vec(sentences, size=250)
 
#     # 保存模型，供以后使用
#     model.save("word2vec.model")
 
    # 模型读取
    # model = word2vec.Word2Vec.load("your_model_name")
 
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    model = word2vec.Word2Vec.load("wiki_corpus.model")
    
    #for origin_item,corr_item in zip(list_origin,list_corr):
    with open('compute_similarity.txt','r',encoding='utf-8') as f:
        data = f.readlines()
        count = 0
        for item in data:
            try:
                origin,corr = item.rstrip().split(' ')
                #print('origin=',origin,'corr=', corr)
                

                res = model.similarity(origin,corr)
                if res>0.5:
                    count = count+1
                    print(origin,corr,'similarity=',res)
            except Exception as e:
                continue
                
                
    print('final count:',count)
```

```
2020-02-11 10:51:18,018 : INFO : loading Word2Vec object from wiki_corpus.model
2020-02-11 10:51:19,267 : INFO : loading wv recursively from wiki_corpus.model.wv.* with mmap=None
2020-02-11 10:51:19,267 : INFO : loading vectors from wiki_corpus.model.wv.vectors.npy with mmap=None
2020-02-11 10:51:19,535 : INFO : setting ignored attribute vectors_norm to None
2020-02-11 10:51:19,536 : INFO : loading vocabulary recursively from wiki_corpus.model.vocabulary.* with mmap=None
2020-02-11 10:51:19,536 : INFO : loading trainables recursively from wiki_corpus.model.trainables.* with mmap=None
2020-02-11 10:51:19,536 : INFO : loading syn1neg from wiki_corpus.model.trainables.syn1neg.npy with mmap=None
2020-02-11 10:51:19,800 : INFO : setting ignored attribute cum_table to None
2020-02-11 10:51:19,800 : INFO : loaded wiki_corpus.model
产品 成品 similarity= 0.5443562
付款 付款 similarity= 1.0
季报 季报 similarity= 1.0
主意 主意 similarity= 1.0
失败 失败 similarity= 1.0
大奖 大奖 similarity= 0.99999994
房地产 房地产 similarity= 0.99999994
功夫 工夫 similarity= 0.534743
花费 花费 similarity= 1.0
游玩 游玩 similarity= 0.99999994
....
....
```

由于存在分词后的数据不在vocabulary中的情况，我们直接采取字符相等操作，即取相似度0,1来规避这种情况。得到3384条数据。

加上代码
```
                if origin == corr:
                    count = count+1
                    print(origin,corr,'similarity=',1)
                    continue
```


这样我们在12484组数据中，正确的纠错为3384，即纠错率为：27.1%



