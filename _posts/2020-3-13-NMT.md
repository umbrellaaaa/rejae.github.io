---
layout:     post
title:      NMT
subtitle:   
date:       2020-3-13
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 前言
之前NMT一直很火的，苦于没有设备，很多技术都没法尝试，望而却步。

现在公司实习，老师要用MASS模型替换小牛翻译，努力加油吧，怼翻小牛。

## MASS
[MASS paper](https://arxiv.org/pdf/1905.02450.pdf)

### MASS模型下载安装测试

Prerequisites
After download the repository, you need to install fairseq by pip:

pip install fairseq==0.7.1


Data Ready
We first prepare the monolingual and bilingual sentences for Chinese and English respectively. The data directory looks like:

- data/
  ├─ mono/
  |  ├─ train.en
  |  ├─ train.zh
  |  ├─ valid.en
  |  ├─ valid.zh
  |  ├─ dict.en.txt
  |  └─ dict.zh.txt
  └─ para/
     ├─ train.en
     ├─ train.zh
     ├─ valid.en
     ├─ valid.zh
     ├─ dict.en.txt
     └─ dict.zh.txt

The files under mono are monolingual data, while under para are bilingual data. dict.en(zh).txt in different directory should be identical. The dictionary for different language can be different. Running the following command can generate the binarized data:

```python
#Ensure the output directory exists
data_dir=data/
mono_data_dir=$data_dir/mono/
para_data_dir=$data_dir/para/
save_dir=$data_dir/processed/

#set this relative path of MASS in your server
user_dir=mass

mkdir -p $data_dir $save_dir $mono_data_dir $para_data_dir


#Generate Monolingual Data
for lg in en zh
do

  fairseq-preprocess \
  --task cross_lingual_lm \
  --srcdict $mono_data_dir/dict.$lg.txt \
  --only-source \
  --trainpref $mono_data_dir/train --validpref $mono_data_dir/valid \
  --destdir $save_dir \
  --workers 20 \
  --source-lang $lg

  #Since we only have a source language, the output file has a None for the target language. Remove this

  for stage in train valid
  do
    mv $save_dir/$stage.$lg-None.$lg.bin $save_dir/$stage.$lg.bin
    mv $save_dir/$stage.$lg-None.$lg.idx $save_dir/$stage.$lg.idx
  done
done

###Generate Bilingual Data
fairseq-preprocess \
  --user-dir $mass_dir \
  --task xmasked_seq2seq \
  --source-lang en --target-lang zh \
  --trainpref $para_data_dir/train --validpref $para_data_dir/valid \
  --destdir $save_dir \
  --srcdict $para_data_dir/dict.en.txt \
  --tgtdict $para_data_dir/dict.zh.txt
Pre-training
We provide a simple demo code to demonstrate how to deploy mass pre-training.

save_dir=checkpoints/mass/pre-training/
user_dir=mass
data_dir=data/processed/

mkdir -p $save_dir

fairseq-train $data_dir \
    --user-dir $user_dir \
    --save-dir $save_dir \
    --task xmasked_seq2seq \
    --source-langs en,zh \
    --target-langs en,zh \
    --langs en,zh \
    --arch xtransformer \
    --mass_steps en-en,zh-zh \
    --memt_steps en-zh,zh-en \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr-scheduler inverse_sqrt --lr 0.00005 --min-lr 1e-09 \
    --criterion label_smoothed_cross_entropy \
    --max-tokens 4096 \
    --dropout 0.1 --relu-dropout 0.1 --attention-dropout 0.1 \
    --max-update 100000 \
    --share-decoder-input-output-embed \
    --valid-lang-pairs en-zh \

```

We also provide a pre-training script which is used for our released model.


## Fine-tuning
After pre-training stage, we fine-tune the model on bilingual sentence pairs:

data_dir=data/processed
save_dir=checkpoints/mass/fine_tune/
user_dir=mass
model=checkpoint/mass/pre-training/checkpoint_last.pt # The path of pre-trained model

mkdir -p $save_dir

fairseq-train $data_dir \
    --user-dir $user_dir \
    --task xmasked_seq2seq \
    --source-langs zh --target-langs en \
    --langs en,zh \
    --arch xtransformer \
    --mt_steps zh-en \
    --save-dir $save_dir \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr-scheduler inverse_sqrt --lr-shrink 0.5 --lr 0.00005 --min-lr 1e-09 \
    --criterion label_smoothed_cross_entropy \
    --max-tokens 4096 \
    --max-update 100000 --max-epoch 50 \
    --dropout 0.1 --relu-dropout 0.1 --attention-dropout 0.1 \
    --share-decoder-input-output-embed \
    --valid-lang-pairs zh-en \
    --reload_checkpoint $model
We also provide a fine-tuning script which is used for our pre-trained model.

## Infer.sh：

基础文件准备：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20200313154432.png）

将模型置于 infer.sh 下

将下载的bpe和dict文件放在processed文件夹下

参考issue修改 xmasked_seq2seq.py  490 row:

```python
def max_positions(self):
    if not self.datasets or len(self.datasets) == 0:
        return (self.args.max_source_positions, self.args.max_target_positions)

    return OrderedDict([
        (key, (self.args.max_source_positions, self.args.max_target_positions))
        for key in next(iter(self.datasets.values())).datasets.keys()
    ])

def build_dataset_for_inference(self, src_tokens, src_lengths):
    return LanguagePairDataset(src_tokens, src_lengths, self.source_dictionary)
```

英文测试数据构建: src.en

infer.sh 脚本：

```python
data_dir=processed
user_dir=mass
input_file=src.en
model=zhen_mass_pre-training.pt

fairseq-interactive $data_dir \
        --input $input_file \
	--user-dir $user_dir \
	-s en -t zh \
	--langs en,zh \
	--source-langs en --target-langs zh \
	--mt_steps en-zh \
	--task xmasked_seq2seq \
	--path $model 
```

测试结果：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20200313160039.png)


### finetune文件准备
 [WMT2017](http://www.statmt.org/wmt17/translation-task.html#download)

mono：

Common Crawl 33G


para：

News Commentary v12	162MB

UN Parallel Corpus V1.0	3.6 GB

CWMT Corpus ？







## 问题解决

fairseq是现有比较完善的seq2seq库，由于是大公司出品，因此也写得较为完善，不论是代码还是文档

[如何使用fairseq复现Transformer NMT](http://www.linzehui.me/2019/01/28/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8fairseq%E5%A4%8D%E7%8E%B0Transformer%20NMT/)

[核心issue](https://github.com/microsoft/MASS/issues/43)


