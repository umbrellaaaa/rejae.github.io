---
layout:     post
title:      NMT
subtitle:   
date:       2020-3-13
author:     RJ
header-img: 
catalog: true
tags:
    - job

---
<p id = "build"></p>
---

## 前言
之前NMT一直很火的，苦于没有设备，很多技术都没法尝试，望而却步。

现在公司实习，老师要用MASS模型替换小牛翻译，努力加油吧。

## MASS
[MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/pdf/1905.02450.pdf)



## MASS 解读

[kaiyuan](https://www.zhihu.com/question/324019899/answer/709577373)

MASS的主要出发点是参考bert的预训练思路去做NLG任务，这一点在之前的XLM也有过探索。不同于XLM的是这里采用了联合encoder-attention-decoder的训练框架：
- Encoder： 输入为被随机mask掉连续部分token的句子，使用双向Transformer对其进行编码；这样处理的目的是可以使得encoder可以更好地捕获没有被mask掉词语信息用于后续decoder的预测；

- Decoder： 输入为与encoder同样的句子，但是mask掉的正好和encoder相反（后面试验直接将masked tokens删去保留unmasked tokens position embedding），使用attention机制去训练，但只预测encoder端被mask掉的词。

该操作可以迫使decoder预测的时候更依赖于source端的输入而不是前面预测出的token，同时减缓了传统seq2seq结构的exposure bias问题。对强依赖于seq2seq的生成任务而言，MASS的思路其实非常自然地就能出来，在几个实验结果上也都相当不错，比如直接拉高了机器翻译的SOTA。
## 无监督学习下的NMT
[ppt](https://zhuanlan.zhihu.com/p/63900399)

### MASS模型下载安装测试

Prerequisites
After download the repository, you need to install fairseq by pip:

pip install fairseq==0.7.1


Data Ready
We first prepare the monolingual and bilingual sentences for Chinese and English respectively. The data directory looks like:

- data/
  ├─ mono/
  |  ├─ train.en
  |  ├─ train.zh
  |  ├─ valid.en
  |  ├─ valid.zh
  |  ├─ dict.en.txt
  |  └─ dict.zh.txt
  └─ para/
     ├─ train.en
     ├─ train.zh
     ├─ valid.en
     ├─ valid.zh
     ├─ dict.en.txt
     └─ dict.zh.txt

The files under mono are monolingual data, while under para are bilingual data. dict.en(zh).txt in different directory should be identical. The dictionary for different language can be different. Running the following command can generate the binarized data:

```python
#Ensure the output directory exists
data_dir=data/
mono_data_dir=$data_dir/mono/
para_data_dir=$data_dir/para/
save_dir=$data_dir/processed/

#set this relative path of MASS in your server
user_dir=mass

mkdir -p $data_dir $save_dir $mono_data_dir $para_data_dir


#Generate Monolingual Data
for lg in en zh
do

  fairseq-preprocess \
  --task cross_lingual_lm \
  --srcdict $mono_data_dir/dict.$lg.txt \
  --only-source \
  --trainpref $mono_data_dir/train --validpref $mono_data_dir/valid \
  --destdir $save_dir \
  --workers 20 \
  --source-lang $lg

  #Since we only have a source language, the output file has a None for the target language. Remove this

  for stage in train valid
  do
    mv $save_dir/$stage.$lg-None.$lg.bin $save_dir/$stage.$lg.bin
    mv $save_dir/$stage.$lg-None.$lg.idx $save_dir/$stage.$lg.idx
  done
done

###Generate Bilingual Data
fairseq-preprocess \
  --user-dir $mass_dir \
  --task xmasked_seq2seq \
  --source-lang en --target-lang zh \
  --trainpref $para_data_dir/train --validpref $para_data_dir/valid \
  --destdir $save_dir \
  --srcdict $para_data_dir/dict.en.txt \
  --tgtdict $para_data_dir/dict.zh.txt
Pre-training
We provide a simple demo code to demonstrate how to deploy mass pre-training.

save_dir=checkpoints/mass/pre-training/
user_dir=mass
data_dir=data/processed/

mkdir -p $save_dir

fairseq-train $data_dir \
    --user-dir $user_dir \
    --save-dir $save_dir \
    --task xmasked_seq2seq \
    --source-langs en,zh \
    --target-langs en,zh \
    --langs en,zh \
    --arch xtransformer \
    --mass_steps en-en,zh-zh \
    --memt_steps en-zh,zh-en \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr-scheduler inverse_sqrt --lr 0.00005 --min-lr 1e-09 \
    --criterion label_smoothed_cross_entropy \
    --max-tokens 4096 \
    --dropout 0.1 --relu-dropout 0.1 --attention-dropout 0.1 \
    --max-update 100000 \
    --share-decoder-input-output-embed \
    --valid-lang-pairs en-zh \

```

We also provide a pre-training script which is used for our released model.


## 提供的Fine-tuning脚本
After pre-training stage, we fine-tune the model on bilingual sentence pairs:

data_dir=data/processed
save_dir=checkpoints/mass/fine_tune/
user_dir=mass
model=checkpoint/mass/pre-training/checkpoint_last.pt # The path of pre-trained model

mkdir -p $save_dir

fairseq-train $data_dir \
    --user-dir $user_dir \
    --task xmasked_seq2seq \
    --source-langs zh --target-langs en \
    --langs en,zh \
    --arch xtransformer \
    --mt_steps zh-en \
    --save-dir $save_dir \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr-scheduler inverse_sqrt --lr-shrink 0.5 --lr 0.00005 --min-lr 1e-09 \
    --criterion label_smoothed_cross_entropy \
    --max-tokens 4096 \
    --max-update 100000 --max-epoch 50 \
    --dropout 0.1 --relu-dropout 0.1 --attention-dropout 0.1 \
    --share-decoder-input-output-embed \
    --valid-lang-pairs zh-en \
    --reload_checkpoint $model
We also provide a fine-tuning script which is used for our pre-trained model.

## 推断脚本Infer.sh：

基础文件准备：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20200313154432.png)

将模型置于 infer.sh 下

将下载的bpe和dict文件放在processed文件夹下

参考issue修改 xmasked_seq2seq.py  490 row:

```python
def max_positions(self):
    if not self.datasets or len(self.datasets) == 0:
        return (self.args.max_source_positions, self.args.max_target_positions)

    return OrderedDict([
        (key, (self.args.max_source_positions, self.args.max_target_positions))
        for key in next(iter(self.datasets.values())).datasets.keys()
    ])

def build_dataset_for_inference(self, src_tokens, src_lengths):
    return LanguagePairDataset(src_tokens, src_lengths, self.source_dictionary)
```

英文测试数据构建: src.en

infer.sh 脚本：

```python
data_dir=processed
user_dir=mass
input_file=src.en
model=zhen_mass_pre-training.pt

fairseq-interactive $data_dir \
        --input $input_file \
	--user-dir $user_dir \
	-s en -t zh \
	--langs en,zh \
	--source-langs en --target-langs zh \
	--mt_steps en-zh \
	--task xmasked_seq2seq \
	--path $model 
```

测试结果：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20200313160039.png)










## 问题解决

fairseq是现有比较完善的seq2seq库，由于是大公司出品，因此也写得较为完善，不论是代码还是文档

[如何使用fairseq复现Transformer NMT](http://www.linzehui.me/2019/01/28/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8fairseq%E5%A4%8D%E7%8E%B0Transformer%20NMT/)

[核心issue](https://github.com/microsoft/MASS/issues/43)



## finetune

之前直接用现成的模型完成了推理，现在要做的是弄清楚怎样进行预训练，这里拿到的是 pt 文件，能否在这个的基础上进行预训练？

文件的准备：

 - bpe文件，应该是 wordpiece 的文件，对于英文是一个单词切开，对于汉字是分词
 - dict文件

 我们现在要准备这两种共4个文件。

 BPE (byte-pair-encoding):It may need to transfer the raw data into bpe data by your self. Using FastBPE and mosesdecoder.

 FastBPE： C++ implementation of Neural Machine Translation of Rare Words with Subword Units, with Python API.

 以上我们是针对sup-NMT, 但是与unsup-NMT的差异在哪里？

- Unsupervised NMT
 
nsupervised Neural Machine Translation just uses monolingual data to train the models. During MASS pre-training, the source and target languages are pre-trained in one model, with the corresponding langauge embeddings to differentiate the langauges. During MASS fine-tuning, back-translation is used to train the unsupervised models. Code is under MASS-unsupNMT.

- Supervised NMT

We also implement MASS on fairseq, in order to support the pre-training and fine-tuning for large scale supervised tasks, such as neural machine translation, text summarization.

Unsupervised pre-training usually works better in zero-resource or low-resource downstream tasks. 

However, in large scale supervised NMT, there are plenty of bilingual data, which brings challenges for conventional unsupervised pre-training. Therefore, we design new pre-training loss to support large scale supervised NMT. The code is under MASS-supNMT.

We extend the MASS to supervised setting where the supervised sentence pair (X, Y) is leveraged for pre-training. The sentence X is masked and feed into the encoder, and the decoder predicts the whole sentence Y. Some discret tokens in the decoder input are also masked, to encourage the decoder to extract more informaiton from the encoder side.

- 有监督：句子对训练  
- 无监督：只用单语

这里，sup-NMT提供的东西太少，要往unsup-NMT查看和学习

注意到unsup-NMT中的install_tools.sh文件,先安装工具
```
# data path
MAIN_PATH=$PWD
TOOLS_PATH=$PWD/tools
# tools
MOSES_DIR=$TOOLS_PATH/mosesdecoder
FASTBPE_DIR=$TOOLS_PATH/fastBPE
FASTBPE=$FASTBPE_DIR/fast
WMT16_SCRIPTS=$TOOLS_PATH/wmt16-scripts
```
其中的wmt16-scripts：
- We built translation models with Nematus ( https://www.github.com/rsennrich/nematus )
- We used BPE as subword segmentation to achieve open-vocabulary translation ( https://github.com/rsennrich/subword-nmt )
- We automatically back-translated in-domain monolingual data into the source language to create additional training data.
- More details about our system are available in the system description paper (see below for reference)






## Unsup-NMT
要想做sup-NMT, unsup-NMT看来是绕不过去了，毕竟这个是先有的东西，这个东西研究好了，才能很好的到sup-NMT中。

### get_data_nmt.sh
Data Ready

We use the same BPE codes and vocabulary with XLM. Here we take English-French as an example.

cd MASS

wget https://dl.fbaipublicfiles.com/XLM/codes_enfr
wget https://dl.fbaipublicfiles.com/XLM/vocab_enfr

./get-data-nmt.sh --src en --tgt fr --reload_codes codes_enfr --reload_vocab vocab_enfr

包含四个参数：src tgt reload_codes reload_vocab
 
查看sh文件:

- Data preprocessing configuration
- Read arguments
- Check parameters
- ************* *Initialize tools and data paths* *****************
- Download monolingual data
- Link monolingual validation and test data to parallel data


```sh

set -e


#
# Data preprocessing configuration
#
N_MONO=5000000  # number of monolingual sentences for each language
CODES=60000     # number of BPE codes
N_THREADS=16    # number of threads in data preprocessing


#
# Read arguments
#
POSITIONAL=()
while [[ $# -gt 0 ]]
do
key="$1"
case $key in
  --src)
    SRC="$2"; shift 2;;
  --tgt)
    TGT="$2"; shift 2;;
  --reload_codes)
    RELOAD_CODES="$2"; shift 2;;
  --reload_vocab)
    RELOAD_VOCAB="$2"; shift 2;;
  *)
  POSITIONAL+=("$1")
  shift
  ;;
esac
done
set -- "${POSITIONAL[@]}"


#
# Check parameters
#
if [ "$SRC" == "" ]; then echo "--src not provided"; exit; fi
if [ "$TGT" == "" ]; then echo "--tgt not provided"; exit; fi
if [ "$SRC" != "de" -a "$SRC" != "en" -a "$SRC" != "fr" -a "$SRC" != "ro" ]; then echo "unknown source language"; exit; fi
if [ "$TGT" != "de" -a "$TGT" != "en" -a "$TGT" != "fr" -a "$TGT" != "ro" ]; then echo "unknown target language"; exit; fi
if [ "$SRC" == "$TGT" ]; then echo "source and target cannot be identical"; exit; fi
if [ "$SRC" \> "$TGT" ]; then echo "please ensure SRC < TGT"; exit; fi
if [ "$RELOAD_CODES" != "" ] && [ ! -f "$RELOAD_CODES" ]; then echo "cannot locate BPE codes"; exit; fi
if [ "$RELOAD_VOCAB" != "" ] && [ ! -f "$RELOAD_VOCAB" ]; then echo "cannot locate vocabulary"; exit; fi
if [ "$RELOAD_CODES" == "" -a "$RELOAD_VOCAB" != "" -o "$RELOAD_CODES" != "" -a "$RELOAD_VOCAB" == "" ]; then echo "BPE codes should be provided if and only if vocabulary is also provided"; exit; fi

```


#### 处理数据
重点看Initialize tools and data paths：


- tokenize data
- reload BPE codes  
- learn BPE codes 
- apply BPE codes 
- reload full vocabulary 
- extract full vocabulary 
- extract source and target vocabulary 

```sh

#
# Initialize tools and data paths
#

# main paths
MAIN_PATH=$PWD
TOOLS_PATH=$PWD/tools
DATA_PATH=$PWD/data
MONO_PATH=$DATA_PATH/mono
PARA_PATH=$DATA_PATH/para
PROC_PATH=$DATA_PATH/processed/$SRC-$TGT

# create paths
mkdir -p $TOOLS_PATH
mkdir -p $DATA_PATH
mkdir -p $MONO_PATH
mkdir -p $PARA_PATH
mkdir -p $PROC_PATH

# moses
MOSES=$TOOLS_PATH/mosesdecoder
REPLACE_UNICODE_PUNCT=$MOSES/scripts/tokenizer/replace-unicode-punctuation.perl
NORM_PUNC=$MOSES/scripts/tokenizer/normalize-punctuation.perl
REM_NON_PRINT_CHAR=$MOSES/scripts/tokenizer/remove-non-printing-char.perl
TOKENIZER=$MOSES/scripts/tokenizer/tokenizer.perl
INPUT_FROM_SGM=$MOSES/scripts/ems/support/input-from-sgm.perl

# fastBPE
FASTBPE_DIR=$TOOLS_PATH/fastBPE
FASTBPE=$TOOLS_PATH/fastBPE/fast

# Sennrich's WMT16 scripts for Romanian preprocessing
WMT16_SCRIPTS=$TOOLS_PATH/wmt16-scripts
NORMALIZE_ROMANIAN=$WMT16_SCRIPTS/preprocess/normalise-romanian.py
REMOVE_DIACRITICS=$WMT16_SCRIPTS/preprocess/remove-diacritics.py

# raw and tokenized files
SRC_RAW=$MONO_PATH/$SRC/all.$SRC
TGT_RAW=$MONO_PATH/$TGT/all.$TGT
SRC_TOK=$SRC_RAW.tok
TGT_TOK=$TGT_RAW.tok

# BPE / vocab files
BPE_CODES=$PROC_PATH/codes
SRC_VOCAB=$PROC_PATH/vocab.$SRC
TGT_VOCAB=$PROC_PATH/vocab.$TGT
FULL_VOCAB=$PROC_PATH/vocab.$SRC-$TGT

# train / valid / test monolingual BPE data
SRC_TRAIN_BPE=$PROC_PATH/train.$SRC
TGT_TRAIN_BPE=$PROC_PATH/train.$TGT
SRC_VALID_BPE=$PROC_PATH/valid.$SRC
TGT_VALID_BPE=$PROC_PATH/valid.$TGT
SRC_TEST_BPE=$PROC_PATH/test.$SRC
TGT_TEST_BPE=$PROC_PATH/test.$TGT

# valid / test parallel BPE data
PARA_SRC_VALID_BPE=$PROC_PATH/valid.$SRC-$TGT.$SRC
PARA_TGT_VALID_BPE=$PROC_PATH/valid.$SRC-$TGT.$TGT
PARA_SRC_TEST_BPE=$PROC_PATH/test.$SRC-$TGT.$SRC
PARA_TGT_TEST_BPE=$PROC_PATH/test.$SRC-$TGT.$TGT

# valid / test file raw data
unset PARA_SRC_VALID PARA_TGT_VALID PARA_SRC_TEST PARA_TGT_TEST
if [ "$SRC" == "en" -a "$TGT" == "fr" ]; then
  PARA_SRC_VALID=$PARA_PATH/dev/newstest2013-ref.en
  PARA_TGT_VALID=$PARA_PATH/dev/newstest2013-ref.fr
  PARA_SRC_TEST=$PARA_PATH/dev/newstest2014-fren-ref.en
  PARA_TGT_TEST=$PARA_PATH/dev/newstest2014-fren-ref.fr
fi
if [ "$SRC" == "de" -a "$TGT" == "en" ]; then
  PARA_SRC_VALID=$PARA_PATH/dev/newstest2013-ref.de
  PARA_TGT_VALID=$PARA_PATH/dev/newstest2013-ref.en
  PARA_SRC_TEST=$PARA_PATH/dev/newstest2016-ende-ref.de
  PARA_TGT_TEST=$PARA_PATH/dev/newstest2016-deen-ref.en
  # PARA_SRC_TEST=$PARA_PATH/dev/newstest2014-deen-ref.de
  # PARA_TGT_TEST=$PARA_PATH/dev/newstest2014-deen-ref.en
fi
if [ "$SRC" == "en" -a "$TGT" == "ro" ]; then
  PARA_SRC_VALID=$PARA_PATH/dev/newsdev2016-roen-ref.en
  PARA_TGT_VALID=$PARA_PATH/dev/newsdev2016-enro-ref.ro
  PARA_SRC_TEST=$PARA_PATH/dev/newstest2016-roen-ref.en
  PARA_TGT_TEST=$PARA_PATH/dev/newstest2016-enro-ref.ro
fi

# install tools
./install-tools.sh


#
# Download monolingual data
#

cd $MONO_PATH

if [ "$SRC" == "de" -o "$TGT" == "de" ]; then
  echo "Downloading German monolingual data ..."
  mkdir -p $MONO_PATH/de
  cd $MONO_PATH/de
  wget -c http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2007.de.shuffled.gz
  wget -c http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2008.de.shuffled.gz

fi

if [ "$SRC" == "en" -o "$TGT" == "en" ]; then
  echo "Downloading English monolingual data ..."
  mkdir -p $MONO_PATH/en
  cd $MONO_PATH/en
  wget -c http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2007.en.shuffled.gz
  wget -c http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2008.en.shuffled.gz

fi


cd $MONO_PATH

# decompress monolingual data
for FILENAME in $SRC/news*gz $TGT/news*gz; do
  OUTPUT="${FILENAME::-3}"
  if [ ! -f "$OUTPUT" ]; then
    echo "Decompressing $FILENAME..."
    gunzip -k $FILENAME
  else
    echo "$OUTPUT already decompressed."
  fi
done

# concatenate monolingual data files
if ! [[ -f "$SRC_RAW" ]]; then
  echo "Concatenating $SRC monolingual data..."
  cat $(ls $SRC/news*$SRC* | grep -v gz) | head -n $N_MONO > $SRC_RAW
fi
if ! [[ -f "$TGT_RAW" ]]; then
  echo "Concatenating $TGT monolingual data..."
  cat $(ls $TGT/news*$TGT* | grep -v gz) | head -n $N_MONO > $TGT_RAW
fi
echo "$SRC monolingual data concatenated in: $SRC_RAW"
echo "$TGT monolingual data concatenated in: $TGT_RAW"

# # check number of lines
# if ! [[ "$(wc -l < $SRC_RAW)" -eq "$N_MONO" ]]; then echo "ERROR: Number of lines does not match! Be sure you have $N_MONO sentences in your $SRC monolingual data."; exit; fi
# if ! [[ "$(wc -l < $TGT_RAW)" -eq "$N_MONO" ]]; then echo "ERROR: Number of lines does not match! Be sure you have $N_MONO sentences in your $TGT monolingual data."; exit; fi

# preprocessing commands - special case for Romanian
if [ "$SRC" == "ro" ]; then
  SRC_PREPROCESSING="$REPLACE_UNICODE_PUNCT | $NORM_PUNC -l $SRC | $REM_NON_PRINT_CHAR | $NORMALIZE_ROMANIAN | $REMOVE_DIACRITICS | $TOKENIZER -l $SRC -no-escape -threads $N_THREADS"
else
  SRC_PREPROCESSING="$REPLACE_UNICODE_PUNCT | $NORM_PUNC -l $SRC | $REM_NON_PRINT_CHAR |                                            $TOKENIZER -l $SRC -no-escape -threads $N_THREADS"
fi
if [ "$TGT" == "ro" ]; then
  TGT_PREPROCESSING="$REPLACE_UNICODE_PUNCT | $NORM_PUNC -l $TGT | $REM_NON_PRINT_CHAR | $NORMALIZE_ROMANIAN | $REMOVE_DIACRITICS | $TOKENIZER -l $TGT -no-escape -threads $N_THREADS"
else
  TGT_PREPROCESSING="$REPLACE_UNICODE_PUNCT | $NORM_PUNC -l $TGT | $REM_NON_PRINT_CHAR |                                            $TOKENIZER -l $TGT -no-escape -threads $N_THREADS"
fi

# tokenize data
if ! [[ -f "$SRC_TOK" ]]; then
  echo "Tokenize $SRC monolingual data..."
  eval "cat $SRC_RAW | $SRC_PREPROCESSING > $SRC_TOK"
fi

if ! [[ -f "$TGT_TOK" ]]; then
  echo "Tokenize $TGT monolingual data..."
  eval "cat $TGT_RAW | $TGT_PREPROCESSING > $TGT_TOK"
fi
echo "$SRC monolingual data tokenized in: $SRC_TOK"
echo "$TGT monolingual data tokenized in: $TGT_TOK"

# reload BPE codes
cd $MAIN_PATH
if [ ! -f "$BPE_CODES" ] && [ -f "$RELOAD_CODES" ]; then
  echo "Reloading BPE codes from $RELOAD_CODES ..."
  cp $RELOAD_CODES $BPE_CODES
fi

# learn BPE codes
if [ ! -f "$BPE_CODES" ]; then
  echo "Learning BPE codes..."
  $FASTBPE learnbpe $CODES $SRC_TOK $TGT_TOK > $BPE_CODES
fi
echo "BPE learned in $BPE_CODES"

# apply BPE codes
if ! [[ -f "$SRC_TRAIN_BPE" ]]; then
  echo "Applying $SRC BPE codes..."
  $FASTBPE applybpe $SRC_TRAIN_BPE $SRC_TOK $BPE_CODES
fi
if ! [[ -f "$TGT_TRAIN_BPE" ]]; then
  echo "Applying $TGT BPE codes..."
  $FASTBPE applybpe $TGT_TRAIN_BPE $TGT_TOK $BPE_CODES
fi
echo "BPE codes applied to $SRC in: $SRC_TRAIN_BPE"
echo "BPE codes applied to $TGT in: $TGT_TRAIN_BPE"

# extract source and target vocabulary
if ! [[ -f "$SRC_VOCAB" && -f "$TGT_VOCAB" ]]; then
  echo "Extracting vocabulary..."
  $FASTBPE getvocab $SRC_TRAIN_BPE > $SRC_VOCAB
  $FASTBPE getvocab $TGT_TRAIN_BPE > $TGT_VOCAB
fi
echo "$SRC vocab in: $SRC_VOCAB"
echo "$TGT vocab in: $TGT_VOCAB"

# reload full vocabulary
cd $MAIN_PATH
if [ ! -f "$FULL_VOCAB" ] && [ -f "$RELOAD_VOCAB" ]; then
  echo "Reloading vocabulary from $RELOAD_VOCAB ..."
  cp $RELOAD_VOCAB $FULL_VOCAB
fi

# extract full vocabulary
if ! [[ -f "$FULL_VOCAB" ]]; then
  echo "Extracting vocabulary..."
  $FASTBPE getvocab $SRC_TRAIN_BPE $TGT_TRAIN_BPE > $FULL_VOCAB
fi
echo "Full vocab in: $FULL_VOCAB"

# binarize data
if ! [[ -f "$SRC_TRAIN_BPE.pth" ]]; then
  echo "Binarizing $SRC data..."
  $MAIN_PATH/preprocess.py $FULL_VOCAB $SRC_TRAIN_BPE
fi
if ! [[ -f "$TGT_TRAIN_BPE.pth" ]]; then
  echo "Binarizing $TGT data..."
  $MAIN_PATH/preprocess.py $FULL_VOCAB $TGT_TRAIN_BPE
fi
echo "$SRC binarized data in: $SRC_TRAIN_BPE.pth"
echo "$TGT binarized data in: $TGT_TRAIN_BPE.pth"

```

#### 评估部分：
```sh
#
# Download parallel data (for evaluation only)
#

cd $PARA_PATH

echo "Downloading parallel data..."
wget -c http://data.statmt.org/wmt18/translation-task/dev.tgz

echo "Extracting parallel data..."
tar -xzf dev.tgz

# check valid and test files are here
if ! [[ -f "$PARA_SRC_VALID.sgm" ]]; then echo "$PARA_SRC_VALID.sgm is not found!"; exit; fi
if ! [[ -f "$PARA_TGT_VALID.sgm" ]]; then echo "$PARA_TGT_VALID.sgm is not found!"; exit; fi
if ! [[ -f "$PARA_SRC_TEST.sgm" ]];  then echo "$PARA_SRC_TEST.sgm is not found!";  exit; fi
if ! [[ -f "$PARA_TGT_TEST.sgm" ]];  then echo "$PARA_TGT_TEST.sgm is not found!";  exit; fi

echo "Tokenizing valid and test data..."
eval "$INPUT_FROM_SGM < $PARA_SRC_VALID.sgm | $SRC_PREPROCESSING > $PARA_SRC_VALID"
eval "$INPUT_FROM_SGM < $PARA_TGT_VALID.sgm | $TGT_PREPROCESSING > $PARA_TGT_VALID"
eval "$INPUT_FROM_SGM < $PARA_SRC_TEST.sgm  | $SRC_PREPROCESSING > $PARA_SRC_TEST"
eval "$INPUT_FROM_SGM < $PARA_TGT_TEST.sgm  | $TGT_PREPROCESSING > $PARA_TGT_TEST"

echo "Applying BPE to valid and test files..."
$FASTBPE applybpe $PARA_SRC_VALID_BPE $PARA_SRC_VALID $BPE_CODES $SRC_VOCAB
$FASTBPE applybpe $PARA_TGT_VALID_BPE $PARA_TGT_VALID $BPE_CODES $TGT_VOCAB
$FASTBPE applybpe $PARA_SRC_TEST_BPE  $PARA_SRC_TEST  $BPE_CODES $SRC_VOCAB
$FASTBPE applybpe $PARA_TGT_TEST_BPE  $PARA_TGT_TEST  $BPE_CODES $TGT_VOCAB

echo "Binarizing data..."
rm -f $PARA_SRC_VALID_BPE.pth $PARA_TGT_VALID_BPE.pth $PARA_SRC_TEST_BPE.pth $PARA_TGT_TEST_BPE.pth
$MAIN_PATH/preprocess.py $FULL_VOCAB $PARA_SRC_VALID_BPE
$MAIN_PATH/preprocess.py $FULL_VOCAB $PARA_TGT_VALID_BPE
$MAIN_PATH/preprocess.py $FULL_VOCAB $PARA_SRC_TEST_BPE
$MAIN_PATH/preprocess.py $FULL_VOCAB $PARA_TGT_TEST_BPE


#
# Link monolingual validation and test data to parallel data
#
ln -sf $PARA_SRC_VALID_BPE.pth $SRC_VALID_BPE.pth
ln -sf $PARA_TGT_VALID_BPE.pth $TGT_VALID_BPE.pth
ln -sf $PARA_SRC_TEST_BPE.pth  $SRC_TEST_BPE.pth
ln -sf $PARA_TGT_TEST_BPE.pth  $TGT_TEST_BPE.pth


#
# Summary
#
echo ""
echo "===== Data summary"
echo "Monolingual training data:"
echo "    $SRC: $SRC_TRAIN_BPE.pth"
echo "    $TGT: $TGT_TRAIN_BPE.pth"
echo "Monolingual validation data:"
echo "    $SRC: $SRC_VALID_BPE.pth"
echo "    $TGT: $TGT_VALID_BPE.pth"
echo "Monolingual test data:"
echo "    $SRC: $SRC_TEST_BPE.pth"
echo "    $TGT: $TGT_TEST_BPE.pth"
echo "Parallel validation data:"
echo "    $SRC: $PARA_SRC_VALID_BPE.pth"
echo "    $TGT: $PARA_TGT_VALID_BPE.pth"
echo "Parallel test data:"
echo "    $SRC: $PARA_SRC_TEST_BPE.pth"
echo "    $TGT: $PARA_TGT_TEST_BPE.pth"
echo ""
```


## finetune文件准备
[WMT2017](http://www.statmt.org/wmt17/translation-task.html#download)

这里，我们是使用sup-NMT所以单语数据不需要

para：

News Commentary v12	162MB

我们刚开始只处理 News这个文件：

- tokenization
- build bpe 
- build dict

相关脚本命令：

```sh
# tokenize data
if ! [[ -f "$SRC_TOK" ]]; then
  echo "Tokenize $SRC monolingual data..."
  eval "cat $SRC_RAW | $SRC_PREPROCESSING > $SRC_TOK"
fi

if ! [[ -f "$TGT_TOK" ]]; then
  echo "Tokenize $TGT monolingual data..."
  eval "cat $TGT_RAW | $TGT_PREPROCESSING > $TGT_TOK"
fi
echo "$SRC monolingual data tokenized in: $SRC_TOK"
echo "$TGT monolingual data tokenized in: $TGT_TOK"

--------------------------------------------------------
```
注意到这里有两个processing:$SRC_PREPROCESSING and $TGT_PREPROCESSING

SRC_PREPROCESSING="$REPLACE_UNICODE_PUNCT | $NORM_PUNC -l $SRC | $REM_NON_PRINT_CHAR |                                            $TOKENIZER -l $SRC -no-escape -threads $N_THREADS"

TGT_PREPROCESSING="$REPLACE_UNICODE_PUNCT | $NORM_PUNC -l $TGT | $REM_NON_PRINT_CHAR | $NORMALIZE_ROMANIAN | $REMOVE_DIACRITICS | $TOKENIZER -l $TGT -no-escape -threads $N_THREADS"

SRC的处理方式，我们不需要改动，但是TGT，我们处理的是中文，这里处理的是罗马文字，所以可能需要改动。

我们看看其中的操作有哪些：
```sh
# moses
MOSES=$TOOLS_PATH/mosesdecoder
REPLACE_UNICODE_PUNCT=$MOSES/scripts/tokenizer/replace-unicode-punctuation.perl #替换标点符号
NORM_PUNC=$MOSES/scripts/tokenizer/normalize-punctuation.perl #标准化标点符号
REM_NON_PRINT_CHAR=$MOSES/scripts/tokenizer/remove-non-printing-char.perl #删除不可打印出的字符
TOKENIZER=$MOSES/scripts/tokenizer/tokenizer.perl #切分
INPUT_FROM_SGM=$MOSES/scripts/ems/support/input-from-sgm.perl
```


```sh
# reload BPE codes
cd $MAIN_PATH
if [ ! -f "$BPE_CODES" ] && [ -f "$RELOAD_CODES" ]; then
  echo "Reloading BPE codes from $RELOAD_CODES ..."
  cp $RELOAD_CODES $BPE_CODES
fi

# learn BPE codes
if [ ! -f "$BPE_CODES" ]; then
  echo "Learning BPE codes..."
  $FASTBPE learnbpe $CODES $SRC_TOK $TGT_TOK > $BPE_CODES
fi
echo "BPE learned in $BPE_CODES"

# apply BPE codes
if ! [[ -f "$SRC_TRAIN_BPE" ]]; then
  echo "Applying $SRC BPE codes..."
  $FASTBPE applybpe $SRC_TRAIN_BPE $SRC_TOK $BPE_CODES
fi
if ! [[ -f "$TGT_TRAIN_BPE" ]]; then
  echo "Applying $TGT BPE codes..."
  $FASTBPE applybpe $TGT_TRAIN_BPE $TGT_TOK $BPE_CODES
fi
echo "BPE codes applied to $SRC in: $SRC_TRAIN_BPE"
echo "BPE codes applied to $TGT in: $TGT_TRAIN_BPE"

# extract source and target vocabulary
if ! [[ -f "$SRC_VOCAB" && -f "$TGT_VOCAB" ]]; then
  echo "Extracting vocabulary..."
  $FASTBPE getvocab $SRC_TRAIN_BPE > $SRC_VOCAB
  $FASTBPE getvocab $TGT_TRAIN_BPE > $TGT_VOCAB
fi
echo "$SRC vocab in: $SRC_VOCAB"
echo "$TGT vocab in: $TGT_VOCAB"

# reload full vocabulary
cd $MAIN_PATH
if [ ! -f "$FULL_VOCAB" ] && [ -f "$RELOAD_VOCAB" ]; then
  echo "Reloading vocabulary from $RELOAD_VOCAB ..."
  cp $RELOAD_VOCAB $FULL_VOCAB
fi

# extract full vocabulary
if ! [[ -f "$FULL_VOCAB" ]]; then
  echo "Extracting vocabulary..."
  $FASTBPE getvocab $SRC_TRAIN_BPE $TGT_TRAIN_BPE > $FULL_VOCAB
fi
echo "Full vocab in: $FULL_VOCAB"

# binarize data
if ! [[ -f "$SRC_TRAIN_BPE.pth" ]]; then
  echo "Binarizing $SRC data..."
  $MAIN_PATH/preprocess.py $FULL_VOCAB $SRC_TRAIN_BPE
fi
if ! [[ -f "$TGT_TRAIN_BPE.pth" ]]; then
  echo "Binarizing $TGT data..."
  $MAIN_PATH/preprocess.py $FULL_VOCAB $TGT_TRAIN_BPE
fi
echo "$SRC binarized data in: $SRC_TRAIN_BPE.pth"
echo "$TGT binarized data in: $TGT_TRAIN_BPE.pth"
```

最终，其实不需要查看这些脚本，将数据按train,valid和dict共6个文件放在对应的mono和para目录下就可以执行脚本，完成数据的准备工作了。

这里我先用30w的train和5w的valid先试试finetune工作。

mono的处理如下：
```
| [en] Dictionary: 40000 types
| [en] data//mono//train.en: 300000 sents, 6016269 tokens, 18.8% replaced by <un                                                                                                 k>
| [en] Dictionary: 40000 types
| [en] data//mono//valid.en: 100000 sents, 2007722 tokens, 18.8% replaced by <un                                                                                                 k>
| [zh] Dictionary: 40000 types
| [zh] data//mono//train.zh: 103324 sents, 216782 tokens, 51.5% replaced by <unk                                                                                                 >
| [zh] Dictionary: 40000 types
| [zh] data//mono//valid.zh: 52061 sents, 108678 tokens, 51.6% replaced by <unk>
```
发现中文有太多的unk几乎占一半，所以，应该采用issue中给的建议，将中文按字切开丢进去试试。

```sh
| [en] Dictionary: 40000 types
| [en] data//mono//train.en: 300000 sents, 6016269 tokens, 18.8% replaced by <unk>
| [en] Dictionary: 40000 types
| [en] data//mono//valid.en: 100000 sents, 2007722 tokens, 18.8% replaced by <unk>
| Wrote preprocessed data to data//processed/

| [zh] Dictionary: 40000 types
| [zh] data//mono//train.zh: 103131 sents, 4125801 tokens, 6.38% replaced by <unk>
| [zh] Dictionary: 40000 types
| [zh] data//mono//valid.zh: 52555 sents, 2110306 tokens, 6.39% replaced by <unk>
```
果然，汉字的unk数目下降了。那英文呢？是不是需要处理一下？

我们将train和valid的en都使用10个句子。按流程处理下来：
```
| [en] Dictionary: 40000 types
| [en] data//mono//train.en: 10 sents, 200 tokens, 22.0% replaced by <unk>
| [en] Dictionary: 40000 types
| [en] data//mono//valid.en: 10 sents, 222 tokens, 17.6% replaced by <unk>
```
手动加上空格以后：
```
| [en] Dictionary: 40000 types
| [en] data//mono//train.en: 10 sents, 221 tokens, 14.5% replaced by <unk>
| [en] Dictionary: 40000 types
| [en] data//mono//valid.en: 10 sents, 244 tokens, 11.9% replaced by <unk>
```
总结一下，tokenize 英文单词和, 以及 . 

Lots of 的Lots 为 UNK. 如何解决

经过处理后，测试的文本对比如下：

![](https://raw.githubusercontent.com/rejae/rejae.github.io/master/img/20200317162220.png)

所以在执行generate_data.sh之前，需要对中文和英文数据进行处理，大致的中文直接对每个字切开，英文将标点符号切开。

但是这对模型有影响不是吗？ 如果中文用字切，那么中文字典的覆盖率应该是相当高的，但是给的40000大小的dict中，大部分是分词后的数据。

在英文的处理中，对于所有格形式， he's,  name's 处理的很不好，还有就是单复数形式也处理的很差，这些没有一个标准，如果贸然改动，完全不知道结果好不好，肯能需要花大量时间做实验进行对比。

所以还是先发issue问问作者吧。



## unk 问题
在上周五的测试中，出现了很多unk，首先，我们的英文句子，应该将表单符号和单词切开。

其次，大小写的问题，比如National Emergency，前一个能识别，但是后一个却是unk，所以考虑将除首字母以外的所有字作小写处理。

关于新词的问题，在NMT交流群中，有使用占位符，外部接入新词的做法，值得参考。

相关的文献：[Addressing the Rare Word Problem in Neural Machine Translation](https://nlp.stanford.edu/pubs/acl15_nmt.pdf)

source to target. then look up table and replace




## 机器翻译 bpe——bytes-pair-encoding

[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)









































## 数据集下载链接

http://www.statmt.org/wmt17/translation-task.html#download

-----------------------------------------

WMT17

Parallel data

News Commentary v12：  http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz


UN Parallel Corpus V1.0：

https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0.en-zh.tar.gz.00

https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0.en-zh.tar.gz.01

CWMT Corpus：
http://nlp.nju.edu.cn/cwmt-wmt/



Monolingual language model training data:


Common Crawl：  http://web-language-models.s3-website-us-east-1.amazonaws.com/ngrams/zh/deduped/zh.deduped.xz

WMT18

Parallel data

News Commentary v13： http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz

CWMT Corpus： http://mteval.cipsc.org.cn:81/agreement/wmt


Monolingual language model training data:

News Commentary ： http://data.statmt.org/wmt18/translation-task/news-commentary-v13.zh.gz


WMT19

Parallel data

News Commentary v14： http://data.statmt.org/news-commentary/v14

Wiki Titles v1： http://data.statmt.org/wikititles/v1

CWMT Corpus： http://mteval.cipsc.org.cn:81/agreement/wmt


Monolingual language model training data:

News crawl： http://data.statmt.org/news-crawl

News Commentary： http://data.statmt.org/news-commentary

common crawl : http://data.statmt.org/ngrams

WMT20

Parallel data

News Commentary v15: http://data.statmt.org/news-commentary/v15

Wiki Titles v2: http://data.statmt.org/wikititles/v2

CCMT Corpus:  http://mteval.cipsc.org.cn:81/agreement/description

WikiMatrix: http://data.statmt.org/wmt20/translation-task/WikiMatrix/

Back-translated news:  http://data.statmt.org/wmt20/translation-task/back-translation/

Monolingual training data:

News crawl:   http://data.statmt.org/news-crawl

News Commentary: http://data.statmt.org/news-commentary

Common Crawl: http://data.statmt.org/ngrams

## 数据预处理
[NLTK使用](https://zhangmingemma.github.io/2017/03/29/Python+NLTK-Natural-Language-Process.html)
```python
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer  
import re
def preprocess(file_path,out_path):
    with open(file_path,'r',encoding='utf-8') as f:
        with open(out_path,'w',encoding='utf-8') as f_out:
            data = f.read().splitlines() # paragraph
            stop_words = stopwords.words('english')
            comp = re.compile('[^-^,^.^$^%^A-Z^a-z^0-9^ ]')
            wordnet_lemmatizer = WordNetLemmatizer()  
            for item in data:
                if len(item)==0:
                    continue
                word_tokens = word_tokenize(item)
                sentence = ' '.join([it for it in word_tokens])
                sentence = comp.sub(' ', sentence)
                sentence = sentence.replace('-',' - ').replace(","," , ").replace('.',' . ')
                f_out.write(sentence+' \n')
file_path ="raw_corpus/"+ 'test.txt'
out_path = 'src.en.txt'
preprocess(file_path,out_path)
        
```


翻译结果：
```
北京—中国星期四报告说,在改变了统计方式后,一种新病毒的死亡和感染人数激增,进一步加剧了疫情蔓延至二十多个国家的恐慌.
在新病例数量下降两天后,疫情激增,对于那些急于了解疫情发展轨迹的人来说,这两天的情况并不清楚.
"我们需要的是随着时间的推移保持一定的一致性,以便让我们了解实际发生的情况,"美国东北大学传染病专家威廉博士说,"传播是否正在发生?"
尽管有官方报告,但答案仍然难以捉摸.
中国因所谓的"19-19"疾病而死亡的人数从前一天的254人上升到了1,000人,而确诊病例的数量上升到59,000人,增加了15,152人.
世界卫生组织(WorldHealthOrganizations)紧急事务主管迈克尔?莱恩(MichaelRyan)博士表示,病例大幅上升的原因是对病例的计算方法进行了修改,并警告称,这并不代表新感染病例的突然激增.
他说:"在过去24小时内,你所看到的这种增加主要是由于病例的诊断和报告方式,他指出,感染的激增是指那些可以追溯到几天或几周的患者,包括疾病爆发的开始."
中国国家卫生计生委表示,湖北省的官员在以武汉为灾区的湖北省开始使用较低的临床诊断标准来统计病例,这似乎是根据医生的分析和肺成像来统计病例的,而不是仅仅依靠实验室的化验结果.在新报告的感染病例中,超过13,000例是新的统计方法的结果.
中国卫生部发言人MiFeng说,这一变化的目的是为了查明患者感染肺炎的可疑病例,以便更快地进行治疗,减少更严重疾病或死亡的可能性.
爱丁堡大学传染病学教授马克说:"显然,在武汉,卫生系统面临着极大的压力,因此,第一要务必须是病人."
为了进一步了解为遏制病毒而进行的铤而走险的努力和公众对政府工作的批评,中国又更换了两名高级官员,即湖北的执政党党委书记和武汉的党委书记.
公众广泛批评地方官员未能迅速果断地应对新的病毒.当局起初向人们保证人与人之间传染的风险很小,但后来得到了证实.武汉居民说医院拥挤不堪,缺乏足够的医疗用品.警方指责早期尝试分享信息的医生散布谣言.
这是个与开放斗争的社会,"他说,"对于那些继续处于权威地位的人来说,改变他们的思想肯定是困难的,所以当你看到这些变化时,你必须有点怀疑."
英格兰东部大学(UniversityofEastBank)的健康保护教授保罗?亨特(PaulHunter)对修订后的数据表达了类似的不满,但他表示,他认为这并不代表疫情爆发的轨迹发生了变化.
他说:"我怀疑,但不能肯定,潜在的趋势依然存在,这几乎肯定不意味着疫情在一夜之间死灰复燃."
尽管如此,中国政府为遏制这一疾病做出了前所未有的努力,将受灾最严重的6,000多万人口聚居的城市置于了以下的控制之下:湖北某市某区实施了战时措施,居民甚至在两周内不得离开公寓.
地方政府通知居民,将分发基本必需品,并提供获得药品的帮助.
徐敏说,在她所在的地区,社区工作人员和安全人员正在守卫通往她的社区的入口.
她说:"这对我们的生活没有太大影响,只是我们不能出去."
中国是世界上99%以上的报告感染者的家乡,但这场危机已经导致其他国家实施旅行限制,并造成了远远超出预期的影响.
```

可见，翻译的结果是较好的。

the Westerdam 识别出来是 < unk >

亚洲锦标赛从4月18日至25日从哈萨克斯坦的××搬到了乌兹别克斯坦的××.


## OOV问题解决

查看参数设置，注意到： --replace_unk 参数

设置一个值，运行翻译，得到：
```
    with open(replace_unk, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'UNKNOWN'
```
应该是替换字典, 那我们将一些unk的单词，设个字典表试试：

今天主要做unk替换的工作，即OOV，新词替换问题。

在查看源码的时候，注意到参数：

在对这个参数设置的时候，尝试构建字典，解决新词问题，字典如下：


但是发现，并没有对COVID等字典中的词进行替换。深入源码，手动print对应的参数，发现：

原句子：The death toll in China from the disease known as COVID - 19 reached 1 , 367  ,  up 254 from a day earlier  ,  and the number of confirmed cases jumped to 59 , 804  ,  up 15 , 152  . 

模型处理后的句子：The death toll in China from the disease known as <unk> - 19 reached 1 , <unk> , up 254 from a day earlier , and the number of confirmed cases jumped to 59 , <unk> , up 15 , 152 .

预测的汉字：中国因所谓的"19-19"疾病而死亡的人数从前一天的254人上升到了1,000人,而确诊病例的数量上升到59,000人,增加了15,152人.

可以看到，COVID  ， 367， 804 这几个字符串都被处理成了 unk ， 数字在翻译的时候，用0来填充信息了。

尝试直接替换：
这是我们的字典数据：align_dict = utils.load_align_dict(args.replace_unk)
在后向处理方法中：
# Process top predictions
for hypo in hypos[:min(len(hypos), args.nbest)]:
    print('hypo type:', type(hypo), 'hypo:', hypo)

    hypo_tokens, hypo_str, alignment = utils.post_process_prediction(
        hypo_tokens=hypo['tokens'].int().cpu(),
        src_str=src_str,
        alignment=hypo['alignment'].int().cpu() if hypo['alignment'] is not None else None,
        align_dict=align_dict,
        tgt_dict=tgt_dict,
        remove_bpe=args.remove_bpe,
    )
注意到除了align_dict参数外，还有一个alignment参数。未设置的时候报错，尝试在infer.sh中加入参数，发现接收不了这个参数，直接到源码中修改这个参数：
    for model in models:
        model.make_generation_fast_(
            beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,
            need_attn=args.print_alignment, # 将其置为True
        )

这应该是NMT中使用attention对齐翻译句子中的词。程序能完整的执行，并且alignment的输出如下：
S-0     The death toll in China from the disease known as <unk> - 19 reached 1 , <unk> , up 254 from a day earlier , and the number of confirmed cases jumped to 59 , <unk> , up 15 , 152 .
type(hypos) <class 'list'> [{'tokens': tensor([   89,   475,  2128,     5,    11,   682,    52,   682,    11,  1213,
           54,   533,     5,   811,    67,  9365,     5, 23457,    34,  1177,
           39,    10,   108,     6,  1696,    34,     6,    54, 16219,  4450,
            5,   851,  1177,    39,  2756,     6,  1696,    34,     6,   220,
           10,   385,     6, 16810,    34,     7,     2], device='cuda:0'), 'score': -0.5140185356140137, 'attention': tensor([[0.0176, 0.0031, 0.0007,  ..., 0.0020, 0.0210, 0.0258],
        [0.0474, 0.0509, 0.0259,  ..., 0.0028, 0.0142, 0.0239],
        [0.0219, 0.0202, 0.0062,  ..., 0.0011, 0.0064, 0.0074],
        ...,
        [0.0066, 0.0019, 0.0009,  ..., 0.0567, 0.0563, 0.0347],
        [0.0072, 0.0041, 0.0032,  ..., 0.4043, 0.3086, 0.1366],
        [0.0658, 0.0506, 0.0411,  ..., 0.3544, 0.3422, 0.3414]],
       device='cuda:0'), 'alignment': tensor([10, 10, 10, 10, 10, 10, 42, 42, 42, 42, 42, 42, 42, 42, 42, 22, 19, 19,
        42, 42, 42, 14, 14, 42, 16, 42, 42, 42, 42, 42, 31, 42, 31, 42, 33, 42,
        42, 42, 41, 41, 41, 40, 40, 42, 41, 42, 42], device='cuda:0'), 'positional_scores': tensor([-7.8625e-01, -1.2459e+00, -1.7020e+00, -5.3779e-01, -3.3952e-01,
        -1.7217e+00, -1.5388e+00, -5.1001e-02, -1.3031e-02, -1.9808e+00,
        -7.0781e-01, -2.2359e-01, -3.8185e-02, -7.7656e-02, -1.2256e+00,
        -4.6812e-01, -3.6058e-01, -1.1854e-02, -8.3603e-02, -5.9268e-01,
        -2.2287e-01, -1.1333e+00, -2.5422e-01, -1.0380e-01, -7.0995e-01,
        -6.1804e-02, -9.3351e-02, -1.0434e+00, -1.7457e-01, -4.7542e-01,
        -1.1062e+00, -3.8883e-01, -2.4622e+00, -6.0059e-02, -1.5048e-01,
        -4.8230e-01, -2.6030e-01, -4.6995e-02, -3.2131e-02, -8.8137e-01,
        -1.6969e-01, -5.7013e-02, -1.9678e-02, -3.9026e-02, -2.1090e-02,
        -2.1210e-03, -2.2507e-04], device='cuda:0')}, {'tokens': tensor([   89,   475,  2128,     5,    11,   682,    52,   682,    11,  1213,
           54,   533,     5,   811,    67,  9365,     5, 23457,    34,  1177,
           39,    10,   108,     6,  1696,    34,     6,    54, 16219,  4450,
            5,   851,   380,  1177,    39,  2756,     6,  1696,    34,     6,
          220,    10,   385,     6, 16810,    34,     7,     2],
       device='cuda:0'), 'score': -0.514339029788971, 'attention': tensor([[0.0176, 0.0031, 0.0007,  ..., 0.0019, 0.0185, 0.0254],
        [0.0474, 0.0509, 0.0259,  ..., 0.0029, 0.0160, 0.0246],
        [0.0219, 0.0202, 0.0062,  ..., 0.0011, 0.0067, 0.0072],
        ...,
        [0.0066, 0.0019, 0.0009,  ..., 0.0535, 0.0539, 0.0361],
        [0.0072, 0.0041, 0.0032,  ..., 0.4090, 0.3188, 0.1417],
        [0.0658, 0.0506, 0.0411,  ..., 0.3571, 0.3290, 0.3375]],
       device='cuda:0'), 'alignment': tensor([10, 10, 10, 10, 10, 10, 42, 42, 42, 42, 42, 42, 42, 42, 42, 22, 19, 19,
        42, 42, 42, 14, 14, 42, 16, 42, 42, 42, 42, 42, 31, 42, 31, 31, 42, 33,
        42, 42, 42, 41, 41, 41, 40, 40, 42, 41, 42, 42], device='cuda:0'), 'positional_scores': tensor([-7.8625e-01, -1.2459e+00, -1.7020e+00, -5.3779e-01, -3.3952e-01,
        -1.7217e+00, -1.5388e+00, -5.1001e-02, -1.3031e-02, -1.9808e+00,
        -7.0781e-01, -2.2359e-01, -3.8185e-02, -7.7656e-02, -1.2256e+00,
        -4.6812e-01, -3.6058e-01, -1.1854e-02, -8.3603e-02, -5.9268e-01,
        -2.2287e-01, -1.1333e+00, -2.5422e-01, -1.0380e-01, -7.0995e-01,
        -6.1804e-02, -9.3351e-02, -1.0434e+00, -1.7457e-01, -4.7542e-01,
        -1.1062e+00, -3.8883e-01, -1.3851e+00, -1.5234e+00, -5.4642e-02,
        -1.6203e-01, -4.7174e-01, -2.7666e-01, -4.4809e-02, -3.3175e-02,
        -9.6977e-01, -1.5878e-01, -5.4434e-02, -2.0742e-02, -3.6633e-02,
        -1.9823e-02, -2.0924e-03, -2.2697e-04], device='cuda:0')}, {'tokens': tensor([   89,   475,  2128,     5,    11,   682,    52,   682,    11,  1213,
           54,   533,     5,   811,    67,  9365,     5, 23457,    34,  1177,
           39,    10,   108,     6,  1696,    34,     6,    54, 16219,  4450,
            5,   851, 18927,    39,  2756,     6,  1696,    34,     6,   220,
           10,   385,     6, 16810,    34,     7,     2], device='cuda:0'), 'score': -0.5221420526504517, 'attention': tensor([[0.0176, 0.0031, 0.0007,  ..., 0.0020, 0.0204, 0.0261],
        [0.0474, 0.0509, 0.0259,  ..., 0.0028, 0.0145, 0.0256],
        [0.0219, 0.0202, 0.0062,  ..., 0.0011, 0.0066, 0.0079],
        ...,
        [0.0066, 0.0019, 0.0009,  ..., 0.0538, 0.0527, 0.0348],
        [0.0072, 0.0041, 0.0032,  ..., 0.4070, 0.3136, 0.1364],
        [0.0658, 0.0506, 0.0411,  ..., 0.3555, 0.3455, 0.3434]],
       device='cuda:0'), 'alignment': tensor([10, 10, 10, 10, 10, 10, 42, 42, 42, 42, 42, 42, 42, 42, 42, 22, 19, 19,
        42, 42, 42, 14, 14, 42, 16, 42, 42, 42, 42, 42, 31, 42, 31, 42, 33, 42,
        42, 42, 41, 41, 41, 40, 40, 42, 41, 42, 42], device='cuda:0'), 'positional_scores': tensor([-7.8625e-01, -1.2459e+00, -1.7020e+00, -5.3779e-01, -3.3952e-01,
        -1.7217e+00, -1.5388e+00, -5.1001e-02, -1.3031e-02, -1.9808e+00,
        -7.0781e-01, -2.2359e-01, -3.8185e-02, -7.7656e-02, -1.2256e+00,
        -4.6812e-01, -3.6058e-01, -1.1854e-02, -8.3603e-02, -5.9268e-01,
        -2.2287e-01, -1.1333e+00, -2.5422e-01, -1.0380e-01, -7.0995e-01,
        -6.1804e-02, -9.3351e-02, -1.0434e+00, -1.7457e-01, -4.7542e-01,
        -1.1062e+00, -3.8883e-01, -2.4127e+00, -3.0809e-01, -8.9973e-02,
        -7.3971e-01, -2.1401e-01, -4.1115e-02, -2.7445e-02, -9.4077e-01,
        -1.5606e-01, -5.8960e-02, -1.9199e-02, -3.5954e-02, -2.0334e-02,
        -1.9989e-03, -2.1362e-04], device='cuda:0')}, {'tokens': tensor([   89,   475,  2128,     5,    11,   682,    52,   682,    11,  1213,
           54,   533,     5,   811,    67,  9365,     5, 23457,    34,  1177,
           39,    10,   108,     6,  1696,    34,     6,    54, 16219,  4450,
            5,   851, 18927,    39,  2756,    34,     6,   220,    10,   385,
            6, 16810,    34,     7,     2], device='cuda:0'), 'score': -0.5343517661094666, 'attention': tensor([[0.0176, 0.0031, 0.0007,  ..., 0.0017, 0.0161, 0.0245],
        [0.0474, 0.0509, 0.0259,  ..., 0.0023, 0.0136, 0.0269],
        [0.0219, 0.0202, 0.0062,  ..., 0.0009, 0.0059, 0.0084],
        ...,
        [0.0066, 0.0019, 0.0009,  ..., 0.0695, 0.0777, 0.0418],
        [0.0072, 0.0041, 0.0032,  ..., 0.3862, 0.3125, 0.1340],
        [0.0658, 0.0506, 0.0411,  ..., 0.3410, 0.2894, 0.3157]],
       device='cuda:0'), 'alignment': tensor([10, 10, 10, 10, 10, 10, 42, 42, 42, 42, 42, 42, 42, 42, 42, 22, 19, 19,
        42, 42, 42, 14, 14, 42, 16, 42, 42, 42, 42, 42, 31, 42, 31, 42, 33, 42,
        42, 42, 41, 40, 40, 42, 41, 41, 42], device='cuda:0'), 'positional_scores': tensor([-7.8625e-01, -1.2459e+00, -1.7020e+00, -5.3779e-01, -3.3952e-01,
        -1.7217e+00, -1.5388e+00, -5.1001e-02, -1.3031e-02, -1.9808e+00,
        -7.0781e-01, -2.2359e-01, -3.8185e-02, -7.7656e-02, -1.2256e+00,
        -4.6812e-01, -3.6058e-01, -1.1854e-02, -8.3603e-02, -5.9268e-01,
        -2.2287e-01, -1.1333e+00, -2.5422e-01, -1.0380e-01, -7.0995e-01,
        -6.1804e-02, -9.3351e-02, -1.0434e+00, -1.7457e-01, -4.7542e-01,
        -1.1062e+00, -3.8883e-01, -2.4127e+00, -3.0809e-01, -8.9973e-02,
        -9.0693e-01, -3.2232e-02, -4.7621e-01, -1.7769e-01, -6.4222e-02,
        -3.9980e-02, -3.2602e-02, -2.8133e-02, -2.6798e-03, -2.3842e-04],
       device='cuda:0')}, {'tokens': tensor([   89,   475,  2128,     5,    11,   682,    52,   682,    11,  1213,
           54,   533,     5,   811,    67,  9365,     5, 23457,    34,  1177,
           39,    10,   108,     6,  1696,    34,     6,    54, 16219,  4450,
            5,   851,  1177,    39,  2756,     6,  1696,    34,     6,   220,
           10,   385,     6, 16810,  3432,     7,     2], device='cuda:0'), 'score': -0.6213326454162598, 'attention': tensor([[0.0176, 0.0031, 0.0007,  ..., 0.0020, 0.0182, 0.0235],
        [0.0474, 0.0509, 0.0259,  ..., 0.0028, 0.0230, 0.0235],
        [0.0219, 0.0202, 0.0062,  ..., 0.0011, 0.0088, 0.0069],
        ...,
        [0.0066, 0.0019, 0.0009,  ..., 0.0567, 0.0420, 0.0309],
        [0.0072, 0.0041, 0.0032,  ..., 0.4043, 0.2732, 0.1420],
        [0.0658, 0.0506, 0.0411,  ..., 0.3544, 0.3181, 0.3751]],
       device='cuda:0'), 'alignment': tensor([10, 10, 10, 10, 10, 10, 42, 42, 42, 42, 42, 42, 42, 42, 42, 22, 19, 19,
        42, 42, 42, 14, 14, 42, 16, 42, 42, 42, 42, 42, 31, 42, 31, 42, 33, 42,
        42, 42, 41, 41, 41, 40, 40, 42, 41, 42, 42], device='cuda:0'), 'positional_scores': tensor([-7.8625e-01, -1.2459e+00, -1.7020e+00, -5.3779e-01, -3.3952e-01,
        -1.7217e+00, -1.5388e+00, -5.1001e-02, -1.3031e-02, -1.9808e+00,
        -7.0781e-01, -2.2359e-01, -3.8185e-02, -7.7656e-02, -1.2256e+00,
        -4.6812e-01, -3.6058e-01, -1.1854e-02, -8.3603e-02, -5.9268e-01,
        -2.2287e-01, -1.1333e+00, -2.5422e-01, -1.0380e-01, -7.0995e-01,
        -6.1804e-02, -9.3351e-02, -1.0434e+00, -1.7457e-01, -4.7542e-01,
        -1.1062e+00, -3.8883e-01, -2.4622e+00, -6.0059e-02, -1.5048e-01,
        -4.8230e-01, -2.6030e-01, -4.6995e-02, -3.2131e-02, -8.8137e-01,
        -1.6969e-01, -5.7013e-02, -1.9678e-02, -3.9026e-02, -5.0639e+00,
        -2.9011e-03, -3.6430e-04], device='cuda:0')}]
H-0     -0.5140185356140137     中国 因 所谓 的 " 19 - 19 " 疾病 而 死亡 的 人数 从 前一天 的 254 人 上升 到 了 1 , 000 人 , 而 确诊 病例 的 数量 上升 到 59 , 000 人 , 增加 了 15 , 152 人 .
P-0     -0.7862 -1.2459 -1.7020 -0.5378 -0.3395 -1.7217 -1.5388 -0.0510 -0.0130 -1.9808 -0.7078 -0.2236 -0.0382 -0.0777 -1.2256 -0.4681 -0.3606 -0.0119 -0.0836 -0.5927 -0.2229 -1.1333 -0.2542 -0.1038 -0.7099 -0.0618 -0.0934 -1.0434 -0.1746 -0.4754 -1.1062 -0.3888 -2.4622 -0.0601 -0.1505 -0.4823 -0.2603 -0.0470 -0.0321 -0.8814 -0.1697 -0.0570 -0.0197 -0.0390 -0.0211 -0.0021 -0.0002

由于我们不清楚，模型是怎么处理数据的，所以尝试打印数据处理流程：

line is:::: ['The death toll in China from the disease known as COVID - 19 reached 1 , 367  ,  up 254 from a day earlier  ,  and the number of confirmed cases jumped to 59 , 804  ,  up 15 , 152  .']
token is : [tensor([   19,   779,  6255,    11,    86,    33,     5,  1685,   734,    24,
            3,    82,   764,  1019,   141,     6,     3,     6,    69, 22779,
           33,    12,   166,  1386,     6,     9,     5,   131,     8,  1721,
          435,  6129,    10,  2755,     6,     3,     6,    69,   359,     6,
        16887,     7,     2])]

对应 11,17，还有倒数的第8个都无法识别，即分别对应上面的 COVID ,  367  , 804, 送入模型后，由于无法识别，被置为 unk对应的编号3

其中的token是通过以下方式得来：
tokens = [
    task.source_dictionary.encode_line(
        encode_fn(src_str), add_if_not_exist=False
    ).long()
    for src_str in lines
]
看来经过字典后，上面三个字都被处理成了unk对应的编号3

在方法：
```python
#Process top predictions
for hypo in hypos[:min(len(hypos), args.nbest)]:
    print('hypo type:', type(hypo), 'hypo:', hypo)

    hypo_tokens, hypo_str, alignment = utils.post_process_prediction(
        hypo_tokens=hypo['tokens'].int().cpu(),
        src_str=src_str,
        alignment=hypo['alignment'].int().cpu() if hypo['alignment'] is not None else None,
        align_dict=align_dict,
        tgt_dict=tgt_dict,
        remove_bpe=args.remove_bpe,
    )
```

可以看到，调用了utils中的post_process_prediction，
这个函数为：

```python
def post_process_prediction(hypo_tokens, src_str, alignment, align_dict, tgt_dict, remove_bpe=None):
    from fairseq import tokenizer
    hypo_str = tgt_dict.string(hypo_tokens, remove_bpe)
    if align_dict is not None:
        hypo_str = replace_unk(hypo_str, src_str, alignment, align_dict, tgt_dict.unk_string())
    if align_dict is not None or remove_bpe is not None:
        # Convert back to tokens for evaluating with unk replacement or without BPE
        # Note that the dictionary can be modified inside the method.
        hypo_tokens = tgt_dict.encode_line(hypo_str, add_if_not_exist=True)
    return hypo_tokens, hypo_str, alignment
```

可以看到这个函数调用了replace_unk方法：

```python
def replace_unk(hypo_str, src_str, alignment, align_dict, unk):
    from fairseq import tokenizer
    # Tokens are strings here
    hypo_tokens = tokenizer.tokenize_line(hypo_str)
    # TODO: Very rare cases where the replacement is '<eos>' should be handled gracefully
    src_tokens = tokenizer.tokenize_line(src_str) + ['<eos>']
    for i, ht in enumerate(hypo_tokens):
        if ht == unk:
            src_token = src_tokens[alignment[i]]
            # Either take the corresponding value in the aligned dictionary or just copy the original value.
            hypo_tokens[i] = align_dict.get(src_token, src_token)
    return ' '.join(hypo_tokens)

```
不明白的是，src_token是已经做处理后的文本，即对应不识别的字已经设为unk，这里再替换有什么用？

整个流程大概清楚了，替换原文等待下一步进行。

